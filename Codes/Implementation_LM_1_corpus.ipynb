{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dc97ad",
   "metadata": {},
   "source": [
    "# This file is for LM_1_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2086d",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675204e7",
   "metadata": {},
   "source": [
    "Importing all required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "510f3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import zipfile\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e659b",
   "metadata": {},
   "source": [
    "Loading data from LM_corpus and storing in document named variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c256933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "dataset_dir = 'LM_1_Corpus'\n",
    "\n",
    "def read_pdf_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = ''\n",
    "        for page_num in range(pdf_reader.getNumPages()):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "        return text\n",
    "\n",
    "documents = []\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if filename.endswith('.pdf'):\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        text = read_pdf_file(filepath)\n",
    "        documents.append(text)\n",
    "print(\"Total Number of Documents:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3d67c64",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "['T urk J Elec Eng & Comp Sci\\n(2019) 27: 3665 – 3681\\n© TÜBİT AK\\ndoi:10.3906/elk-1806-132\\nTurkish Journal of Electrical Engineering & Computer Sciences \\nhttp://journals.tubitak.gov.tr/elektrik/\\nResearch Article \\nIncremental author name disambiguation using author profile models and\\nself-citations\\nIjaz HUSSAIN∗, Sohail ASGHAR\\nDepartment of Computer Science, COMSA TS University Islamabad, Islamabad, Pakistan\\nReceived: 18.06.2018 • Accepted/Published Online: 05.06.2019 • Final V ersion: 18.09.2019\\nAbstract: Author name ambiguity in bibliographic databases (BDs) such as DBLP is a challenging problem that\\ndegrades the information retrieval quality , citation analysis, and proper attribution to the authors. It occurs when\\nseveral authors have the same name (homonym) or when an author publishes under several name variants (synonym).\\nT raditionally , much research has been conducted to disambiguate whole bibliographic database at once whenever some\\nnew citations are added in these BDs. However, it is more time-consuming and discards the manual disambiguation\\neffects (if any). Only a few incremental author name disambiguation methods are proposed but these methods produce\\nfragmented clusters which lower their accuracy . In this paper, a method, called CAND, that uses author profile models\\nand self-citations for incremental author name disambiguation is proposed. CAND introduces name indices that enhance\\nthe overall system response by comparing the newly inserted references to the indexed author clusters. Author profile\\nmodels are generated for the existing authors in BDs which help in disambiguating the newly inserted references. A\\ncomparator function is proposed to resolve the incremental author name ambiguity which utilizes the most strong\\nbibliometric features such as coauthor, titles, author profile models, and self-citations. T wo real-world data sets, one\\nfrom Arnetminer and the other from BDBComp, are used to validate CAND’s performance. Experimental results show\\nthat CAND’s performance is overall better than the existing state-of-the-art incremental author name disambiguation\\nmethods.\\nKey words: Incremental author name disambiguation, author profile models, name indices, self-citations, bibliographic\\ndatabases\\n1. Introduction\\nDue to a limited number of names or some popular names, different authors may have the same name and\\nin contrast to this, an author name may be represented in different ways due to different journals/conferences\\nnaming conventions. Author name ambiguities can cause wrong attributions and incorrect search results [ 1–3].\\nThis is quite common in Asian names, particularly in Chinese and Korean. The methods that resolve these\\nauthor name ambiguities are called author name disambiguation (AND) methods. The increased growth of\\nscientific publications has made the author name ambiguity problem much harder than in the past. Bollen et\\nal. predicted the substantial growth in coming years for the research articles [ 4]. In 2010, Jinaha estimated\\nthat until now 50 million research articles have been published, and on average one article per minute is being\\npublished [ 5].\\nHussain et al., in a recent survey of author name disambiguation techniques, pointed out different\\nchallenges in AND methods [ 6]. One of the major challenges among others is an incremental author name\\n∗Correspondence: ijazhussain7979@gmail.com\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.3665HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\ndisambiguation. Citation records are constantly generated and inserted into bibliographic databases (BDs) that\\nare already disambiguated. Each of these newly inserted references rishould be assigned to their respective\\nreal authors. The majority of existing BDs rerun the whole disambiguation process upon new insertions of\\ncitations which is called batch AND. However, this is infeasible due to three reasons. First, these methods have\\nscalability issues as they again apply disambiguation algorithms on the whole citation records of these BDs.\\nSecond, these methods are not feasible for supervised disambiguation methods because they need to retrain the\\nmodel on each new citation update/insertion. Third and last, they destroy the manual disambiguation effects\\nwhich are sometimes necessary for fine-tuning of the disambiguation results.\\nAlthough, several AND methods have been proposed (see, for instance, [ 6]), but the vast majority of those\\nmethods work for batch AND. T o the best of our knowledge, there are only three incremental AND methods\\n[7–9]. These methods resolve only newly inserted articles and are more effective than batch (traditional) AND,\\nand preserve the manual disambiguation effects (if any).\\nDe Carvalho et al. proposed a solution called “incremental unsupervised name disambiguation in cleaned\\ndigital libraries (INDi)”, which produces very pure clusters on the basis of title and coauthors, but it splits an\\nauthor’s papers into many authors [ 7]. Esperidiao et al. enhanced the INDi by dropping the assumption that\\nBDs are already cleaned, and proposed five record-selection strategies for newly inserted records in these BDs\\nand found that a newly inserted record closer to the centroid of the existing disambiguated records called CEN\\nis best among these [ 8]. The fragment comparison method is used by Santana et al. to retrieve the relevant\\nauthor blocks to the new record [ 9]. All these methods suffer from fragmentation problem which lowers their\\naccuracy and compares the new reference to existing clusters by comparing it with all the individual references\\nin that clusters which is not feasible for larger clusters.\\nIn this paper, we propose and evaluate a novel incremental AND method which creates less fragmented\\nclusters and improves the accuracy of the method1. W e used our previously proposed batch AND method [ 10]\\nas an input to the proposed incremental AND method (CAND), as shown in Figure 1.\\nIn a nutshell, the main contributions of this paper are as follows:\\n–A blocking-based name index structure that enhances the overall system response is proposed and author\\nprofile models are built, which helps in disambiguating the newly inserted citations,\\n–A comparison function is presented; this function utilizes the most strong bibliometric features such as\\ncoauthors, titles, author profile models, and self-citations to effectively fuse the newly inserted reference\\nto the existing cluster/clusters. Self-citations are used for the first time to solve the incremental author\\nname ambiguity problem. CAND exploits author name indices, author profile models, and a comparison\\nfunction to solve the incremental author name ambiguity , and\\n–Experiments on two real-world data sets are performed to validate the effectiveness of CAND. Experi-\\nmental results show that CAND is overall better than the existing incremental AND methods.\\nThe rest of the paper proceeds as follows: Some studies related to CAND are described in Section 2,\\nwhile CAND architecture is discussed in Section 3. At Section 4, CAND is compared with existing incremental\\nAND methods. Finally , we conclude the paper and discuss some possible future research directions of our work\\nat Section 5.\\n1In terms of clustering metrics (AAP , ACP , and K-metric)\\n3666HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nData Extraction \\nBatch AND Results \\nAuthor 1 Author 2 Author 3 Author 4 Publications \\nComparing\\nAuthor 1 +Author 2 Author 3 Author 4 Author 5 Updates \\nIncremental AND results Profiling \\nNew recor ds Indexing \\nFigure 1 .Incremental AND architecture diagram.\\n2. Related work\\nCurrent AND methods can be categorized into two groups: the first are the methods that resolve whole citations\\nin BDs on new insertions called batch AND and the second are those methods which disambiguate only newly\\ninserted citations called incremental AND. Similarly , Hussain et al. in [ 6] proposed a taxonomy for existing\\nAND methods and divided all methods into supervised [ 3,11–13], unsupervised [ 14–19], semisupervised [ 20–22],\\ngraph-oriented approaches using graph models or social networks [ 1,23–25], and string processing or heuristic-\\nbased methods [ 26–28]. In this section, we only overview the incremental AND methods. F or detailed discussions\\nabout AND methods and AND data sets, interested readers are referred to our recent survey of these techniques\\npresented in [ 6] and [ 29], respectively .\\nKim et al. proposed an algorithm which consists of two main steps-data generation and matching\\nprocedure. In the first step, data that are used for feature matching in the later stage are generated. Per\\nfeature matching and clustering is used to generate the training/evaluation data. Experiments are done on the\\nW eb of Science data set and the performances are compared [ 30].\\nCarvalho et al. proposed INDi a solution for the existing cleaned BDs. INDi utilizes similarity among\\nbibliographic records and groups the new records to authors with similar citation records in the BD or to new\\nauthors when the similarity evidence is not strong enough. Heuristics such as titles and coauthors are used for\\nchecking whether references of new citation records belong to preexisting authors of the BD or if they belong\\nto new ones (i.e. authors without citation records in the BD), avoid running the disambiguation process on the\\n3667HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nentire BD. They run simulations on BDBComp collection and synthetic data set to assess the effectiveness of\\nINDi.\\nAbdulhayoglu and Thijs used Research Gate (RG), connected components, and a graph-based machine\\nlearning approach to form groups of authors. Connected components are compared with research gate retrieved\\nauthor pages for the same author. Then, they retained the groups that have at least 10 authors for detailed\\nexamination and employed Google custom search engine API to access the pages of authors for complementing\\nthe research gate pages [ 31].\\nEsperidiao et al. improved INDi by proposing a new technique INDi+ which try to overcome fragmen-\\ntation problem that is present in INDi. Of course, these INDi and INDi+ are more efficient than the batch\\n(traditional) AND methods, but they clearly lack a balance between purity and fragmentation of author papers.\\nSantana et al. proposed “Incremental author name disambiguation by exploiting domain-specific heuris-\\ntics (INC)” that use fragment comparison method for retrieving the relevant author blocks to the new record [ 9].\\nThey used author names, coauthor names, titles, and venue for similarity calculation between new records and\\nthe retrieved block of records. However, these methods suffer from different problems such as fragmentation,\\nnot handling transitivity problem, how to configure their large set of parameters with limited training data.\\nZhao et al. presented a naive Bayes probabilistic model which has three stages. In the first stage, they\\ninitialize a model and if there is no model present in the existing disambiguation results, and then they use high-\\nprecision rules for generating labeled training data. In the second stage, they trained a naive Bayes classifier\\nfor each cluster group. In the third stage, clusters that achieve top posterior probabilities are matched with\\ncoauthor similarities for predicting its class. This method creates very pure but fragmented clusters [ 32].\\nX\\nY ZCo-Authors Co-Authors\\nFigure 2 .T ransitivity problem.\\nIn contrast to existing related methods, CAND uses author profile models, self-citations, and requires no\\ntraining. CAND compares new ambiguous record to a set of records (clusters) as a whole, so CAND does not\\nsuffer from transitivity problem. When there are three or more papers where paper “A” is written by author\\n“X” and author “Y”, paper “B” is written by author “X” and author “Z”, and another paper is written by\\nauthor “Y” and author “Z”, in the first two papers, we are not sure that author “X” in the first paper and\\nauthor “X” in the second paper are the same. This is called transitivity problem as there is no direct relationship\\nbetween these two authors, but indirect relationship via their coauthors as shown in Figure 2. Generally , in\\nAND domain two papers are compared at a time and hence those methods are unable to solve this problem.\\nCAND compares new papers with an existing cluster. Therefore, it does not suffer from transitivity problem.\\nCAND is similar to INDi, INDi+, and INC as these methods use coauthors, and titles for disambiguation. A\\ncomparison of CAND with state-of-the-art methods is given in T able 1.\\n3668HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nT able 1 .A comparison between CAND and other related works (Ref. is Reference, Inc. is Incremental).\\nRef. Inc. Methodology Dataset Evidence used Capability Limitations\\n[30] No T riangulation approach DBLP Automatic combined with\\nmanual labelingBoth Manual labeling is time-\\nconsuming and compared\\nwith baseline algorithms.\\n[31] No Research Gate and Google\\ncustom search engine were\\ncompared with clustering\\nresults.W eb of Science\\nCore CollectionResearch Gate Profile and\\nGoogle Scholar pagesHomonyms External web accesses\\nmake it slow.\\n[32] Y es A probabilistic naive\\nBayes model that simul-\\ntaneously uses a rich set\\nof metadata and reduces\\nthe amount of pairwise\\ncomparisons needed for\\nnew articles.W eb of Science Authors, coauthors, e-\\nmail, self-citation, middle\\ninitial with subject, exact\\ncitation and exact venue.- Parameter sweeping is uti-\\nlized.\\n[33] No High-precision rules are\\nused for generating initial\\ntraining instances for the\\ntraining of the supervised\\nmethod.QIAN, Arnetminer ORCIDs linkage, e-mail\\naddress, authors, coau-\\nthors, citationsHomonyms Performance of the pro-\\nposed method relies on\\nthe availability of match-\\ning features.\\n[9] Y es F ragment comparison\\nmethodKISTI,\\nSyGar,BDBCompAuthors, coauthors, e-\\nmail and venue.Homonyms F ragmentation, not han-\\ndling transitivity problem,\\nhow to configure their\\nlarge set of parameters\\nwith limited training data\\n[8] Y es AND heuristic such as\\nsimilar titles and coau-\\nthors are used.BDBComp, SyGar Titles and coauthors - F ragmented clusters.\\nCAND Y es Author profile model com-\\nbined with heuristics and\\nself-citation.Arnetminer, BDB-\\nCompAuthors, coauthors, titles,\\nself-citations, author sig-\\nnatures.Both Cannot handle the case of\\nvery ambiguous authors.\\n3. The proposed system: CAND\\nCAND consists of the following main modules: index creator, authors profile builder, data extractor, and\\ncomparator, as shown in Figure 1. There are two inputs to the CAND algorithm. The first one is the\\ndisambiguated results using the batch AND, and the second is the incremental updates of publications received\\nfrom different publishing venues. CAND assumed that at the start the BD is disambiguated. Thus, we use the\\noutput of our previously proposed batch AND method as an input to CAND [ 10]. It is worth to note that the\\nbatch AND method is applied only once and in subsequent citation loads, only the CAND algorithm is used for\\ndisambiguation.\\n3.1. Indices creator\\nAuthor node consists of node ID, node name, and node publications. All author names are parsed into two\\ncomponents: first name and last name. W e index all these names using the last names of the authors. This\\nindex is important for efficient retrieval of the results, and it improves the overall performance of the system.\\nThe structure of the author name index is shown in Figure 3.\\nAuthor names are divided into first name and last name. Indices are created using the last name of the\\nauthors, for example, Akram Abay and Altaf Abay would go into the same block “Abay , A” as shown in Figure\\n3. Any name having more than two name parts is also divided into the same two name parts (tokens). In this\\n3669HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nLastnameindex\\nAbay,A Hussain,I Zain,H\\nAbay,\\nAkramAbay,Altaf Abay,Amir Zain,Hassan Zain,Hussain Abay,Aziz\\nFigure 3 .A blocking-based author names index structure.\\nindexing, Akram Abay in that block precedes Altaf Abay because the first name in Akram Abay alphabetically\\ncomes before that in Altaf Abay .\\n3.2. Authors profile builder\\nDisambiguated results are clusters of nodes (authors) in coauthor’s graph. As mentioned in the previous\\nsection, the author node consists of node ID, node name, node publications. All author names are parsed into\\ntwo components: first name and last name. W e index all these names according to the proposed index in Section\\n3.1 , using the last names of the authors.\\nWhen there is a huge number of existing authors, there is a very high probability that the new author\\nbelongs to some of the existing authors. The same intuition is used for other features of the citations like\\ncoauthors, titles, self-citations, and references. F or example, an author usually publishes one’s research with a\\nspecific set of coauthors, in certain venues, on certain topics, and self-cites very often [ 34]. According to a recent\\nstudy conducted by King et al., 9.4% of all citations are self-citations [ 35]. Snyder and Bonzi estimated the\\npattern of self-citations in different disciplines and found that self-citation in physical sciences (15%) is larger\\nthan in social sciences (6%) and humanities (3%) [ 36]. In the light of these facts, we assume that self-citation\\nmay be a strong feature for author disambiguation.\\nF ollowing these intuitions, we build a set of candidate author profile models, which we call author profile\\nmodels. Recall from the preceding paragraphs that these author profile models have author names, set of\\ncoauthors, titles feature vectors, set of venues, and set of references. F or example, we take an author Akram\\nAbay , which is already disambiguated (batch AND) with the help of algorithms presented in [ 10]. Our batch\\nAND algorithms keep a record of publications that belong to author Akram Abay . With these pieces of\\ninformation, we can easily build the profiles of candidate authors that are equivalent (or similar) to the newly\\ninserted authors. Figure 3shows the structure of these author profiles. These profiles act as an author’s\\nsignature and help to disambiguate authors.\\n3.3. Data extractor\\nWhen a number of new citations are inserted in BDs, citation records that are composed of a set of attributes\\nsuch as authors, coauthors, titles, venues, and references of the papers are tokenized. These newly inserted\\nrecords need disambiguation and assignment to their respective clusters. CAND takes these records one by one\\nand retrieves a set of equivalent (similar) author profiles from the already disambiguated BDs. CAND constructs\\nkeyword feature vectors from the citation title words for comparison among author’s research interests. In this\\nresearch, it is assumed that such keyword feature vectors represent the author’s research interests, at least\\n3670HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nAuthorIndex Profile(Author)\\nAaby,Aamir\\nAaby,Ajaz\\nAbay,Akram:\\n:...\\n::\\n:\\n:\\n:Co-authorsList TitlesFVVenuesFV ReferencesList\\nCo-authorsList TitlesFVVenuesFV ReferencesList:\\n:\\n:\\n:\\n:\\n:\\nCo-authorsList TitlesFVVenuesFV ReferencesList\\nSohailAsghar,MuhammadRiaz,Muhammad\\nRehan,ShakeelAhmad,MohsinSattar,\\nMuhammadIbrahim,MaryamChoudary\\nAuthornamedisambiguation,homonymresolution,\\nsynonymresolution,graphstructuralclustering,\\nsemanticsimilarity, entityresolution,recordlinkageFrontiersofinformationtechnology,\\nKnowledgeengineeringreview,Journalofinformation\\nscience,Turkishjournalofelectricalengineeringand\\ncomputerscience,Arabianjournalofscience\\n#%945584,#%2842290,#%6205546,#%6270759,\\n#%564877,#%5624235,#%5944847,#%4799177,\\n#%5866408,#%9664584,#%28662290,#%6056546,\\n#%564877,#%5642375,#%59487447,#%47917667......\\nAaby,A\\n:\\n:\\n:\\n:\\nZhu,Z:\\n:\\n:\\n:\\nFigure 4 .Author profiles from disambiguated BDs.\\npartially . However, such kind of title feature vector construction demands preprocessing the title words. In the\\ntitles, some words that are called stop words, for example, ‘a’, ‘an’,‘the’, ‘for’, ‘and’,‘of’,‘that’, are removed.\\nThese words usually do not carry any semantic information and are frequently used in titles. In addition,\\nthe remaining title keywords are stemmed. F or example, the word “deputation” is transformed into the stem\\n“deput” by a stemmer. An advantage of the stemming is that all morphological variants are transformed to the\\nbase word. Hence, it considerably reduces the dimensions of the feature vectors. CAND uses standard English\\nStop W ord list, and Porter stemmer [ 37] for stemming.\\n3.4. Comparator\\nGiven a set of candidate clusters and a new record, there are three possibilities that the newly inserted record\\neither belongs to a candidate cluster or to more than one candidate clusters, or does not belong to any of the\\ncandidate clusters. Figure 5illustrates an example of possible ways of assignments of new reference record to\\nexisting record of clusters. In the first case, the new record is merged with the equivalent (or similar) cluster\\nand author profile is updated accordingly while in the second case, all the equivalent (or similar) clusters are\\nmerged into one cluster along with the new record, and author profiles are updated. When it is established\\nwith the help of the CAND algorithm that the new record does not belong to any existing cluster, then a new\\ncluster is generated, and its author profile is built and saved for further disambiguation.\\nThe pseudocode of the incremental author name ambiguity resolver algorithm is given in Algorithm 1.\\nThe algorithm starts with the preprocessing of the newly inserted citations as described in Section 3.3 of this\\npaper (line 1). CAND retrieves the equivalent (or similar) author clusters to the newly inserted author in the\\ncleaned BDs clusters and builds their profiles. These profiles are built and indexed for each cluster according\\nto the procedures given in Sections 3.2 and 3.1 , respectively . If there is no equivalent (or similar) cluster in\\nthe cleaned BDs, then a new cluster of the newly inserted reference is created and inserted in the cleaned BDs\\n3671HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nAuthor 1 Author 2 Author 3 Author 4 \\nAuthor 1 +Author 2 Author 3 Author 4 Author 5 CAND Algorithm\\n1. Merge with a cluster 2. Merge with two or more existing clusters 3. Make new clusters New records Existing Clusters \\nCase 2 Case 1 \\nCase 3 \\nFigure 5 .An example of new reference assignment to existing clusters.\\n(lines 2–11). Upon finding equivalent (or similar) clusters with respect to the newly inserted reference record\\nusing Jaro similarity as given in Equation 1[38], CAND tests two conditions between all the clusters and the\\nnewly inserted records.\\nd(n1;n2) =1\\n3\\x03(c\\nn1+c\\nn2+c\\x00m\\nc)\\n; (1)\\nwhere cis the number of common characters in two name strings. A character that is considered a common\\ncharacter at position iin the string n1has to be within the H window of the equivalent jthcharacter in the\\nstring n2. Here H=⌊\\nmax(jn1j;jn2j)\\n2⌋\\n\\x001. Similarly , m is equal to the number of characters matched from the\\nwindow but not at the same index divided by 2.\\nThere are more than one similar coauthors present between the equivalent (or similar) cluster Acj and\\nnewly inserted paper Aiusing Equation 2:\\nCoauth (Ai; Acj) =jAi\\\\Acjj\\nmin(jAij;jAcjj): (2)\\nThe\\\\ along with the enclosing jj-operator finds the number of common coauthors. The second condition\\nis that we find the self-citations using Equation 3, when the newly inserted paper cites some paper from the\\nexisting clusters.\\n3672HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nAlgorithm 1: CAND:Proposed Incremental AND Algorithm\\nData: Existingset of clusters C; A set of records R\\nResult: Set of disambiguated clusters DC\\n1 R′ \\x00Preprocess Records (R)\\n2 DC \\x00 0\\n3 forrefrence r2R′do\\n4 C′ \\x00Get Similar Clusters (C ; r )\\n5 C′′ \\x00Build Author Profiles (C′; r)\\n6 P \\x00Load Profiles (C′′; r)\\n7 ifP==null then\\n8 c \\x00create new cluster ()\\n9 DC \\x00C[c\\n10 Update (DC)\\n11 end\\n12 forcluster p2P do\\n13 ifp:coauthors\\\\r:coauthors ⩾1and Sim (PF V; rF V)>0:1\\n14 orr cites p:publications\\n15 then\\n16 p=p[r\\n17 DC \\x00C[p\\n18 Update (DC)\\n19 end\\n20 else\\n21 c \\x00Create new Cluster (r)\\n22 DC \\x00DC[c\\n23 Update (DC)\\n24 end\\n25 end\\n26 end\\n27 returnUpdatedClusters DC\\nSelf cit=n∑\\nj=1(Pi\\\\Rcj): (3)\\nF eature vectors of titles are generated for all the papers in the data set. The similarity between the newly\\ninserted paper title feature vector and cluster feature vector is found using Equation 4. The similarity index,\\nSimilarity (Afv; Bfv) =Afv\\\\Bfv\\nmaxj(Afv; Bfv)j(4)\\nbetween these titles feature vectors is used for comparison between cluster and newly inserted paper. If any\\ncondition satisfies then CAND merges that newly inserted paper to the existing cluster and updates the cluster\\n(lines 12–19). When these conditions are not satisfied, then a new cluster for the newly inserted paper is created\\nand set of clusters is updated accordingly . At the end, a set of updated clusters is returned (lines 20–27).\\n3673HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\n4. Experiments and results\\nIn this section, the performance of CAND is compared with two incremental AND techniques using Arnetminer\\nand BDBComp in the form of clustering evaluation metrics. W e used these data sets because they have citation\\ninformation along with other attributes used in CAND and are open source data sets.\\n4.1. Data sets and evaluation measures\\nT ang et al. created Arnetminer data set from DBLP , ACM, and Microsoft academic graph, and other sources.\\nArnetminer has 629,814 papers and more than 632,752 citation relationships. It is organized into 600,000 blocks,\\none for each paper. F urther details of Arnetminer can be viewed online at https://aminer.org/data . CAND\\nuses only coauthors, titles, and citations for the complete solution of the incremental author name ambiguity\\nproblem.\\nBDBComp is a relatively small data set of 363 records belonging to 184 distinct authors, but it is very\\ndifficult to disambiguate as majority of the authors have only one or two citation records. BDBComp statistics\\nare given in T able 2and this collection also has been frequently used in similar AND studies [ 7,8,25].\\nT able 2 .Details of BDBComp data set.\\nS No. Name No. of citation records No. of ambiguous authors\\n1 A. Oliveira 52 16\\n2 A. Silva 64 32\\n3 F. Silva 26 20\\n4 J. Oliveira 48 18\\n5 J. Silva 36 17\\n6 J. Souza 35 11\\n7 L. Silva 33 18\\n8 M. Silva 21 16\\n9 R. Santos 20 16\\n10 R. Silva 28 20\\nW e analyze the citations per author, which is also called the diversity of the data set. It is obtained by\\nclustering the names of the authors in citations’ data set. In Figure 6, the left figure shows the distribution of\\nBDBComp, where 74.3% of the authors have published one paper. In contrast to this, in Arnetminer (right\\nfigure), 48.2% of the authors published only one paper.\\nThree clustering metrics, namely author cluster purity (ACP), Author average purity (AAP), and K-\\nmetric (K) as given in Equation 5, are used for evaluation of CAND [ 1,9,10].\\nACP =1\\nNR∑\\nr=1S∑\\ns=1n2\\nrs\\nnr; AAP =1\\nNS∑\\ns=1R∑\\nr=1n2\\nrs\\nns; K=p\\nACP\\x03AAP; (5)\\nwhere N is total number of references in ambiguous group, R is empirical clusters, S is ground truth clusters,\\nnrs is total references which are present in both empirical and in ground truth clusters, and nris total references\\npresent in empirical clusters.\\n3674HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\n4.2. Baseline methods\\nW e found three incremental AND methods in the literature: INDi [ 7], reducing fragmentation in incremental\\nauthor name disambiguation (INDi+) [ 8], and INC [ 9]. W e choose these methods as baselines to compare with\\nCAND because these are the state-of-the-art incremental AND methods and are closely related to our method.\\nDecarvalho et al., in [ 7], proposed INDi as a solution for the existing cleaned BDs. INDi utilized similarity\\nbetween bibliographic records and groups of the records of authors with similar citation records in the BD or\\nto new authors when the similarity evidence is not strong enough. INDi used coauthors, titles, and venues\\nheuristics for checking whether references of new citation records belong to preexisting authors, or to new\\nones (i.e. authors without citation records in the BD). However, INDi suffered from two problems: first, it\\ncreated fragmented clusters of authors (i.e. records of the equivalent author is split into multiple groups), and\\nsecond, it could not handle the transitivity problem. Esperidiano et al., in [ 8], dropped the assumption that\\nthe existing BDs are cleaned, and they tried to merge the fragmented clusters that were produced by INDi.\\nThey also proposed different selection criteria to improve the cluster purity . Their strategy , which compared\\nthe newly inserted record with only the leading cluster record that is the closest to the centroid of each cluster,\\nis considered the best overall performance in their experiments. W e call this INDi+ and use this for comparison\\nbecause it is an improved version of INDi. W e set its parameters as \\x0btitle= 0:01; \\x0b venue = 0:1and \\x0e = 0:6.\\nx = 174.3%1 < x < = 10 \\n21.4%\\n10 < x < = 100 and 0.2\\n4.1%x = 1\\n48.2%\\n1 < x < = 10 43.9%10 < x < = 100 and 0.37.6%\\nFigure 6 .Author’s citation distribution of BDBComp (left) and Arnetminer (right).\\nRecently , Santana et al., in [ 9], proposed INC that used fragment comparison method for retrieving the\\nrelevant author blocks to the new record, and then used author names, coauthor names, titles, and venue for\\nsimilarity calculation between new records and the retrieved block of records. W e use its best parameter values\\nand set them as Wa= 3; Wc= 3; Wt= 1; Wv= 1and \\r = 2 .\\n4.3. Experimental results and discussion\\nW e used an incremental load from each year starting from 1987 to 2007. In 1987, whole BD was disambiguated\\nusing the batch AND algorithm proposed by us [ 18], whereas, for subsequent loads after 1987, CAND is used\\nfor each new year. Figure 7shows CAND’s performance on Arnetminer.\\nAs seen in this figure, fragmentation is extremely low from 1987 to 1990 and from 2003 to 2007, but there\\nis a slight decrease in between these two periods. Purity remains very high throughout 1987 to 2007, as it is one\\n3675HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nof the desirable characteristics of AND methods because in the later stages it is difficult or even impossible to\\nsplit the merging error. At the end of the twenty-one-year period, the values of K, ACP , and AAP for CAND\\nare 0.89, 0.87, and 0.92, respectively . On the other hand, as seen in Figure 8, purity is declining throughout the\\nperiod from 1987 to 2007.\\nF ragmentation value is relatively stable and almost equal to the purity of CAND on Arnetminer. K,\\nACP , and AAP are 0.89, 0.78, and 0.83, respectively , for CAND using BDBComp at the end of the experiment.\\nAnother important aspect of the results is that CAND performance on Arnetminer is relatively higher, and in a\\nnarrower band than on BDBComp. W e suspect that this is due to the fact that on average 56% of publications\\nbelong to new authors in BDBComp, whereas in Arnetminer only 34% belong to new authors. It is highly\\nprobable that the new inserted reference may be merged with some of the existing clusters.\\n1 , 990 1,995 2,000 2, 0050.40.60.81\\nPubl/i.dotcat/i.dotons Load Year Value \\nACP\\nAAP\\nK-metr/i.dotc \\n1,990 1,995 2,000 2,0050.40.60.81\\nPubl/i.dotcat/i.dotons Load Year Value \\nACP \\nAAP \\nK-metr/i.dotc \\nFigure 7 .ACP , AAP , and K-metric performance evalua-\\ntion of CAND on Arnetminer.Figure 8 .ACP , AAP , and K-metric performance evalua-\\ntion of CAND on BDBComp.\\nT able 3lists results using CAND, INDi+, and INC on Arnetminer for the period from 1987 to 2007.\\nCAND achieves overall better performance than competing techniques during the whole period from 1987 to\\n2007. At the end of the twenty-one-year period, CAND achieves 14% and 11% higher value than INDi+ and\\nINC in K metric using Arnetminer, respectively .\\nSimilarly , T able 4shows ACP , AAP , and K-metric of CAND, INDi+ and INC on BDBComp. CAND\\ncompares the new records with only the equivalent (or similar) cluster not with all the records in that cluster,\\nso it improves the running time compared to INDi+ and INC, which compare the newly inserted reference to\\nall the existing records. In general, ACP of CAND is higher than AAP because CAND produces pure clusters.\\nCAND achieves overall better performance compared to INDi+ and INC during the whole period. At the\\nend of the twenty-one-year period, CAND achieves 11% and 4% higher than INDi+ and INC in K-metric on\\nBDBComp, respectively .\\nAs seen, overall results of the CAND using Arnetminer are higher than the results of BDBComp. CAND\\nperformance using BDBComp is 6% less than Arnetminer. There are two main reasons behind this: first, in\\nBDBComp on average 56% of the publications belong to new authors and the majority of authors has published\\nonly one article.\\n3676HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nT able 3 .Performance Evaluation of CAND, INDi+, and INC for each year starting from 1987 to 2007 on Arnetminer\\nCollection\\nArnetminer\\nCAND INDi+ INC\\nY ear ACP AAP K ACP AAP K ACP AAP K\\n1987 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\n1988 0.99 0.99 0.99 0.99 0.98 0.98 0.97 0.96 0.96\\n1989 0.99 0.99 0.99 0.98 0.96 0.97 0.96 0.95 0.95\\n1990 0.98 0.94 0.96 0.97 0.93 0.95 0.95 0.94 0.94\\n1991 0.98 0.93 0.95 0.97 0.92 0.94 0.95 0.95 0.95\\n1992 0.97 0.92 0.94 0.96 0.91 0.93 0.94 0.94 0.94\\n1993 0.97 0.91 0.94 0.95 0.90 0.92 0.92 0.93 0.92\\n1994 0.97 0.92 0.94 0.94 0.89 0.91 0.91 0.93 0.92\\n1995 0.96 0.91 0.93 0.93 0.87 0.90 0.91 0.92 0.91\\n1996 0.96 0.92 0.94 0.94 0.86 0.90 0.89 0.91 0.90\\n1997 0.96 0.89 0.92 0.92 0.85 0.88 0.87 0.90 0.88\\n1998 0.95 0.88 0.91 0.92 0.84 0.88 0.86 0.89 0.87\\n1999 0.94 0.89 0.91 0.91 0.83 0.87 0.85 0.89 0.87\\n2000 0.93 0.90 0.91 0.91 0.82 0.86 0.84 0.90 0.87\\n2001 0.92 0.88 0.90 0.91 0.81 0.86 0.82 0.91 0.86\\n2002 0.92 0.88 0.90 0.91 0.79 0.85 0.81 0.89 0.85\\n2003 0.91 0.89 0.90 0.91 0.78 0.84 0.80 0.87 0.83\\n2004 0.92 0.89 0.90 0.90 0.76 0.83 0.80 0.85 0.82\\n2005 0.92 0.88 0.90 0.90 0.74 0.82 0.79 0.83 0.81\\n2006 0.92 0.88 0.90 0.89 0.73 0.81 0.79 0.83 0.81\\n2007 0.92 0.87 0.89 0.88 0.70 0.78 0.79 0.82 0.80\\nThe reason why CAND has better results than INDi+ and INC is that it exploits strong features;\\ncoauthors, content similarity , and self-citations. As INDi+ uses coauthor, titles, and venue similarity , some\\nwrong merges and split of the publications on the basis of these similarities may occur. Although INC tries\\nto improve the accuracy by comparing the new records with the similar existing records, they used the same\\ncoauthors, titles, and venues similarities, which did not improve the performance of the system because of the\\nabsence of discriminating training data.\\nCAND consists of four main stages in which building author profiles is a rather time-consuming step as\\ncompared to the other steps. However, it is worth noting that this is a one time process. Once the profiles of the\\nauthors are built, they are saved for subsequent retrievals and comparisons. CAND compares newly inserted\\nrecords to similar clusters in contrast to all records, which is more efficient than competing methods. T able 5\\nshows the running times of the three incremental disambiguation methods using Arnetminer and BDBComp.\\nAll experiments were performed on a personal computer with Intel(R) Core(TM)i7-5200U CPU @ 2.20 GHz\\n2.20 GHz and 8 GB memory and all methods were implemented using Python 3.6.4. All times are an average\\nrun time of the twenty-one runs given in seconds.\\n3677HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nT able 4 .Performance evaluation of CAND, INDi+, and INC for each year starting from 1987 to 2007 on BDBComp\\nBDBComp\\nCAND INDi+ INC\\nY ear ACP AAP K ACP AAP K ACP AAP K\\n1987 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\n1988 0.98 0.94 0.96 0.96 0.98 0.97 0.97 0.96 0.96\\n1989 0.97 0.93 0.95 0.96 0.96 0.96 0.96 0.94 0.95\\n1990 0.97 0.92 0.94 0.95 0.93 0.94 0.98 0.92 0.95\\n1991 0.96 0.92 0.94 0.94 0.92 0.93 0.97 0.90 0.93\\n1992 0.96 0.91 0.93 0.93 0.91 0.92 0.95 0.90 0.92\\n1993 0.95 0.89 0.92 0.92 0.90 0.91 0.95 0.90 0.92\\n1994 0.94 0.88 0.91 0.90 0.89 0.89 0.96 0.89 0.92\\n1995 0.94 0.87 0.90 0.91 0.87 0.89 0.94 0.88 0.91\\n1996 0.94 0.86 0.90 0.87 0.86 0.86 0.95 0.88 0.91\\n1997 0.93 0.84 0.88 0.88 0.85 0.86 0.93 0.87 0.90\\n1998 0.94 0.83 0.88 0.87 0.84 0.85 0.97 0.86 0.91\\n1999 0.94 0.82 0.88 0.86 0.83 0.84 0.96 0.85 0.90\\n2000 0.92 0.82 0.87 0.85 0.82 0.83 0.94 0.85 0.89\\n2001 0.92 0.81 0.86 0.82 0.81 0.81 0.92 0.84 0.88\\n2002 0.90 0.80 0.85 0.80 0.80 0.80 0.91 0.82 0.86\\n2003 0.90 0.80 0.85 0.79 0.79 0.79 0.90 0.80 0.85\\n2004 0.90 0.79 0.84 0.77 0.77 0.77 0.90 0.79 0.84\\n2005 0.90 0.79 0.84 0.76 0.77 0.76 0.89 0.78 0.83\\n2006 0.89 0.78 0.83 0.75 0.77 0.76 0.88 0.76 0.82\\n2007 0.89 0.78 0.83 0.74 0.77 0.75 0.87 0.74 0.80\\nT able 5 .Running times comparison among CAND, INDi+, and INC on Arnetminer and BDBComp.\\nMethod\\nData Set CAND INDi+ INC\\nArnetminer 434:098\\x0620:054 684:847\\x0634:524 875:421\\x0643:867\\nBDBComp 0.296\\x06 0.087 0.387\\x06 0.094 0.514\\x06 0.108\\nAs seen in T able 5, INC is the slowest among all the methods on both data sets. INC calculates prior\\nprobability distributions of all the attributes which is a time consuming step. CAND is faster than the other\\ntechniques due to its indexing structure and comparing newly inserted records with the same clusters (not\\nall records). CAND is about 1.6 to 2.1 times faster than INDi+ and INC, respectively . CAND fails on very\\nambiguous authors cases as an example is given in T able 6. In different citations, if two ambiguous authors share\\nambiguous coauthors, then these ambiguous authors are called very ambiguous authors as shown in T able 6,\\nthe first two publications belong to two different “Chun Chen” and “Bing Liu” and last two ambiguous authors\\n“Chuen-Liang Chen” and “C. Chen” share ambiguous coauthors “Biing-F eng W ang” and “B. W ang” .\\n3678HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\nT able 6 .A very ambiguous author case.\\nPublications\\nCitation Id Authors Title of Publication\\nC1 Chun Chen , Guang Qiu, Jiajun Bu, Bing\\nLiuExpanding domain sentiment lexicon\\nthrough double propagation\\nC2 Robert F. Lucas, Mary W. Hall, Jacqueline\\nChame, Chun Chen , Nastaran Baradaran,\\nY oon-Ju Lee, Bing Liu , Pedro C. DinizECO: An empirical-based compilation and\\noptimization system\\nC3 Chuen-Liang Chen , Biing-F engW ang ,\\nGen-Huey ChenA Simple Approach to Implementing Mul-\\ntiplication with Small T ables\\nC4 C. Chen , Y ang Xiao, B.W ang Bandwidth Degradation QoS for Adaptive\\nMultimedia in Wireless/Mobile Networks\\n5. Conclusions and future work\\nSelf-citations and author profiles are little or never used in the domain of author name disambiguation. In\\nthis paper, we propose CAND, which solves the incremental author name ambiguity problem in ever-growing\\nBDs using coauthors, titles, author profile models, and self-citations. CAND is an unsupervised method that\\nneither require costly training data nor a priori hidden information such as the number of ambiguous authors.\\nCAND performance is tested on two real-world benchmark data sets of Arnetminer and BDBComp. It shows\\noverall better results than incremental baseline techniques. CAND delivers an effective (in term of clustering\\nmetrics) solution to the incremental author name ambiguity problem by using coauthors, content similarity , and\\ncitation networks. In the future, we intend to use some strategies to automatically find thresholds for different\\nparameters of CAND.\\nAcknowledgment\\nW e are thankful to Higher Education Commission (HEC) of Pakistan for partial funding of this research under\\nthe Indigenous fellowship 2PS2-566.\\nReferences\\n[1] Shin D, Kim T, Choi J, Kim J. Author name disambiguation using a graph model with node splitting and merging\\nbased on bibliographic information. Scientometrics 2014; 100(1): 15-50. doi: 10.1007/s11192-014-1289-4\\n[2] Han H, Xu W, Zha H, Giles CL. A hierarchical naive Bayes mixture model for name disambiguation in author\\ncitations. In: Proceedings of the 2005 ACM symposium on Applied computing; Santa F e, NM, USA; 2005. pp.\\n1065-1069.\\n[3] Han D, Liu S, Hu Y, W ang B, Sun Y. Elm-based name disambiguation in bibliography . W orld Wide W eb 2015; 18\\n(2): 253-263. doi: 10.1007/s11280-013-0226-4\\n[4] Bollen J, Rodriguez MA, V an de Sompel H, Balakireva LL, Hagberg A. The largest scholarly semantic network\\never. In: Proceedings of the 16th international conference on W orld Wide W eb 2007; Banff, Alberta, Canada. pp.\\n1247-1248.\\n[5] Jinha AE. Article 50 million: an estimate of the number of scholarly articles in existence. Learned Publishing 2010;\\n23(3): 258-263. doi: 10.1087/20100308\\n3679HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\n[6] Hussain I, Asghar S. A survey of author name disambiguation techniques: 2010–2016. Knowledge Engineering\\nReview 2017; 32. doi: 10.1017/S0269888917000182\\n[7] De Carvalho A P , F erreira A A, Laender A H, Gon calves M A. Incremental unsupervised name disambiguation in\\ncleaned digital libraries. Journal of Information and Data Management 2011; 2(3): 289.\\n[8] Esperidiao L VB, F erreira AA, Laender AH, Goncalves MA, Gomes DM et al. Reducing fragmentation in incremental\\nauthor name disambiguation. Journal of Information and Data Management 2014; 5 (3): 293.\\n[9] Santana AF, Gonçalves MA, Laender AH, F erreira AA. Incremental author name disambiguation by exploiting\\ndomain-specific heuristics. Journal of Association of Information Science and T echnology 2017; 68(4): 931-945. doi:\\n10.1002/asi.23726\\n[10] Hussain I, Asghar S. DISC: Disambiguating homonyms using graph structural clustering. Journal of Information\\nScience 2018; Journal of Information Science, 44(6), 830-847. doi: 10.1177/0165551518761011\\n[11] W ang J, Berzins K, Hicks D, Melkers J, Xiao F et al. A boosted-trees method for name disambiguation. Sciento-\\nmetrics 2012; 93 (2): 391-411. doi: 10.1007/s11192-012-0681-1\\n[12] T ran HN, Huynh T, Do T. Author name disambiguation by using deep neural network. In: Asian Conference on\\nIntelligent Information and Database Systems 2014; Cham; 2014. pp. 123-132.\\n[13] Shoaib M, Daud A, Khiyal M. Improving Similarity Measures for Publications with Special F ocus on Author Name\\nDisambiguation. Arabian Journal for Science and Engineering 2015; 40(6) : 1591-1605. doi: 10.1007/s13369-015-\\n1636-7\\n[14] T ang J, F ong AC, W ang B, Zhang J. A unified probabilistic framework for name disambiguation in digital library .\\nIEEE T ransactions on Knowledge and Data Engineering 2012; 24 (6): 975-987. doi: 10.1109/TKDE.2011.13\\n[15] Cota RG, F erreira AA, Nascimento C, Gonçalves MA, Laender AH. An unsupervised heuristic-based hierarchical\\nmethod for name disambiguation in bibliographic citations. Journal of the American Society for Information Science\\nand T echnology 2010; 61(9): 1853-1870. doi: 10.1002/asi.21363\\n[16] W u H, Li B, Pei Y, He J. Unsupervised author disambiguation using dempster-shafer theory . Scientometrics 2014;\\n101 (3): 1955-1972. doi: 10.1007/s11192-014-1283-x\\n[17] Hussain I, Asghar S. Resolving namesakes using the author’s social network. T urkish Journal of Electrical Engi-\\nneering & Computer Science 2018; 26(1): 554-569. doi:10.3906/elk-1702-293\\n[18] Hussain I, Asghar S. Author name disambiguation by exploiting graph structural clustering and hybrid similarity .\\nArabian Journal for Science and Engineering 2018; 1-17. doi: 10.1007/s13369-018-3099-0\\n[19] Onodera N, Iwasawa M, Midorikawa N, Y oshikane F, Amano K et al. A method for eliminating articles by\\nhomonymous authors from the large number of articles retrieved by author search. Journal of the American Society\\nfor Information Science and T echnology 2011; 62 (4): 677-690. doi: 10.1002/asi.21491\\n[20] Imran M, Gillani S, Marchese M. A real-time heuristic-based unsupervised method for name disambiguation in\\ndigital libraries. D-Lib Magazine 2013; 19 (9): 1. doi: 10.1045/september2013-imran\\n[21] Zhu J, Y ang Y, Xie Q, W ang L, Hassan SU. Robust hybrid name disambiguation framework for large databases.\\nScientometrics 2014; 98 (3): 2255-2274. doi: 10.1007/s11192-013-1151-0\\n[22] Louppe G, Al-Natsheh HT, Susik M, Maguire EJ. Ethnicity sensitive author disambiguation using semi-supervised\\nlearning. In: International Conference on Knowledge Engineering and the Semantic W eb 2016; Springer, Cham,\\n2016. pp. 272-287.\\n[23] F an X, W ang J, Pu X, Zhou L, Lv B. On graph-based name disambiguation. Journal of Data and Information\\nQuality 2011; 2 (2): 10. doi: 10.1145/1891879.1891883\\n[24] W ang X, T ang J, Cheng H, Philip SY. Adana: Active name disambiguation. In: 2011 IEEE 11th International\\nConference on Data Mining 2011 (ICDM); IEEE V ancouver, BC, Canada. pp. 794-803.\\n3680HUSSAIN and ASGHAR/T urk J Elec Eng & Comp Sci\\n[25] Levin FH, Heuser CA. Evaluating the use of social networks in author name disambiguation in digital libraries.\\nJournal of Information and Data Management 2010; 1(2): 183.\\n[26] Pereira DA, Ribeiro-Neto B, Ziviani N, Laender AH, Gonçalves MA et al. Using web information for author name\\ndisambiguation. In: Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries 2009 (JCDL);\\nAustin, TX, USA. pp. 49-58.\\n[27] V eloso A, F erreira AA, Gonçalves MA, Laender AH, Meira Jr W. Cost-effective on-demand associative author name\\ndisambiguation. Information Processing & Management 2012; 48(4):680-697. doi: 10.1016/j.ipm.2011.08.005\\n[28] W u J, Ding X H. Author name disambiguation in scientific collaboration and mobility cases. Scientometrics 2013;\\n96(3): 683-697. doi: 10.1007/s11192-013-0978-8\\n[29] Müller M C, Reitz F, Roy N. Data sets for author name disambiguation: an empirical analysis and a new resource.\\nScientometrics 2017;111(3):1467-1500. doi: 10.1007/s11192-017-2363-5\\n[30] Kim J. Evaluating author name disambiguation for digital libraries: a case of DBLP . Scientometrics. 2018; 116(3):\\n1867-1886. doi: 10.1007/s11192-018-2824-5\\n[31] Abdulhayoglu MA, Thijs B. Use of ResearchGate and Google CSE for author name disambiguation. Scientometrics\\n2017; 111(3): 1965-1985. doi: 10.1007/s11192-017-2341-y\\n[32] Zhao Z, Rollins J, Bai L, Rosen G. Incremental author name disambiguation for Scientific Citation Data. In: IEEE\\nInternational Conference on Data Science and Advanced Analytics (DSAA) 2017; T okyo, Japan 2017. pp. 175-183.\\n[33] Kim J, Kim J, Owen-Smith J. Generating automatically labeled data for author name disambiguation: an iterative\\nclustering method. Scientometrics 2018; 1-28. doi: 10.1007/s11192-018-2968-3\\n[34] Hellsten I, Lambiotte R, Scharnhorst A, Ausloos M. Self-citations, co-authorships and keywords: a new approach\\nto scientists’ field mobility?. Scientometrics 2007; 72(3): 469-486. doi: 10.1007/s11192-007-1680-5\\n[35] King MM, Bergstrom CT, Correll SJ, Jacquet J, W est JD. Men set their own cites high: Gender and self-citation\\nacross fields and over time. Socius 2017; 3: 2378023117738903. doi: 10.1177/2378023117738903\\n[36] Snyder H, Bonzi S. Patterns of self-citation across disciplines (1980-1989). Journal of Information Science 1998;\\n24(6): 431-435. doi: 10.1177/016555159802400606\\n[37] Porter MF. An algorithm for suffix stripping. PROGRAM 1980; 14(3): 130-137. doi: 10.1108/eb046814\\n[38] Jaro MA. Advances in record-linkage methodology as applied to matching the 1985 census of T ampa, Florida.\\nJournal of the American Statistical Association 1989; 84(406): 414-420. doi: 10.1080/01621459.1989.10478785\\n3681', '123\\nArabian Journal for Science and\\nEngineering\\n \\nISSN 2193-567X\\n \\nArab J Sci Eng\\nDOI 10.1007/s13369-018-3099-0Author Name Disambiguation by\\nExploiting Graph Structural Clustering and\\nHybrid Similarity\\nIjaz Hussain & Sohail Asghar\\n123\\nYour article is protected by copyright and\\nall rights are held exclusively by King Fahd\\nUniversity of Petroleum & Minerals. This e-\\noffprint is for personal use only and shall not\\nbe self-archived in electronic repositories. If\\nyou wish to self-archive your article, please\\nuse the accepted manuscript version for\\nposting on your own website. You may\\nfurther deposit the accepted manuscript\\nversion in any repository, provided it is only\\nmade publicly available 12 months after\\nofficial publication or later and provided\\nacknowledgement is given to the original\\nsource of publication and a link is inserted\\nto the published article on Springer\\'s\\nwebsite. The link must be accompanied by\\nthe following text: \"The final publication is\\navailable at link.springer.com”.Arabian Journal for Science and Engineering\\nhttps://doi.org/10.1007/s13369-018-3099-0\\nRESEARCH ARTICLE - COMPUTER ENGINEERING AND COMPUTER SCIENCE\\nAuthor Name Disambiguation by Exploiting Graph Structural\\nClustering and Hybrid Similarity\\nIjaz Hussain1·Sohail Asghar1\\nReceived: 16 March 2017 / Accepted: 4 February 2018\\n© King Fahd University of Petroleum & Minerals 2018\\nAbstract\\nAuthor name ambiguity occurs when multiple authors share a common name and an author writes one’s name in many ways.\\nThis hinders the quality of information retrieval and correct attribution to authors in bibliographic databases. Despite much\\nresearch in the past decade, the author name ambiguity problem remains largely unsolved. Outstanding issues include limited\\ncapabilities (solve only homonyms or synonyms), require extra information (Web or user feedback), actual number of authors\\nKin advance and not scalable. In this paper, a method called GCLUSIM is proposed which uses graph structural clustering\\nand proposed similarity measure to resolve ambiguous authors. GCLUSIM preprocesses citation data set and constructs\\nco-authors graph. Graph-based structural clustering is applied to the constructed graph to identify hub nodes, outliers, and\\nclusters of nodes. It resolves homonyms by splitting these clusters if the feature vector similarity between these clusters is\\nless than the predeﬁned threshold and synonyms by exploiting proposed similarity. Finally, it disambiguates sole authors by\\ncomparing name and feature vector similarities with the disambiguated clusters. Experiments are performed with Arnetminer\\nand BDBComp to validate the performance of the GCLUSIM. Results show that GCLUSIM is scalable, overall better in\\nperformance than baselines and the number of clusters found is close to the ground truth clusters.\\nKeywords Author name disambiguation ·Co-authors graph ·Homonym resolver ·Synonyms resolver ·Sole author resolver\\n1 Introduction\\nSeveral people in the world may have exactly the same name,\\nand it is also possible that one may write one’s name in many\\ndifferent ways—abbreviations, changing the order and omit-\\nting some name part. The former is called homonym, and\\nthe latter is called synonym. For example, when we search\\nin the DBLP (Digital Bibliography & Library Project) an\\nauthor name “Wei Wang”, it turns out eighty-ﬁve different\\nauthors that have exactly the same name “Wei Wang”. Simi-\\nlarly, when we search an author name “W. Wang”, it turns out\\nmore than a thousand likely matches. More or less this is the\\nsituation in all bibliographic databases. This is considered a\\nsubproblem of entity resolution problem among information\\nretrieval research community [ 1–3]. Author name ambigui-\\nties can cause wrong attributions and incorrect search results\\nBIjaz Hussain\\nfa14-pcs-006@student.comsats.edu.pk\\n1Department of Computer Science, COMSATS Institute\\nof Information Technology, Islamabad, Pakistan[4–6]. The methods that resolve these author name ambigui-\\nties are called Author Name Disambiguation (AND) systems.\\nNumerous AND methods were proposed in the last decade\\nthat used machine learning and data mining techniques [ 1–\\n15]. However, many of these dealt only with the homonyms\\n[6,12,16], some of them disambiguated only the synonyms\\n[17] and few others heavily relied on ancillary evidence such\\nas email, afﬁliation or related web pages, or required some\\nhidden information such as the number of ambiguous authors\\nor groups [ 2,13,18]. Some others required costly training data\\nto train their model parameters [ 9,19,20]. Still others were\\nnot scalable and produced false positives that severely affect\\ntheir performance [ 5,12,16]\\nIn this paper, we propose “Author Name Disambiguation\\nby Exploiting Graph Structural Clustering and Hybrid Sim-\\nilarity Index” called GCLUSIM, to resolve these problems.\\nGCLUSIM is a graph-based technique in that citation data set\\nis preprocessed and ambiguous author blocks are identiﬁed,\\nas given in [ 8]. Co-authors graph is constructed using the pre-\\nprocessed citation data and “SCAN: A Structural Clustering\\nAlgorithm for networks” is used to identify hub nodes, out-\\nliers, and clusters of nodes [ 21]. SCAN is used due to three\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nreasons, in contrast to other graph methods, it is able to detect\\noutliers, is scalable as its complexity is O(e), where eis the\\nnumber of edges in co-authors graph and is capable of iden-\\ntifying overlapping communities unlike other community\\ndetection algorithms. Homonyms are resolved by splittingidentiﬁed clusters of nodes if the similarity between theseclusters is less than a predeﬁned threshold, details are given\\nin Sect. 3.2. Synonyms are solved using proposed hybrid\\nsimilarity index. When two names are compatible and theirhybrid similarity index is greater than a predeﬁned threshold,\\nthen these nodes are merged into one node, as described in\\nSect. 3.3. Finally, sole authors are disambiguated by com-\\nparing the similarity between feature vectors of two similar\\nnames. GCLUSIM only uses the co-authors information and\\nabstracts to fully resolve the author name ambiguity prob-lems. GCLUSIM simultaneously resolves homonyms andsynonyms. In the last, step sole authors are resolved that\\nmany previous studies did not address [ 6,12,17].\\nGHOST presented by [ 12] used valid path-based similar-\\nity between two nodes that is in the order of\\nO((|R\\n2|)×(|V|+| E|)) (1)\\nwhere Vis the vertices, Eis the edges and Ris the set of\\nnames to resolve. Another method ADANA given by [ 16]\\nused pairwise factor graph that was also intractable due to thecalculation of normalization factor that made their method\\ncomplexity to exponential to the number of nodes in the\\ngraph. Outliers are nodes that have only one or a few cita-tions common with ambiguous author; details are presented\\nin Sect. 3.2. Similarly GFAD proposed by [ 5] used Johnson’s\\nsimple cycle enumeration algorithm [ 22] that has complexity\\nin the order of\\nO((n+e)(c+1)) (2)\\nfornvertices, eedges and celementary cycles in the graph.\\nIn the uniﬁed probabilistic framework, [ 3] transformed the\\ncontent and structure information about citation attributesinto the Hidden Markov Random Fields as feature func-\\ntions and formed an objective function to ﬁnd the hidden\\nparameters. However, this method worked well on smallambiguous author groups. As the number of ambiguousauthors increased, it was unable to correctly predict the num-\\nber of ambiguous authors ‘k’.\\nTo evaluate the performance of GCLUSIM, we compare\\nit with three unsupervised name disambiguation methods [ 5,\\n12,23] using collection of citations from two real-world bibli-\\nographic databases Arnetminer and BDBComp. GCLUSIMperformance is overall better than these systems from the\\nperspective of representative clustering evaluation metrics\\ndespite the fact that GCLUSIM uses only the co-authors andabstract. In summary, the main contributions of this work are:– Graph structural clustering is employed to solve the\\nhomonyms in contrast to the existing techniques thateither used cycle enumerations or graph paths. To the\\nbest of our knowledge, the identiﬁcation of outliers for\\nthe author name ambiguity in graph-based methods hasbeen addressed for the ﬁrst time,\\n– A hybrid similarity index is proposed, to resolve the syn-\\nonyms, in which information of syntactic similarity and\\ngraph geodesics between vertices is exploited,\\n– A complete author name disambiguation method,\\n“GCLUSIM,” is presented that requires no costly training\\ndata, no hidden information about number of unknownclusters, resolves sole authors and simultaneously solves\\nboth author name ambiguity problems, and\\n– Extensive experiments on real-world data sets of Arnet-\\nminer and BDBComp are performed to demonstrate theeffectiveness of the GCLUSIM in Sect. 4.4.\\nThe rest of this paper is structured as follows. Some\\nrelated works are outlined, critically reviewed and analyzed\\nat Sect. 2. In Sect. 3, GCLUSIM architecture and its work-\\ning is discussed in detail. We have tested this approach onreal-world data sets from Arnetminer and BDBComp to val-idate its performance against baselines, as given at Sect. 4.\\nWe conclude with a discussion of contributions and future\\nresearch directions in Sect. 5.\\n2 Related Work\\nExisting AND methods can be categorized as supervised\\n[6,20,24], unsupervised [ 3,10,13,25], semi-supervised [ 26],\\nand graph-oriented approaches using graph models or socialnetworks [ 5,12,16,27].\\nIn [20], Wang et al. presented a boosted tree classiﬁca-\\ntion method in that they ﬁlter authors on the basis of nameand afﬁliation matching. Similarity scores for different pub-\\nlication features were calculated, the false rate was used for\\nauthor screening, and in the last stage boosted tree classiﬁerwas applied to the manually crafted data set. Two extreme\\nlearning machine-based algorithms for author name disam-\\nbiguation were proposed in [ 6]. However, this method needed\\nto train a new classiﬁer model for every ambiguous name. Adeep neural network-based approach to automatically learn\\nthe features and disambiguate the authors from the data set is\\nproposed in [ 24] . They utilized the multicolumn deep neural\\nnetwork technique to improve the generalization capabili-\\nties of the system that is very similar to ensemble method\\nbagging.\\nCota et al. in [ 10] presented a two-step heuristic-\\nbased hierarchical clustering algorithm. They solved both\\nhomonyms and synonyms by exploiting similarities in cita-tion attributes. The clusters were generated that had some\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\ncommon co-author names among citations. This step gener-\\nated fragmented but very pure clusters. Then these clusterswere pairwise compared and merged if they have a simi-\\nlarity greater than some threshold in titles and venues. This\\nprocess was iterative, and successive iterations had a syner-gistic effect on the disambiguation of authors, as more titlesand venues were combined in merged clusters. A discrimi-\\nnation function that predicted true authors and false authors\\nusing logistic regression on Web of Science data was pro-posed in [ 13]. They extracted true author citations from a\\nlarge amount of retrieved citations by using two stage ﬁlter-\\ning. This method is not good for citations whose subject ﬁeldsand afﬁliation addresses do not vary too much. Wu et al. in\\n[25] proposed an algorithm that used Dempster–Shafer The-\\nory (DST) in combination with Shannon Entropy (SE) forauthor disambiguation. Some high-level features and theirco-relation similarities were calculated and fused using DST\\nand SE. In clustering stage, they used three different con-\\nvergence conditions for clustering algorithm, namely—thepreset number of clusters, the number of available evidence\\nand the distance between clusters.\\nA uniﬁed probabilistic framework to address the\\nhomonyms was proposed in [ 3]. They formalized the dis-\\nambiguation problem using Markov random ﬁelds. They\\nautomatically estimate the number of unknown ambiguousauthors and the required unknown parameters. Zhu et al. [ 26]\\nproposed a hybrid name disambiguation framework that not\\nonly used the implicit information, but also web page genre\\ninformation. This framework consists of two main steps: webpage genre identiﬁcation and re-clustering model. A method\\ncalled “GHOST” was presented in [ 12], in which they mod-\\neled the relationships among publications using undirectedgraphs. They solved homonym problem by iteratively ﬁnd-\\ning valid paths, computing similarities, clustering with the\\nhelp of afﬁnity propagation algorithm and in the last stepusing user feedback as a complementary tool to enhancethe performance. This approach does not handle outliers and\\nfails in the case of solo authors. Two multilevel graph par-\\ntitioning algorithms for author name disambiguation wereproposed in [ 23]. First is the multilevel graph partitioning\\nalgorithm (MGP) and the second the multilevel graph parti-\\ntioning and merging (MGPM) algorithm. MGPM repeatedlymerged the sub-graphs until the number of sub-graphs is\\nequal to the given number of clusters ( k) in the gold stan-\\ndard. This method requires the hidden information about thenumber of unknown authors in a cluster. Shin et al. in [ 5]\\nproposed a graph framework for author name disambigua-\\ntion “GFAD” that exploited co-authors and titles. Namesake\\nproblem was resolved by splitting the vertex that was com-mon to multiple non-overlapping cycles, so that each cycle\\ncorrespond to a unique author. The heteronymous name prob-\\nlem was handled by merging multiple author vertices into oneby identifying those vertices that actually represented a sin-gle author with different names. It resolves sole authors by\\ncomparing similarity among outliers with the help of cosinesimilarity measure. Shoaib et al. proposed a syntax-based\\nsimilarity measure for AND in [ 28]. Some authors pro-\\nposed ontologies to better search and retrieve data [ 29–31].\\nRecently, community detection algorithms are being used forinformation recommendation and in similar domains [ 32,33].\\nSemantic similarity measures and graph structural clustering\\nare used in many domains such as Biomedical Informat-ics, GeoInformatics, Computational Linguistics, Community\\nDetection, and in Natural Language Processing. Similarly,\\nwe are using graph-based semantic similarity measure andgraph structural clustering in author name disambiguation\\ndomain because we believe it would prove to be useful in\\nthis domain.\\nGCLUSIM is different to other methods because it\\nrequires no training data while [ 6,20,24] required a lot of\\ntraining data, it solves homonyms and synonyms simultane-\\nously, whereas [ 3,12,13] solves only homonyms, it does not\\nrequire user interaction as required in these methods [ 26,34],\\nit does not need number of clusters K, and it solves sole\\nauthors, whereas [ 10,12,\\n26] does not. GCLUSIM is congru-\\nent to methods that used community detection and semantic\\nsimilarity in some different domains [ 29–33].\\n3 GCLUSIM Details\\nGCLUSIM architecture consists of following main stages—\\npreparation and graph construction, homonym resolver, syn-\\nonym resolver and sole authors resolver algorithms, as shown\\nin Fig. 1. Details of all these stages are given in subsequent\\nparagraphs.\\n3.1 Preparation and Graph Construction Stage\\nIn the preprocessing, all the citation data set is extracted and\\ntokened into authors, title, and venue terms. Stop words are\\nremoved from the titles and stemmed using Porter stemmer[35]. In blocking stage, similar names are grouped together\\nin the candidate classes if their similarity is higher than some\\npredeﬁned threshold value. The blocking stage is importantas it affects the computations in the later stages of the namedisambiguation algorithms. Suppose if there are two author\\nnames lists of AandB, for every name a∈A, a set of author\\nnames b\\n1,b2,...bn∈Bis found. In this way, the block-\\ning stage returns Mnumber of blocks if given Nauthors, as\\npictorially shown in Fig. 2. A block in author name disam-\\nbiguation is all name variants which are potential candidatesfor ambiguous author names.\\nThe computational complexity is O(N\\n2)forNauthors\\nif we do not use blocking, whereas, blocking considerablyreduces computational complexity to O(R|S|), where Ris\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nSole Author ResolutionAmbiguous Authors \\nHomonyms Resolution Synonyms ResolutionBlocking\\nPreprocessing Graph ConstructionCo-Authors GraphVertices\\nEdgesTokenization\\nStopword Filtering\\nStemming\\nRaw Citations DS\\nGraph Clustering\\nFeature Vector\\nSimilarity\\nGraph SplittingFeature Vector\\nSimilarity\\nGraph Merging Graph MergingJaro-Winkler\\nSimilarity\\nHybrid Similarity\\nFig. 1 The GCLUSIM Architecture\\nthe number of blocks and Sis the their size. If we set a very\\nhigh threshold for blocking, then it produces low recall butvery pure blocks. In contrast, if we set a very low threshold,then it produces high false positives. We randomly selected\\n10% similar name pairs, and 10% name variant pairs from\\neach block of our ambiguous author data set and empiricallyfound the threshold 0.8 that produces optimal author name\\nblocks.\\nThe citation data set provides basic features like names\\nof authors, titles, venues and year of publications. However,\\nas mentioned in the previous section, the most inﬂuential\\nfeature among these is the co-authors for solving the problemof author ambiguity [ 5,10,36]. In GCLUSIM, an author is\\nrepresented by a node that has its unique id for identiﬁcation,\\nits name (not unique), and publications which that author has\\npublished. The edges modeled the bidirectional relationshipbetween two co-authors. Every citation that has nnumber\\nof authors (nodes) exactly produces Enumber of edges, as\\ngiven in Eq. 3. An example citation data set is shown in\\nTable 1.\\nExtracted nodes and constructed graph can be shown in\\nFig.3, where there are twelve author nodes and twenty edges\\nthat are created from six citations. Chang Chen is a node thatis common to ﬁve citations and in one citation it is a sole\\nauthor, so an isolated node is created for it in the co-author’sgraph.\\nE=(n)∗(n−1)\\n2(3)\\n3.2 Proposed Homonym Resolver Algorithm\\nIt is assumed that different homonyms have a different set of\\nco-authors (social circles) and work in the related domain.Different authors with the same name seldom work in the\\nsame organization or social circle. So, they should form quite\\ndifferent author communities. In GCLUSIM, a communityis generated from each citation and thus each communitydenotes the co-authors for each citation that is the small-\\nest social circle in the academic domain. For example, in\\n‘citation 4’ there are four co-authors B,D,Cand Athen\\nthe upper four nodes and their relationships are constructed\\nas shown in Fig. 3and when we add more citations to this\\ngraph, it becomes wider and bigger. Finally, when we con-struct the graph of all sample citations of Table 1, it looks\\nlike as depicted in Fig. 3. With the help of co-authors graph,\\nit is possible to infer a social circle of an author from one’sco-authors by ﬁnding shared communities.\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nA\\n1. C Chen\\n…………..………….\\nM. Ijaz HussainB\\n………….\\n1001. Ijaz Hussain\\n…………….…………...\\nn. Chang Chen1789. C D Chen\\n2897. I Hussain\\n2898. H Ijaz \\nIjaz Hussain’s Block C Chen’s Block………….………….\\nFig. 2 Example of ambiguous author blocking\\nIn GCLUSIM we construct a feature vector from abstract\\nkeywords after removing stop words and then stemming\\nthese words. GCLUSIM assumes that an author linked withmultiple social circles contains mixed information for sev-\\neral authors if the similarity between feature vectors of two\\nclusters is <0.2. This threshold is chosen empirically by\\nvarying its value in small increments and ﬁnding the optimal\\nvalue of it. It is worthwhile to mention here that this step\\nis needed only one time. GCLUSIM splits that node and itsinformation into a number of different nodes that are presentin non-overlapping communities. GCLUSIM considers each\\nnon-overlapping community emanating from the same node\\nas a different social circle of an author if its feature vectorsimilarity is less than the threshold. GCLUSIM community\\ndetection and split algorithm, pseudocode is described in\\nAlgorithm 1and its details are given in Sect. 3.2.AB\\nC D\\nE\\nF\\nGH I\\nJ KS\\nAmbiguous Author ohoh mbbbigP\\nFig. 3 An example of co-authors graph constructed from sample cita-\\ntion data set\\n3.2.1 Community Detector\\nIn the co-authors graph, an edge represents a co-author\\nrelationship between the two authors. The GCLUSIM’s com-\\nmunity detection algorithm is based on “SCAN: Structural\\nClustering Algorithm for Networks” to detect different com-munities in the co-authors graph. This step is given in\\nAlgorithm 1at line 1. In this section, we give brief description\\nof necessary terms used in our method so that GCLUSIM canbe understood in a self-contained fashion. However, inter-\\nested readers are requested to read [ 21] for further technical\\ndetails.\\nIn homonym resolution algorithm, when we use SCAN\\non co-authors graph it outputs clusters of nodes (communi-\\nties), hub nodes and outliers. The community consists of a\\nset of nodes from this co-authors graph. We deﬁne hub as thenode that is involved in many social circles in the co-authors\\ngraph. Hub node is the potential homonym that needs dis-\\nambiguation. Outliers are nodes that only contribute one or\\nTable 1 An example citations data set\\nNo. Citations data Author identities\\n1 Chang Chen : author name disambiguation—a review: SEIT, 2015: pages 29–38 S\\n2 Chang Chen, Tasawar Ali : frequent graph pattern mining comparison: KDD, 2016: pages 124–140 A , E\\n3 Chang Chen, F. Ali, M. Ibrahim : graph utilization in author name ambiguity problem: FIT, 2012: pages 78–87 A, F, G4 Farman Ali, Shakeel Ali, M. Rehan, Chang Chen: graph-based author name disambiguation framework: OIR,\\n2014: pages 329–339B,D ,C ,A\\n5 Chang Chen, Muaz Shah, Bilal Ahmad, Sanab Gul, Maham Ali : HAPT:HeArt failure prediction tool: FIT,\\n2016: page 295–308A,I,J,K,H\\n6 M. Rehan, C. Chen: nonlinear control for hot air blower system: scientometrics, 2014: pages 319–329 C, P\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nES HubSole author\\nOutlier\\nF\\nGB\\nC D\\nHub\\nH I\\nJ KACluster 1 nodes\\nCluster 3 nodesCluster 2 nodesP\\nFig. 4 An example of detected communities, hubs, and outliers after\\napplying SCAN on co-authors graph\\nfew publications with hub node. Clusters are vertices that are\\nmore similar to one another and different to other clusters. Inexample graph, detected communities, hub, and outlier canbe seen in Fig. 4.\\nIn this ﬁgure, there are three clusters of nodes (communi-\\nties), one hub and one outlier. The ﬁrst, second and thirdcluster consists of nodes (P,B,C,D),(H,I,J,K)and\\n(F,G), respectively. The hub is node (A), and the outlier\\nis node (E).\\n3.2.2 Community Splitter\\nA hub node that has multiple non-overlapping communities\\ncontains mixed information for several homonym authors.Feature vector of clusters is constructed as given in Algorithm\\n2.\\nGCLUSIM splits the information of these authors along\\nthe non-overlapping communities if its abstract feature vec-\\ntor similarity is <0.2. This part starts from the line 2 of\\nAlgorithm 1, where the hub node publications list is retrieved\\nfrom the graph. Similarly, publications of all cluster nodes areretrieved and saved in a list (lines 6–7). If feature vector sim-\\nilarity is less than the threshold, then for each such homonym\\nthe intersection of hub publications and community publica-tions is found. A new node is created in the co-authorship\\ngraph for each community detected that has the same name\\nas that of the hub node and has an identity one more than inthe current nodes in the co-authors graph, this newly created\\nnode has the publication list that is found at line 12. Like-\\nwise, for all detected communities that have feature vectorsimilarity less than the threshold new nodes in the co-authorsAlgorithm 1: Homonyms Resolver Algorithm\\nData :Co−authors Graph (CG)\\nResult :Homonym Reso l ved Graph (HG)\\n1[CommunitiesList ,Hub,Outliers ]← − runSCAN (CG)\\n2HubPubs ←− get Pubs (Hub)\\n3FVh←− get FV (Hub)\\n4i,j←− ∅\\n5forcommunity ∈CommunitiesList do\\n6 communityPubs ←− ∅\\n7 FVc←− get FV (Community )\\n8 FV.similarity ←− sim(FVC,FVh)if\\nFV.similarity <0.2then\\n9 fornode∈community do\\n10 communityPubs ←−\\ncommunityPubs ∪get Pubs (node)\\n11 end\\n12 HN iPubs←− HubPubs ∩communityPubs\\n13 HN iName ←− get Name (Hub)\\n14 HN iId←− getLength (CG)\\n15 CG.Inse rtNo de (HN iId,HN iName ,HN iPubs)\\n16 UpdateGraph (CG,community )\\n17 HubPubs ←− HubPubs \\\\communityPubs\\n18 i←− i+1\\n19 else\\n20 community ←− community +1\\n21 end\\n22 end\\n23 foroutlier ∈outliers do\\n24 outlier Publications ←− ∅\\n25 FVO←− get FV (outlier )\\n26 FV.similarity ←− sim(FVO,FVh)\\n27 ifFV.similarity <0.2then\\n28 OjPubs←− HubPubs ∩outlier Pubs\\n29 OjName ←− get Name (Hub)\\n30 OjId←− getLength (CG)\\n31 CG.Inse rtNo de (OjId,OjName ,OjPubs)\\n32 UpdateGraph (CG,outlier )\\n33 HubPubs ←− HubPubs \\\\outlier Pubs\\n34 j←− j+1\\n35 returnUpdatedGraphCG\\n36 else\\n37 outlier ←− outlier +1\\n38 end\\n39 end\\nAlgorithm 2: Feature Vector Constructor Algorithm\\nData :Clusters of Nodes\\nResult :FeatureV ector (FV)\\n1Inititlize FV ←− 0\\n2while Clusters do\\n3 text←− get.Abstract (Cluster )\\n4 tokens ←− get.To ke ns (text)\\n5 fortoken ∈tokens do\\n6 iftoken /∈FV then\\n7 FV.Add(token)\\n8 token ←− token +1\\n9 else\\n10 token ←− token +1\\n11 end\\n12 end\\n13 end\\n14\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\ngraph are created and the graph is updated after every new\\nnode insertion. Publications of hub node are also updatedaccordingly. In graph update, some edges to the hub node\\nto existing communities are removed and some new edges\\nto newly created nodes are created (lines 12–22). The sameprocedure is repeated for all outliers that have feature vectorsimilarity <0.2, as done in the case of all communities (lines\\n23–39). This whole process is shown in Fig. 5.Here hub node Ais split into three new nodes (A1,A2,\\nA3), as there are only three clusters of nodes (commu-\\nnities) that have feature vector similarity <0.2. Cluster\\n(P,B,C,D)and cluster (F,G)have feature vector simi-\\nlarity>0.2, so they are not split. GCLUSIM’s this feature\\nis very useful for the detection of the same authors (likePh.D. advisors) that have no overlapping between some of\\ntheir co-authors, as many Ph.D. students work with their\\nadvisors as co-authors and then go to their home coun-\\nFig. 5 An example of\\nhomonym resolution\\nB\\nC D\\nH I\\nJ KSSole authorB\\nC D\\nH I\\nJ KA1\\nA3Three homonyms\\nF\\nGA2 E A2 EP\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\ntry and never co-authors again with them. In this way, the\\nhomonym problem is solved using graph structural cluster-ing and graph operations. The use of SCAN here brings three\\nbeneﬁts (1) It detects outliers that are not possible with exist-\\ning graph-based methods. (2) It makes algorithm scalable asits computational complexity is much lower than other com-peting graph methods, as given in Sect. 1. (3) In contrast to\\nother community detection algorithms, it is useful to identify\\noverlapping communities in the co-authors graph\\n3.3 Proposed Synonym Resolver Algorithm\\nThe other name ambiguity is the synonym that occurs whenan author writes her name in different ways as explained in thefollowing paragraphs. In academic domain, authors often ﬁll\\nout their names in several forms like abbreviations, sequence\\nchanging, an omission of some part of their name, etc. So,the same author may appear in DL with different forms of\\nher name.\\n3.3.1 Proposed Hybrid Similarity Index\\nThe synonyms are identiﬁed with the help of proposed hybrid\\nsimilarity index that is given in Eq. 5; hereafter, we simply\\ncall this as hybrid similarity. We used Jaro-Winkler similarityfor ﬁnding similarity between two author name strings. This\\nmeasure proved to be useful for short strings such as names\\n[37]. The similarity score is normalized in a range of [0–1],\\nwhere 0 means no similarity and 1 means the exact same\\nstrings. Given two name strings n\\n1andn2, their distance is\\ncalculated as\\nd(n1,n2)=1\\n3∗/parenleftbiggc\\nn1+c\\nn2+c−m\\nc/parenrightbigg\\n(4)\\nwhere cis the number of common characters in two name\\nstrings. A character is considered as a common character atposition iin the string n\\n1has to be within the Hwindow\\nof the equivalent jth character in the string n2.H e r e H=/floorleftBig\\nmax(|n1|,|n2|)\\n2/floorrightBig\\n−1. Similarly mis equal to the number of\\ncharacters matched from the window but not at the same\\nindex divided by 2. After computing syntactic similarity, weﬁnd the geodesic distance between two compared nodes andpass these values to our proposed hybrid similarity index.\\nCompatible names are those names that are either fully\\npart of another name or they are having same initial of theﬁrst and last name. For example, a name “John Smith” is\\nfully part of another name “John Michael Smith” and a name\\n“John Gold” is not having the same initials as that of “JohnSmith”. Similarly, another pair of names is “J. Smith” and\\n“J. Smith Gold” that has the same initials. So, the ﬁrst and\\nthird pair of names is compatible and the second pair is notcompatible.Deﬁnition 1 HYBRID SIMILARITY\\nLetαis the Jaro-Winkler similarity between two compat-\\nible compared names and βis the geodesic distance between\\nthem. Then hybrid similarity will be given by\\nHybrid Similarity =1\\n2/parenleftbigg\\nα+1\\n1+β2/parenrightbigg\\n(5)\\nIf two names are compatible then we ﬁnd similarity between\\nthem using the formula given in Eq. 5. The co-authors graph\\nindicates the author’s past and present connections with\\nother researchers. So, if apparently different researchers in\\nco-authors graph have greater hybrid similarity then it isprobably the same authors. In GCLUSIM, the nodes that\\nhave hybrid similarity >0.5 are considered to be the same\\nauthors else different ones. GCLUSIM ﬁrst searches for thenodes that have compatible names and applies hybrid sim-ilarity on these name pairs as given in Eq. 5. Pseudocode\\nfor computing hybrid similarity and ﬁnding similar names is\\ndescribed in Algorithm 3(lines 7–16).\\nAlgorithm 3: Synonyms Resolver Algorithm\\nData :Homonym Reso l ved Graph (HG)\\nResult :Synonym Resol ved Graph (SG)\\n1HubName ←− HG.get Name (Hub)\\n2all Author Names ,similarNamesList ←− ∅\\n3all Author Names ←− HG.get AllName (node)\\n4comparisonName ←− HubName\\n5nameTokens ←− comparisonName .split()\\n6all Author Names .delete (HubName )\\n7forauthor ∈all Author Names do\\n8 ifisAuthorCompatible then\\n9 α←−\\ngetJWSimilarity (comparisonName ,author )\\n10 β←−\\ngeodesicDistance (comparisonName ,author )\\n11 hybridSimilarityIndex ←−\\ngethybridSimilarityIndex (α, β)\\n12 ifhybridSimilarityIndex >0.5then\\n13 similarNamesList .append (author )\\n14 end\\n15 end\\n16 end\\n17 forname ∈similarNamesList do\\n18 mPub lic atio ns ←− ∅\\n19 mPub lic atio ns ←−\\ncomparisonNamePublications ∪namePublications\\n20 ifnameId <comparisonNameId then\\n21 mName ←− nameName\\n22 mId←− nameId\\n23 end\\n24 mName ←− comparisonName\\n25 mId←− comparisonId\\n26 HG.Inse rtNo de (mId,mName ,mPub lic atio ns )\\n27 deleteNode (comparisonNode )\\n28 UpdateGraph (HG)\\n29 returnUpdatedGraphHG\\n30 end\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nTwo synonym nodes are merged into one node that has\\npublications list of both of these nodes and id of the full name,whereas the other name node is deleted from the graph. Pseu-docode for merging similar names is described in Algorithm\\n3(lines 17–30).An example of synonym merging is shown in Fig. 6, since\\nnode Pand node A1 are compatible names and hybrid simi-\\nlarity between them is >0.5. So in this case, they are merged\\ninto one node A1 and the other node Pis deleted from the\\nco-authors graph and the edges are updated accordingly. Theproposed hybrid similarity not only considers the syntactic\\nFig. 6 An example of synonym\\nresolution\\nB\\nC D\\nH I\\nJ KSSole authorB\\nC D\\nH I\\nJ KA1\\nA3Three homonyms\\nF\\nGA2 E A2 EP\\nB\\nC D\\nH I\\nJ KSSole authorB\\nC D\\nH I\\nJ KA1\\nA3Three homonyms\\nF\\nGA2 E A2 ENode \"P\" and Node \"A1\" are having HSF >0.5 so merged.\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nsimilarity, but it also takes into account the close relationship\\nof nodes in co-authors graph. This similarity may also be ableto detect some typo errors and improves performance.\\n3.4 Proposed Sole Author Resolver Algorithm\\nDespite the fact that the resolution of the sole authors is nec-essary for the completeness of clusters and for the completesolution of AND, the majority of previous studies [ 2,11,34]\\nignored the sole author’s cases. So, for the sake of clus-\\nter completeness and complete solution to the author nameambiguity problem, GCLUSIM ﬁnds the nodes that have no\\nco-authors and adds them into the sole author’s list. It then\\ntakes one by one these sole authors and merges them withthe most similar node on the basis of abstract feature vector\\nsimilarity if it is >0.2. We used Jaccard index for feature\\nvector similarity because it is computationally fast, as givenin Eq. 6. The sole author resolver algorithm proceeds as fol-\\nlows. It ﬁnds all sole authors in the graph by selecting those\\nnodes in co-authors graph that has no co-authors (lines 4–8).\\nThen it takes one by one sole authors and ﬁnds their simi-lar names in the co-authors graph that have similarity higher\\nthan 0.8 to sole author nodes (lines 13–17) and selects the\\nnode which has the highest value of similarity between thefeature vectors of two nodes provided that it sufﬁce the con-\\ndition of minimum threshold of 0.2. Finally, sole authors\\nare deleted from the co-authors graph by merging them withthe selected node between their abstract feature vectors andupdate the co-authorship graph (lines 19–34). An example of\\nmerging the sole author node to most similar node is shown\\nin Fig. 7, where it is found that sole author node Sand similar\\nauthor node A1 have similarity >0.2. So, these two nodes\\nare merged and graph is updated accordingly.\\n4 Experiments and Discussions\\nIn this section, the performance of GCLUSIM is compared\\nwith three baseline methods using Arnetminer and BDB-Comp data set in the form of clustering evaluation metrics.\\n4.1 Data Set: Arnetminer and BDBComp\\nTwo real-world data sets from Arnetminer and BDBCompare used to measure the effectiveness of GCLUSIM. Thestatistics of the Arnetminer data set are given in Table 2and\\nmore details are found online at Arnetminer . The original\\nversion of this collection was created by [ 16]. Then, [ 3] man-\\nually checked, labeled and included more ambiguous authors\\nto expand this data set. It is used with slight variations in many\\nAND studies [ 3,5,25]. Subsets of this data set have also been\\nused in other works [ 2,4,6,8,10,19].Algorithm 4: Sole Authors Resolver Algorithm\\nData :Synonyms Resol ved Graph (SG)\\nResult :Disambiguated Graph (DG)\\n1soleAuthorsList ←− ∅\\n2all AuthorsList ←− ∅\\n3similar AuthorsList ←− ∅\\n4fornode∈SG do\\n5 coAuthors ←− node.getCoAuthors ()\\n6 ifcoAuthors <one then\\n7 soleAuthorsList ←−\\nsoleAuthorsList ∪SG.get Name (node)\\n8 end\\n9 all AuthorsList ←− SG.get AllNames (node)\\n10 forauthor ∈soleAuthorsList do\\n11 forcomparisonName ∈all AuthorsList do\\n12 similarity ←−\\nnameSimilarity (comparisonName ,author )\\n13 ifsimilarity >threshold and\\nauthorCompatible then\\n14 similar AuthorsList .append (author )\\n15 end\\n16 end\\n17 end\\n18 end\\n19 forsoleAuthor ∈soleAuthorsList do\\n20 sFV←− CG.get FV (soleAuthor )\\n21 maxNode ,maxValue ←− null\\n22 forauthor ∈similar AuthorsList do\\n23 aFV←− CG.get FV (author )\\n24 FV.similarity ←− sim(sFV,aFV)\\n25 ifFV.similarity >0.2then\\n26 mergNode ←− author\\n27 maxValue ←− FV.similarity\\n28 maxNode ←− soleAuthor\\n29 else\\n30 soleAuthor ←− soleAuthor +1\\n31 end\\n32 returnUpdatedGraphSG\\n33 end\\n34 end\\nBDBComp is relatively a small data set of 363 records\\nbelonging to 184 distinct authors, but it is very difﬁcult to\\ndisambiguate as some authors have only one citation record.\\nBDBComp data set statistics are given in Table 3, and this\\ncollection has also been used in many author name disam-\\nbiguation tasks.\\nIn GCLUSIM, only co-authors information and abstract\\nof this data set are used for the complete solution of theauthor name ambiguity problem. GCLUSIM considered two\\nauthors same if their full name is identical in citation data\\nset. Abstract keyword feature vectors are generated for allpublications in the data set. The Jaccard index\\nJaccard (A,B)=A∩B\\nA∪B(6)\\nbetween these feature vectors is used for resolving homonyms\\nand sole authors.\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nFig. 7 An example of sole\\nauthors resolution\\nNode \\'S\\' and Node \\'A1\\' are having  \\nsimilarity >0.2 \\nSSole authorB\\nC D\\nH I\\nJ KA1\\nA3Three homonyms\\nF\\nGA2A2 EP\\nB\\nC D\\nH I\\nJ KA1\\nA3Three homonyms\\nF\\nGA2 E\\n4.2 Evaluation Metrics\\nThree metrics—K-metric, pairwise-F1, and cluster-F1—are\\nused to measure the effectiveness of GCLUSIM, as deﬁned\\nin [38].\\nK-metric is deﬁned as the geometric mean of the Average\\nCluster Purity (ACP) and the Average Author Purity (AAP).\\nACP evaluates the purity of the algorithm-generated clusters\\nwith respect to the gold standard reference clusters, so it mea-sures the amount of data misclassiﬁed into the wrong clusters\\nby checking whether the generated clusters include only the\\npublication records belonging to the reference clusters. AAPevaluates the level of splitting author information into severalclusters, so it checks how fragmented the generated clusters\\nare. ACP, AAP and K-metric are expressed in Eq. 7.\\nACP=1\\nNR/summationdisplay\\nr=1S/summationdisplay\\ns=1n2\\nrs\\nnr,AAP=1\\nNS/summationdisplay\\ns=1R/summationdisplay\\nr=1n2\\nrs\\nns,\\nK=√\\nACP∗AAP(7)\\nHere, Ndenotes the size of the citations in the collection,\\nSis the number of gold standard reference clusters manually\\ngenerated, and Ris the number of clusters automatically gen-\\nerated by the GCLUSIM. Also, nsis the number of elements\\nin cluster s and nrsis the number of elements belonging to\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nTable 2 The Arnetminer data set (Authors denotes the number of dis-\\ntinct authors and Records represents citation records associated withthat author)\\nName Authors Record\\nAjay Gupta 11 93\\nBin Zhu 15 49Charles Smith 4 7Michael Siege 6 54David C. Wilson 5 67Eric Martin 5 85Yu Zhang 72 236Sanjay Jain 4 216Hui Yu 21 32\\nXiaoyan Li 6 33\\nJie Yu 9 32Bo Liu 65 306Kai Zhang 28 82Bin Li 65 306Michael Wagner 14 71Paul Brown 7 26Peter Phillips 3 13Lu Liu 17 58Cheng Chang 5 27David Brown 25 61David Cooper 7 18R. Ramesh 9 46Fei Su 4 37Gang Luo 9 47Hui Fang 8 42Jie Tang 6 66Yang Yu 19 71John F. McDonald 2 34Lei Wang 109 307\\nLei Fang 7 17\\nPing Zhou 18 36Rakesh Kumar 10 96Shu Lin 2 76Thomas D. Taylor 3 4Mark Davis 6 24Michael Lang 4 17Lei Chen 36 192Ning Zhang 31 125Paul Wang 7 16Robert Allen 9 24S Huang 13 14J. Guo 10 13Ji Zhang 16 64Wen Jao 9 487X. Zhang 40 62Yan Tang 6 27Table 2 continued\\nName Authors Record\\nWei Xu 47 153\\nXiaoming Wang 14 41Lei Jin 6 16Li Shen 6 65\\nTable 3 The BDBComp data set\\nS. No. Ambiguous group Total records Distinct authors\\n1 A. Oliveira 52 16\\n2 A. Silva 64 32\\n3 F. Silva 26 20\\n4 J. Oliveira 48 18\\n5 J. Silva 36 176 J. Souza 35 117 L. Silva 33 188 M. Silva 21 16\\n9R . S a n t o s 2 0 1 6\\n10 R. Silva 28 20\\nboth cluster r and cluster s. Pairwise F1 (PF1) is deﬁned as\\nthe harmonic mean of Pairwise Precision (PP) and Pairwise\\nRecall (PR). PP is the fraction of publication records cor-\\nresponding to the same author in the algorithm-generatedclusters, and PR is the fraction of publication records asso-\\nciated with the same author in the gold standard reference\\nclusters. The PP, PR and PF1 measures are expressed in Eq. 8,\\nwhere C(n,r)denotes the number of combinations of rele-\\nments from nelements: C(n,r)=\\nn!\\nr!(n−r)!,n≥r. Other\\nparameters including r,s,nsandnsrare deﬁned as before in\\nEq.7.\\nPP=/summationtextR\\nr=1/summationtextS\\ns=1C(nrs,2)\\n/summationtextR\\nr=1C(nr,2),\\nPR=/summationtextR\\nr=1/summationtextS\\ns=1C(nrs,2)\\n/summationtextS\\nr=1C(ns,2),PF1=2∗PP∗PR\\nPP+PR(8)\\nCluster F1 (CF1) is deﬁned as the harmonic mean of Clus-\\nter Precision (CP) and Cluster Recall (CR), where CP is the\\nfraction of the generated clusters that are equal to the ref-\\nerence clusters and CR is the fraction of correctly retrievedclusters from the reference clusters. The CP, CR and CF1\\nmeasures are given in Eq. 9.\\nCP=R∩S\\nR,CR=R∩S\\nS,CF−1=2∗CP∗CR\\nCP+CR(9)\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\n4.3 Baseline Methods\\nThree graph-based author name disambiguation methods—\\nOn Graph-based Name Disambiguation (GHOST) [ 12],\\nscalable clustering methods (MGPM) [ 23] and Author name\\ndisambiguation using a graph model (GFAD) [ 5]—were cho-\\nsen as baseline methods to compare with GCLUSIM.\\nGHOST was proposed by [ 12], which they called “GrapH-\\nical framewOrk for name diSambiguaTion (GHOST)”. Itmodeled the relationships among publications using undi-\\nrected graphs and solved name disambiguation by iteratively\\nﬁnding valid paths, computing similarities, clustering withthe help of afﬁnity propagation algorithm and in the last\\nusing user feedback as a complementary tool to enhance the\\nperformance. MGM and MGPM repeatedly merged the sub-graphs until the number of sub-graphs is equal to the givennumber of clusters kin the gold standard. MGPM evalu-\\nated on the same Arnetminer data set to make a comparison\\nwith GCLUSIM. More details of the MGPM method can beseen in [ 23]. GFAD as proposed by [ 5] resolves homonyms\\nby ﬁnding maximum non-overlapping cycles in co-authors\\ngraph. It then solves synonyms by applying longest commonsubsequence with a threshold of 0.8 and ﬁnd a connection\\nbetween two nodes in co-authors graph. In the last step, it\\nsolves sole author by merging the sole authors with the mostsimilar author node with the help of title similarity.\\n4.4 Performance Evaluation of GCLUSIM\\nGCLUSIM effectiveness is measured using nine cluster-\\ning evaluation metrics on Arnetminer collection of citation\\nrecords. Figure 8shows average ACP, AAP, K-metric, PP,\\nPR, PF1, CP, CR and CF1 of GCLUSIM on Arnetminer dataset. On average it achieves both ACP and PP of 97% , Kand\\nPF1 are 86% and 80%, respectively, and CF1 of 58%.\\nTable 4lists the complete details of each ambiguous author\\ngroup in the Arnetminer collection. In general ACP and PP\\nof GCLUSIM in the majority of ambiguous author cases are\\nnearly equal to 100% but in some cases such as X. Zhangand Paul Brown it is low. CF1 is sometimes as low as 0%.\\nThis usually happens when there are few imbalance clusters\\n(one of them is having less than 10% and the other havingmore than 90% citations). This happened in the case of JohnF. McDonald where there are two ground truth clusters, one\\ncluster has only one citation and the other one has 33 citations\\nin it.\\nAverage ACP, AAP, K-metric, PP, PR, PF1, CP, CR and\\nCF1 of GCLUSIM on BDBComp data set are shown in\\nFig. 9. As seen in this ﬁgure, overall results are comparable\\nto the results of Arnetminer collection. However, the aver-\\nage K-metric was dropped to 3.5%. This might be due to a\\nlarge number of authors have only one citation, so these arewrongly merged with others. On the other hand, GCUSIMACP AAP K PP PR PF1 CP CR CF1506070809010097\\n778697\\n7180\\n5270\\n58Percentage (%)\\nFig. 8 Average ACP, AAP, K-metric, pairwise precision, pairwise\\nrecall, pairwise F1, cluster precision, cluster recall and cluster F1 ofthe GCLUSIM on Arnetminer\\nachieved PF1 and CF1 of 84% and CF1 of 62% that is higher\\nas compared to Arnetminer.\\nIn Fig. 10, GCLUSIM performance is compared with\\nbaseline methods—GFAD, GHOST, and MGPM. GCLUSIM\\nachieves 13, 10 and 18% higher in K-metric and 28.9, 18.4\\nand 52.6% higher in CF1 to GFAD, GHOST, and MGPM,respectively. GCLUSIM has a loss of 1.25% in PF1 than\\nGFAD, but higher PF1 than GHOST and MGPM. So, its\\nperformance is overall better than baselines.\\nThe reason why GCLUSIM has statistically better results\\nthan GFAD, GHOST and MGPM is that it exploited graph\\nstructural clustering and relationships between authors usinghybrid similarity index. As GHOST only uses syntactic simi-larity, so some wrong merges of the publications on the basis\\nof this similarity may occur. One other reason of its low\\nperformance might be its optimization parameters, as theseparameters require careful selection and it may not ﬁt equally\\nwell to all ambiguous author groups.\\nIn Fig. 11, performance of GCLUSIM is compared to\\nbaselines using the BDBComp collection and once again\\nGCLUSIM shows overall better results than these methods.\\nGCLUSIM achieves 6, 3 and 5% higher in K-metric and 5,16 and 31% higher in CF1 to GFAD, GHOST and MGPM,respectively. GCLUSIM has comparable performance in PF1\\nthan GFAD, but higher than GHOST and MGPM.\\nGFAD is unable to identify those authors that only share\\none co-author, whereas GCLUSIM is able to detect these\\nauthors. One other possible reason for GFAD comparatively\\nlow performance in K and CF1 may be due to its use ofonly title words for similarity calculations that are not a\\nrepresentative of an author’s work. GCLUSIM shows com-\\nparable PF1 as the GFAD that outperforms other techniques[4,10,24]. As seen in Figs. 10and 11, the MGPM perfor-\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nTable 4 The performance\\nevaluation of GCLUSIM for\\neach ambiguous group on theArnetminer collectionName ACP AAP K PP PR PF1 CP CR CF1\\nAjay Gupta 0.87 0.58 0.71 0.80 0.53 0.64 0.19 0.33 0.24\\nBin Li 0.95 0.84 0.89 0.93 0.77 0.84 0.72 0.83 0.78Bin Zhu 1.00 0.86 0.93 1.00 0.73 0.84 0.68 0.87 0.76Charles Smith 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00David C. Wilson 1.00 0.40 0.63 1.00 0.33 0.50 0.25 0.60 0.35Eric Martin 0.99 0.56 0.74 1.00 0.56 0.71 0.18 0.60 0.27Fei Su 0.96 0.71 0.83 0.99 0.76 0.86 0.25 0.50 0.33Hui Yu 1.00 0.97 0.98 1.00 0.95 0.97 0.91 0.95 0.93Ji Zhang 0.98 0.83 0.90 0.99 0.84 0.91 0.50 0.69 0.58Jie Yu 1.00 0.89 0.94 1.00 0.97 0.99 0.50 0.67 0.57Kai Zhang 0.98 0.84 0.91 0.99 0.82 0.90 0.55 0.71 0.62Bo Liu 0.97 0.80 0.88 0.91 0.47 0.62 0.78 0.89 0.83Cheng Chang 0.96 0.67 0.81 0.98 0.53 0.69 0.33 0.60 0.43\\nDavid Brown 0.98 0.84 0.91 0.99 0.83 0.90 0.71 0.88 0.79\\nDavid Cooper 1.00 0.92 0.96 1.00 0.90 0.95 0.75 0.86 0.80Gang Luo 0.96 0.85 0.90 0.99 0.76 0.86 0.60 0.67 0.63Hui Fang 1.00 0.62 0.79 1.00 0.52 0.69 0.29 0.50 0.36J. Guo 1.00 0.92 0.96 1.00 0.67 0.80 0.82 0.90 0.86Jie Tang 1.00 0.73 0.85 1.00 0.69 0.82 0.38 0.83 0.53John F. McDonald 0.91 0.94 0.93 0.94 0.94 0.94 0.00 0.00 0.00Lei Chen 0.97 0.65 0.79 0.95 0.49 0.64 0.58 0.80 0.67Lei Fang 1.00 0.90 0.95 1.00 0.77 0.87 0.75 0.86 0.80Lei Wang 0.93 0.84 0.88 0.85 0.71 0.77 0.71 0.82 0.76Lu Liu 0.97 0.84 0.90 0.99 0.73 0.84 0.65 0.88 0.75Michael Lang 1.00 0.72 0.85 1.00 0.54 0.70 0.50 0.75 0.60Ning Zhang 0.98 0.56 0.74 1.00 0.30 0.46 0.43 0.70 0.53Paul Wang 0.94 0.92 0.93 0.95 0.90 0.93 0.71 0.71 0.71S Huang 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00Wen Jao 0.99 0.89 0.94 0.99 0.90 0.94 0.24 0.70 0.36X. Zhang 0.79 0.90 0.84 0.47 0.76 0.58 0.67 0.60 0.63Xiaoyan Li 0.95 0.90 0.92 0.94 0.84 0.88 0.57 0.67 0.62Yan Tang 1.00 0.85 0.92 1.00 0.81 0.90 0.64 0.82 0.72Lei Jin 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00Li Shen 0.99 0.70 0.83 1.00 0.69 0.82 0.33 0.67 0.44\\nMark Davis 1.00 0.57 0.75 1.00 0.34 0.51 0.40 0.67 0.50\\nMichael Siege 0.96 0.75 0.85 1.00 0.77 0.87 0.25 0.50 0.33Michael Wagner 0.96 0.51 0.70 0.95 0.26 0.41 0.29 0.50 0.37Paul Brown 0.86 0.77 0.81 0.85 0.66 0.75 0.56 0.62 0.59Peter Phillips 1.00 0.77 0.88 1.00 0.74 0.85 0.20 0.33 0.25Ping Zhou 1.00 0.92 0.96 1.00 0.82 0.90 0.80 0.89 0.84R. Ramesh 1.00 0.54 0.74 1.00 0.48 0.65 0.32 0.78 0.45Rakesh Kumar 0.99 0.66 0.81 1.00 0.74 0.85 0.19 0.50 0.28Robert Allen 0.96 0.92 0.94 0.99 0.87 0.92 0.78 0.78 0.78Sanjay Jain 0.99 0.62 0.78 1.00 0.67 0.80 0.04 0.20 0.06Shu Lin 1.00 0.38 0.62 1.00 0.37 0.54 0.08 0.50 0.14Thomas D. Taylor 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00Wei Xu 0.96 0.72 0.83 0.94 0.63 0.76 0.47 0.65 0.54Xiaoming Wang 0.97 0.80 0.88 1.00 0.89 0.94 0.50 0.64 0.56\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nTable 4 continuedName ACP AAP K PP PR PF1 CP CR CF1\\nYang Yu 0.99 0.72 0.84 0.99 0.57 0.72 0.47 0.74 0.57\\nYu Zhang 0.95 0.65 0.78 0.91 0.44 0.59 0.49 0.69 0.57\\nACP AAP K PP PR PF1 CP CR CF15060708090100\\n93\\n758398\\n7384\\n5375\\n62Percentage (%)\\nFig. 9 Average ACP, AAP, K-metric, pairwise precision, pairwise\\nrecall, pairwise F1, cluster precision, cluster recall and cluster F1 ofthe GCLUSIM on BDBComp\\nGCLUSIM GFAD GHOST MGPM40506070809086\\n7678\\n738081\\n6569\\n58\\n4549\\n38Percentage (%)K PF1 CF1\\nFig. 10 Average K, pairwise F1 and cluster F1 of the GCLUSIM,\\nGFAD, GHOST and MGPM on Arnetminer\\nmance is overall worst as compared to GCLUSIM, even\\nthough it used the hidden information about the exact num-\\nber‘K’of ambiguous author groups. As in Arnetminer 4.3%\\nauthors are sole authors and 35.6% have only one citation, soMGPM might perform some wrong partitioning and merging\\nof these authors in both phases.\\nComparison between ground truth clusters and GCLUSIM\\ngenerated clusters is shown in Fig. 12. About 80% clustersGCLUSIM GFAD GHOST MGPM40506070809083\\n7880798486\\n6974\\n62\\n59\\n52\\n43Percentage (%)K PF1 CF1\\nFig. 11 Average K, pairwise F1 and cluster F1 of the GCLUSIM,\\nGFAD, GHOST and MGPM on BDBComp\\n0 1 02 03 04 05 0020406080100120\\nAmbiguous authorNumber of clustersGTC\\nGGC\\nFig. 12 Comparison of ground truth clusters (GTC) and GCLUSIM\\ngenerated clusters (GGC)\\nare within the range (Ground Truth Clusters ±5) that’s why\\nGCLUSIM shows better CF1 than baseline methods. How-ever, GCLUSIM produced more clusters than the ground\\ntruth clusters. This is due to the fact that few authors have\\nchanged their research interests and thus have different socialcircles.\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\nTable 5 Summary of running times (seconds) on Arnetminer and BDB-\\nComp data set of all methods\\nData set GHOST MGPM GFAD GCLUSIM\\nArnetminer 57 49 749 38\\nBDBComp 31 24 257 29\\n4.5 Run Time Analysis of GCLUSIM\\nThe GCLUSIM consists of ﬁve stages, in which graph struc-\\ntural clustering is rather time-consuming step as compared\\nto other steps. To show the scalability of GCLUSIM, thecomparison of running time of all methods for disambiguat-\\ning Arnetminer and BDBComp data sets is shown in Table 5.\\nAll experiments were performed on a personal computer withIntel(R) Core(TM)i5-5200U CPU @ 2.20 GHz 2.20 GHzand 8 Gigabyte memory. All methods were implemented\\nusing Python. In GHOST and MGPM, we set the thresh-\\nold parameter ‘K’ equal to the number of clusters in the goldstandard data set for hierarchical clustering. GFAD has two\\nparameters; the ﬁrst is the maximum non-overlapping cycles\\nin co-authors graph and the other is a threshold for stringcomparison (longest common subsequence) that we set 0.8.\\nSimilarly, in GCLUSIM there are three threshold parame-\\nters; the ﬁrst is 0.8 for optimal blocking; the second is 0.2 forfeature vector similarity; and the last one is the 0.5 for theproposed hybrid similarity. After different experiments, we\\ncarefully chose these parameters.\\nAs seen in Table 5, GFAD was the slowest among all the\\nmethods on both collections. Its most time-consuming stage\\nis the cycle ﬁnding in the co-authors graph. GHOST and\\nGCLUSIM were the fastest on BDBComp and Arnetminer,respectively. The reason why GHOST outperforms on BDB-\\nComp is that there are small length paths in the co-authors\\ngraph of this collection, as the majority of authors have one ortwo citations. The running time of GLUSIM was statisticallytied with GHOST on BDBComp collection. GCLUSIM is\\nabout 8–19 times faster than GFAD and overall faster than\\nall other baseline methods.\\n5 Conclusions and Future work\\nSemantic similarity and community detection algorithms arelittle used in the domain of author name disambiguation.In this paper, we have proposed GCLUSIM a graph-based\\nmethod that does not require costly training data or a pri-\\nori hidden information (how many ambiguous authors k\\nin an ambiguous block) and web searches, solves both\\nhomonyms and synonyms. In contrast to many previous stud-\\nies, GCLUSIM also solves the sole authors. In this work, wepresent three algorithms, namely homonyms resolver algo-rithm, synonyms resolver algorithm and sole authors resolver\\nalgorithm, as our main contributions. GCLUSIM perfor-mance is tested on Arnetminer and BDBComp. It shows\\noverall better results than baseline methods. GCLUSIM\\ndelivers an effective solution to author name ambiguity prob-lem as it utilizes the powers of graph algorithms. However,it is unable to detect very ambiguous author cases. In future,\\nwe plan to analyze self-citation and email address of authors\\nto overcome this limitation.\\nReferences\\n1. Bhattacharya, I.; Getoor, L.: Collective entity resolution in rela-\\ntional data. ACM Trans. Knowl. Discov. Data (TKDD) 1(1), 5\\n(2007)\\n2. Ferreira, A.A.; Veloso, A.; Gonçalves, M.A.; Laender, A.H.: Effec-\\ntive self-training author name disambiguation in scholarly digitallibraries. In: Proceedings of the 10th Annual Joint Conference onDigital Libraries, pp. 39–48. ACM (2010)\\n3. Tang, J.; Fong, A.C.; Wang, B.; Zhang, J.: A uniﬁed probabilistic\\nframework for name disambiguation in digital library. IEEE Trans.\\nKnowl. Data Eng. 24(6), 975–987 (2012)\\n4. Han, H.; Xu, W.; Zha, H.; Giles, C.L.: A hierarchical naive bayes\\nmixture model for name disambiguation in author citations. In:Proceedings of the 2005 ACM symposium on Applied computing,pp. 1065–1069. ACM (2005)\\n5. Shin, D.; Kim, T.; Choi, J.; Kim, J.: Author name disambiguation\\nusing a graph model with node splitting and merging based onbibliographic information. Scientometrics 100(1), 15–50 (2014)\\n6. Han, D.; Liu, S.; Hu, Y .; Wang, B.; Sun, Y .: Elm-based name disam-\\nbiguation in bibliography. World Wide Web 18(2), 253–263 (2015)\\n7. On, B.W.; Lee, D.; Kang, J.; Mitra, P.: Comparative study of name\\ndisambiguation problem using a scalable blocking-based frame-work. In: Proceedings of the 5th ACM/IEEE-CS Joint Conferenceon Digital Libraries, pp. 344–353. ACM (2005)\\n8. Huang, J.; Ertekin, S.; Giles, C.L.: Efﬁcient name disambiguation\\nfor large-scale databases. In: European Conference on Principlesof Data Mining and Knowledge Discovery, pp. 536–544. Springer(2006)\\n9. Treeratpituk, P.; Giles, C.L.: Disambiguating authors in academic\\npublications using random forests. In: Proceedings of the 9thACM/IEEE-CS Joint Conference on Digital Libraries, pp. 39–48.ACM (2009)\\n10. Cota, R.G.; Ferreira, A.A.; Nascimento, C.; Gonçalves, M.A.;\\nLaender, A.H.: An unsupervised heuristic-based hierarchical\\nmethod for name disambiguation in bibliographic citations. J. Am.\\nSoc. Inf. Sci. Technol. 61(9), 1853–1870 (2010)\\n11. de Carvalho, A.P.; Ferreira, A.A.; Laender, A.H.; Gonçalves, M.A.:\\nIncremental unsupervised name disambiguation in cleaned digitallibraries. J. Inf. Data Manag. 2(3), 289 (2011)\\n12. Fan, X.; Wang, J.; Pu, X.; Zhou, L.; Lv, B.: On graph-based name\\ndisambiguation. J. Data Inf. Qual. (JDIQ) 2(2), 10 (2011)\\n13. Onodera, N.; Iwasawa, M.; Midorikawa, N.; Yoshikane, F.; Amano,\\nK.; Ootani, Y .; Kodama, T.; Kiyama, Y .; Tsunoda, H.; Yamazaki,S.: A method for eliminating articles by homonymous authors fromthe large number of articles retrieved by author search. J. Am. Soc.Inf. Sci. Technol. 62(4), 677–690 (2011)\\n14. Huynh, T.; Hoang, K.; Do, T.; Huynh, D.: Vietnamese author\\nname disambiguation for integrating publications from heteroge-neous sources. In: Asian Conference on Intelligent Information andDatabase Systems, pp. 226–235. Springer (2013)\\n123\\nAuthor\\'s personal copyArabian Journal for Science and Engineering\\n15. Liu, Y .; Tang, Y .: Network based framework for author name dis-\\nambiguation applications. Int. J. u and e Serv. Sci. Technol. 8(9),\\n75–82 (2015)\\n16. Wang, X.; Tang, J.; Cheng, H.; Philip, S.Y .: Adana: Active name\\ndisambiguation. In: 2011 IEEE 11th International Conference onData Mining, pp. 794–803. IEEE (2011)\\n17. On, B.W.; Elmacioglu, E.; Lee, D.; Kang, J.; Pei, J.: Improving\\ngrouped-entity resolution using quasi-cliques. In: Sixth Interna-tional Conference on Data Mining (ICDM’06), pp. 1008–1015.IEEE (2006)\\n18. Peng, H.T.; Lu, C.Y .; Hsu, W.; Ho, J.M.: Disambiguating authors\\nin citations on the web and authorship correlations. Expert Syst.Appl. 39(12), 10521–10532 (2012)\\n19. Han, H.; Giles, L.; Zha, H.; Li, C.; Tsioutsiouliklis, K.: Two\\nsupervised learning approaches for name disambiguation in authorcitations. In: Proceedings of the 2004 Joint ACM/IEEE Conferenceon Digital Libraries, 2004, pp. 296–305. IEEE (2004)\\n20. Wang, J.; Berzins, K.; Hicks, D.; Melkers, J.; Xiao, F.; Pinheiro, D.:\\nA boosted-trees method for name disambiguation. Scientometrics93(2), 391–411 (2012)\\n21. Xu, X.; Yuruk, N.; Feng, Z.; Schweiger, T.A.: Scan: a structural\\nclustering algorithm for networks. In: Proceedings of the 13th ACM\\nSIGKDD International Conference on Knowledge Discovery and\\nData Mining, pp. 824–833. ACM (2007)\\n22. Johnson, D.B.: Finding all the elementary circuits of a directed\\ngraph. SIAM J. Comput. 4(1), 77–84 (1975)\\n23. On, B.W.; Lee, I.; Lee, D.: Scalable clustering methods for the\\nname disambiguation problem. Knowl. Inf. Syst. 31(1), 129–151\\n(2012)\\n24. Tran, H.N.; Huynh, T.; Do, T.: Author name disambiguation by\\nusing deep neural network. In: Asian Conference on IntelligentInformation and Database Systems, pp. 123–132. Springer (2014)\\n25. Wu, H.; Li, B.; Pei, Y .; He, J.: Unsupervised author disambiguation\\nusing dempster-shafer theory. Scientometrics 101(3), 1955–1972\\n(2014)\\n26. Zhu, J.; Yang, Y .; Xie, Q.; Wang, L.; Hassan, S.U.: Robust hybrid\\nname disambiguation framework for large databases. Scientomet-rics 98(3), 2255–2274 (2014)27. Levin, F.H.; Heuser, C.A.: Evaluating the use of social networks in\\nauthor name disambiguation in digital libraries. J. Inf. Data Manag.1(2), 183 (2010)\\n28. Shoaib, M.; Daud, A.; Khiyal, M.S.H.: Improving similarity\\nmeasures for publications with special focus on author name dis-ambiguation. Arab. J. Sci. Eng. 40(6), 1591–1605 (2015)\\n29. Al-Safadi, L.; Al-Rgebh, D.; AlOhali, W.: A comparison between\\nontology-based and translation-based semantic search engines forarabic blogs. Arab. J. Sci. Eng. 38(11), 2985 (2013)\\n30. Al-Rajebah, N.I.; Al-Khalifa, H.S.: Extracting ontologies from\\narabic wikipedia: a linguistic approach. Arab. J. Sci. Eng 39(4),\\n2749–2771 (2014)\\n31. Mansouri, D.; Mille, A.; Hamdi-Cherif, A.: Adaptive delivery of\\ntrainings using ontologies and case-based reasoning. Arab. J. Sci.Eng. 39(3), 1849 (2014)\\n32. Huang, Z.; Zhang, J.; Zhang, B.: Information recommendation\\nbetween user groups in social networks. Arab. J. Sci. Eng. 40(5),\\n1443–1453 (2015)\\n33. Liu, Q.; Zhou, B.; Li, S.; Li, A.p; Zou, P.; Jia, Y .: Community\\ndetection utilizing a novel multi-swarm fruit ﬂy optimization algo-rithm with hill-climbing strategy. Arab. J. Sci. Eng. 41(3), 807–828\\n(2016)\\n34. Imran, M.; Gillani, S.; Marchese, M.: A real-time heuristic-based\\nunsupervised method for name disambiguation in digital libraries.DL i b .M a g . 19(9), 1 (2013)\\n35. Porter, M.F.: An algorithm for sufﬁx stripping. Program 14(3),\\n130–137 (1980)\\n36. Kang, I.S.; Na, S.H.; Lee, S.; Jung, H.; Kim, P.; Sung, W.K.; Lee,\\nJ.H.: On co-authorship for author disambiguation. Inf. Process.Manag. 45(1), 84–97 (2009)\\n37. Cohen, W.; Ravikumar, P.; Fienberg, S.: A comparison of string\\nmetrics for matching names and records. In: Kdd Workshop onData Cleaning and Object Consolidation, vol. 3, pp. 73–78 (2003)\\n38. Pereira, D.A.; Ribeiro-Neto, B.; Ziviani, N.; Laender, A.H.;\\nGonçalves, M.A.; Ferreira, A.A.: Using web information for authorname disambiguation. In: Proceedings of the 9th ACM/IEEE-CSJoint Conference on Digital Libraries, pp. 49–58. ACM (2009)\\n123\\nAuthor\\'s personal copy', 'Research Paper\\nJournal of Information Science\\n1–18/C211The Author(s) 2018\\nReprints and permissions:\\nsagepub.co.uk/journalsPermissions.nav\\nDOI: 10.1177/0165551518761011journals.sagepub.com/home/jis\\nDISC: Disambiguating homonyms\\nusing graph structural clustering\\nIjaz Hussain\\nDepartment of Computer Science, COMSATS Institute of Information T echnology, Pakistan\\nSohail Asghar\\nDepartment of Computer Science, COMSATS Institute of Information T echnology, Pakistan\\nAbstract\\nAuthor name ambiguity degrades information retrieval, database integration, search results and, more importantly, correct attributions\\nin bibliographic databases. Some unresolved issues include how to ascertain the actual number of authors, how to improve the perfor-mance and how to make the method more effective in terms of representative clustering metrics (average cluster purity, averageauthor purity, K-metric, pairwise precision, pairwise recall, pairwise-F1, cluster precision, cluster recall and cluster-F1). It is a non-trivial task to disambiguate authors using only the implicit bibliographic information. An effective method ‘DISC’ is proposed that usesgraph community detection algorithm, feature vectors and graph operations to disambiguate homonyms. The citation data set is pre-processed and ambiguous author blocks are formed. A co-authors graph is constructed using authors and their co-author’s relation-ships. A graph structural clustering ‘gSkeletonClu’ is applied to identify hubs, outliers and clusters of nodes in a co-author’s graph.Homonyms are resolved by splitting these clusters of nodes across the hub if their feature vector similarity is less than a predefinedthreshold. DISC utilises only co-authors and titles that are available in almost all bibliographic databases. With little modifications,DISC can also be used for entity disambiguation. T o validate the DISC performance, experiments are performed on two Arnetminerdata sets and compared with five previous unsupervised methods. Despite using limited bibliographic metadata, DISC achieves onaverage K-metric, pairwise-F1, and cluster-F1 of 92%, 84% and 74%, respectively, using Arnetminer-S and 86%, 80% and 57%, respec-tively, using Arnetminer-L. About 77.5% and 73.2% clusters are within the range (ground truth clusters ±3) in Arnetminer-S and\\nArnetminer-L, respectively.\\nKeywords\\nAuthor name disambiguation; community detection; graph structural clustering; homonyms\\n1. Introduction\\nBibliographic databases such as DBLP,1MEDLINE,2Cite Seer,3arXiv,4MAS,5Google Scholar6and BDBComp7con-\\nserve bibliographic citations and provide several services, for example, receiving items associated with a particularauthor, multiple searches, browsing personalisation and building communities with certain educational fields [2]. Some\\nof the main challenges highlighted in Han et al. [2] are to have high-quality content in digital libraries (DLs) come from\\nseveral sources of errors such as lack of (the enforcement of) standards, imperfect citation-gathering software, disparatecitation formats, data-entry errors, ambiguous author names, decentralised generation of content (i.e. by means of auto-\\nmatic harvesting) and abbreviations of publication venue, titles. Among these sources of errors, a great attention is paid\\nto ambiguous author names from the research community due to its inherent difficulty [1,3,4]. Resolving author nameambiguity from citations is referred as author name disambiguation (AND). Author name ambiguity occurs when manyauthors share the same name. In this case, it is too difficult to be certain about the accuracy of retrieved results. This phe-\\nnomenon of citation merger, the case of two or more persons having the same name (e.g. ‘Wei Wang’), is also known as\\nmixed citations [4].\\nCorresponding author:\\nIjaz Hussain, Department of Computer Science, COMSATS Institute of Information T echnology, Park Road, Islamabad 45550, Pakistan.\\nEmail: ijazhussain7979@hotmail.com\\nGenerally, ambiguity in author names is resolved by means of different publication attributes such as co-authors,\\ntitles, abstract, venue, affiliation and publication year [1,4]. However, every bibliographic database does not provide allthese attributes; they provide only limited information and manual annotation is not possible at such a large scale.Furthermore, in recent years, large amounts of publication data are being created and accepted by bibliographic data-\\nbases that make the name ambiguity problem even more severe than what it has been in the past [1].\\nWhen we search for an author named ‘Wei Wang’ in DBLP (a renowned bibliographic database), we get more than\\n86 authors having exactly the same names ‘Wei Wang’. The same situation turns out in almost all major bibliographic\\ndatabases [4]. This example is merely a representative of the magnitude of the mixed citation problem which motivated\\nus. Additional motivation comes from the innumerable cases of Asian authors who have similar surnames, specifically inDBLP bibliographic database, which is loaded with such cases. Moreover, DBLP and other bibliographic databases arefrequently not informative enough in their metadata and lack pieces of important information such as an author’s affilia-\\ntion, email and publication references.\\nSeveral AND methods have been proposed in the literature [2–27]. These methods have improved the situation some-\\nhow, but still, there is a space for improvement in current solutions. Supervised methods require a labelled data set for\\nthe training of the model and are less scalable due to the requirement of training thousands of models for each ambiguousauthor Han et al. [9], Ferreira et al. [10], Tran et al. [17] and Levin et al. [18]. The majority of the unsupervised ANDmethods assume that a number of ambiguous authors/clusters ‘K’ is known in advance [2,17,18,20]. Some techniquesare not scalable when the number of ambiguous authors increases [3–5,17,25]. Others require hidden or extra informa-\\ntion from the web or user feedback for disambiguation [11,13,25,27,28].\\nIn this article, a novel method ‘DISC: Disambiguating homonyms usIng graph Structural Clustering’ is proposed to\\nresolve homonyms. DISC is a graph-based method in which citation data set is pre-processed and ambiguous author\\nblocks are created, and co-authors graph and feature vectors are constructed using the pre-processed citation data. Then,‘gSkeletonClu: Revealing Density-Based Clustering Structure from the Core-Connected Tree of a Network’ is used toidentify hubs, outliers and clusters of vertices (communities) from the co-author’s graph [29]. DISC resolves homonymsby splitting these communities across the hub nodes if their title feature vector similarity is less than a predefined thresh-\\nold and using graph operations. DISC uses only co-authors and titles to resolve the homonyms. Co-authors information\\nis considered a more discriminating feature than other citation features [3,4,8,19]. However, distinct from existing tech-niques, we exploit the community detection algorithm in combination with feature vector similarity and graph operationsto disambiguate homonyms. To the best of our knowledge, DISC algorithm is the first that uses a gSkeletonClu commu-\\nnity detection algorithm and feature vector similarity to disambiguate homonyms.\\nDISC performance is evaluated by comparing it with three unsupervised graph-based and two non-graph-based AND\\nmethods using Arnetminer data sets [4,8,11]. DISC proved to be overall better than these methods from the perspective\\nof frequently used clustering evaluation metrics despite the fact that DISC uses only the co-authors and titles. The maincontributions of this work are followed:\\n•We design the AND task as co-author’s graph that only uses implicit information of bibliographic databases.This work is motivated by the needs of a method that works for almost all bibliographic databases and does notrequire any explicit information. DISC does not require costly training data, hidden information about the num-\\nber of clusters or expert knowledge of the domain.\\n•We propose a novel solution ‘DISC’ that exploits graph structural clustering and feature vector similarity tosolve the name ambiguity. To the best of our knowledge, the identification of outliers in graph-based methods\\nhas been addressed for the first time. DISC has utilised a recently introduced community detection techniquegSkeletonClu: a graph skeleton–based structural clustering algorithm for clustering to identify hub, outliers andclusters of nodes. The advantage of DISC is that it resolves homonyms using the output of clustering, featurevector similarity and graph operations.\\n•Experiments on Arnetminer – a real-world data set – are performed to empirically validate the effectiveness ofDISC. The results show that DISC performance is overall better than three graph-based and two non-graph-based\\nmethods. This is a confirmatory contribution and indirectly proves our design and claim.\\nThe rest of this article is structured as follows. ‘Related work’ outlines related works and critically reviews their tech-\\nniques. In ‘Proposed methodology of DISC’, the proposed methodology is discussed in detail along with the definitionsof graph structural clustering. We test this approach on real-world data sets from Arnetminer and benchmark it againstother methods in ‘Performance evaluation of DISC’. Finally, we conclude with a discussion of contributions and futureresearch directions in ‘Conclusion and future work’.Hussain and Asghar 2\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/01655515187610112. Related work\\nThree brief surveys about AND are presented in the literature that defined the basics of the AND problem and over-\\nviewed some representative AND methods [1,30,31]. According to Ferreira et al. [1], AND methods can be categorised\\nas author assignment methods [9,10,15–17] and author grouping methods [3,4,7,8,11,20].\\nOnodera et al. [21] proposed a methodology for target author from the false homonym authors. They used publications\\nfeatures such as affiliation, title and co-authors for discriminating oeuvre of an author. Zhu et al. [22] proposed a hybrid\\nname disambiguation framework that used the traditional information (co-authors) along with web page genre informa-tion. This framework consisted of two main steps – web page genre identification and re-clustering model. Dempster–\\nShafer theory (DST) was fused with Shannon entropy (SE) for AND in Wu et al. [20]. They used some high-level fea-\\ntures like author affiliation, citation venue, co-authorship information and web correlation to calculate the similaritiesamong citations. Subsequently, these features were combined using DST and SE. On the basis of this information, plausi-\\nbility and belief of all authors were computed. Then, a matrix of pairwise correlation of papers was calculated. Each entry\\nin this matrix was linked to a belief and a plausibility function. Finally, they applied the DST-based hierarchical agglom-erative clustering for author disambiguation.\\nAn algorithm that not only disambiguates author names but reconstructs the h-index of the individual authors was pro-\\nposed in Schulz et al. [23]. They applied it to a large-scale Web of Science data set. They calculated the pairwise similar-\\nity between all papers on the basis of a number of shared co-authors, self-citations, common references and the number\\nof papers citing both publications. They constructed a link between those publications that have similarity score greaterthan some predefined threshold and made clusters. Then, these connected clusters were merged, resulting in bigger clus-ters, which were the set of papers by a unique author. Information about the h-index is necessary for the working of this\\nsystem.\\nINDi, a solution for the disambiguated DLs, was presented in De Carvalho et al. [24], which utilised similarity among\\nbibliographic records and grouped the new records to authors with similar citation records in the DL or to new authors\\nwhen the similarity evidence was not strong enough. Some particular heuristics were used for checking whether refer-ences of new citation records belong to pre-existing authors of the DL or if they belonged to new ones. This step prevents\\nrunning the disambiguation process on the entire DL.\\nLiu et al. [16] used three-step clustering framework for name disambiguation. In the first step, they obtained clusters\\non the basis of common co-authors. Then, titles are used to make bigger clusters from the first step fragmented clusters.\\nFinally, they fused clusters on the basis of venues. In Maguire [15], an ethnicity sensitive method that mainly comprisesthree parts was presented. In the first part, phonetic-based blocking for similar author signatures was done. Supervised\\nmachine learning–based linkage function was used that exploited the ethnicity sensitive information. Finally, hierarchi-\\ncal agglomerative clustering was done on the basis of a distance between two pairs of publications linkage function. Amethod named self-training associative name disambiguator (SAND) was proposed in Ferreira et al. [10] that consisted\\nof three steps after pre-processing of the data set. In the first step, co-authors heuristic was used to find the clusters of\\nauthor records that seem to be included in the same author group. Remaining clusters that were not used for training ofthe model were used for testing purposes. Finding the pure clusters in the first phase is a non-trivial task. Peng et al. [25]proposed an algorithm that used both web co-relation and author correlation based approach to measure similarities\\nbetween publications. They basically used two assumptions, citations on the same web page and citations with same\\nrarer authors belonged to the same author. They measured both types of correlations using the modified sigmoid func-tion, cosine metric and name popularity metric.\\nActive name disambiguation for the name ambiguity (ADANA) problem was proposed in Wang et al. [11]. In this\\nmethod, they modelled pairwise factor graph that can be used to integrate several types of features as well as user feed-\\nback into a unified model. They defined three types of feature functions, that is, document pair, correlation and\\nconstraint-based features. In document pair feature functions, they found known relationships from publications. In cor-relation feature function, they found some hidden feature with the help of known functions and in the constraint-based\\nfeature functions the user was involved in finding the unknown features. Finally, they exploited active selection of the\\nuser corrections in an interactive mode to improve the disambiguation performance after some preliminary clusteringresults. They exploited some additional information, such as affiliation and references, that is not present in every DL.\\nIn Santana et al. [12], a solution for incremental disambiguation was proposed, and in Shen et al. [13], a system that uti-\\nlised user feedback for disambiguation was proposed. Momeni and Mayr [32] argued that a co-author’s network alone isnot sufficient for ambiguous author names and improved the performance using a community detection algorithm withvarying resolutions for different names along with co-author’s network. They also contributed an ambiguous author data\\nset that can be used in future for AND research.Hussain and Asghar 3\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011As pointed out in this section that unsupervised methods required explicit features such as author affiliation, email\\naddress, web searches or a number of unknown ambiguous authors ‘K’ a priori, whereas supervised techniques requireda lot of training data that is costly and difficult. In contrast to these techniques, DISC neither requires costly training data\\nnor explicit bibliographic information. It uses only co-authors and titles to disambiguate homonyms.\\n3. Proposed methodology of DISC\\nIn this section, we present some definitions that are used in structural clustering, particularly in gSkeletonClu. DISC algo-\\nrithm is presented in subsequent paragraphs, whereas DISC architecture is shown in Figure 1.\\n3.1. Preliminaries\\nLetG= (V, E, w ) be a weighted undirected graph, where Vis a set of vertices, Eis set of edges and wis the weights\\nassigned to the edges on the basis of the co-author relationship (e.g. Assigned 2 to the edge if co-authorship occurred two\\ntimes in the citation’s data set) in this graph G. The structure of a vertex can be described by its neighbourhood vertices.\\nIf two vertices share more neighbours then these are more similar and vice versa. When a vertex shares similar structurewith one of its neighbouring vertices in a cluster then, their structural similarity will be high. A threshold /epsilon1is applied to\\nthe computed structural similarity when assigning cluster memberships. If a vertex share structural similarity with enough\\nneighbours, it becomes a nucleus or seed for that cluster and is called a core vertex. Core vertices are a special class of\\nvertices that have a minimum of μneighbours with a structural similarity that exceeds the specified threshold /epsilon1. From\\ncore vertices, the clusters are grown. In this way, the parameters /epsilon1andμdetermine the clustering of networks. For a\\ngiven /epsilon1, the minimal size of a cluster is determined by μ.\\n3.1.1. Cluster. A cluster of a network Gw.r.t. the given parameters /epsilon1andμas all structure-connected clusters in G. Let\\nε∈<andμ∈@. A clustering Pof network G=<V, E>w.r.t. /epsilon1andμconsists of all structure-connected clusters\\nw.r.t. /epsilon1andμinG, formally\\nCluster ε, μ(P),P=C∈VjCluster ε, μ(C)/C8/C9\\nð1Þ\\nA vertex is either a member of a structure-connected cluster or it is an isolated vertex, that is, it does not belong to any\\nof the structure-connected clusters. If a vertex is not a member of any structure-connected clusters, it is either a hub or anoutlier, depending on its neighbourhood.\\nFigure 1. Phases of DISC algorithm.Hussain and Asghar 4\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/01655515187610113.1.2. Hub. Letε∈<andμ∈@. For a given clustering P, that is, Cluster ε, μ(P), if an isolated vertex v∈Vhas neigh-\\nbours belonging to two or more different clusters w.r.t. /epsilon1andμ, it is a hub (it bridges different clusters) w.r.t. /epsilon1andμ\\nformally\\nHUB ε, μ(V),8 (C)∈P:v∈/negationslashC9(p,q)∈/C0(v):9(X,Y)∈P:X6¼Y:^p∈X^q∈Y ð2Þ\\n3.1.3. Outlier. For a given clustering P, that is, Cluster ε, μ(P), an isolated vertex v∈Vis an outlier if and only if all its\\nneighbours either belong to only one cluster or do not belong to any cluster\\nOUTLIER ε, μ(V),8 (C)∈P:v∈/negationslashC:9(p,q)∈/C0(v):9(X,Y)∈P:X6¼Y:^p∈X^q∈Y ð3Þ\\nWe implemented gSkeletonClu algorithm [29] in Python to detect hubs, outliers and clusters (communities) in the\\nco-author’s graph. This step is given in ‘DISC algorithm’ in Figure 5 at line 4. Interested readers are requested to read\\nHuang et al. [29] article for further details.\\n3.2. Proposed DISC algorithm\\nThe raw citation data set is pre-processed and split into authors, titles and venues terms. Author names are further dividedinto first name and last name. Longer names that have a middle name are also divided into the same two parts. The initial\\nand middle name becomes the first name in this strategy.\\n3.2.1. Author name blocking. The blocking stage is important as it affects the computations in the later stages of the name\\ndisambiguation algorithms. On et al. [33] defined four name blocking strategies – spelling-based heuristics, token-based,\\nn-gram and sampling-based. However, we used spelling-based heuristic strategy that also has different heuristics for\\nblocking such as the initial of the first name and the full last name (iFfL), the initial of the first name and the initial of the\\nlast name (iFiL) or full last name (fL) or some combination of these. However, in previous AND studies iFfL blocking\\nstrategy proved to be effective [8,10,27,32] and creates relatively small blocks as compared with other spelling-based\\nheuristics such as iFiL or fL. For example, the three names ‘Anubhav Gupta’, ‘Alok Gupta’ and ‘Aarti Gupta’ all grouped\\ninto the same candidate block of ‘A Gupta’. The blocking stage returns bnumber of blocks if given anumber of authors,\\nit is shown in Figure 2. The computation complexity is O(a\\n2), for aauthors if we do not use blocking, whereas blocking\\n(a) (b)\\nFigure 2. An example of ambiguous author blocking.Hussain and Asghar 5\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011considerably reduces computational complexity to O(B|S|). Where Bis the number of blocks and Sis the average size of\\nblocks.\\n3.2.2. Graph construction. After blocking, we represent all authors and their co-author relations as a graph G= (V, E, w ).\\nFor instance, each vertex v∈Vrepresents an author ai∈A, each edge (v1,v2)∈Erepresents the co-author’s relation and\\nwis the number of co-authorship relations between two authors. In DISC, a vertex represents an author which has a dis-\\ntinct identity, a name (not necessarily distinct) and citations set where that author is present. Two co-authors are modelled\\nusing bidirectional edges between two authors (vertices). More specifically, in a citation, set of nauthors is converted into\\na graph G=(V, E, w ). Authors are extracted from the citations data set and each distinct author aiis mapped to a vertex\\nv∈V. Then, for each citation, if there are anumber of authors then there are ½a(a/C01)=2/C138number of edges created in the\\ngraph G. For example, if a citation has three authors, then the number of edges would be (3 *2)/2 = 3. Similarly, if a cita-\\ntion has four authors, then the number of edges would be (4 *3)/2 = 6 and so on. Using this strategy for all citations, we\\nbuild the whole graph of all the citations in the data set. A small working example of citation data set, vertices, edges,weights and the constructed graph is shown in Figure 3. As shown in the figure, there are seven authors (nodes), nine\\nedges that are created from six citations and respective weights are shown on the edges that represent number of co-\\nauthorship relations between two authors. ‘Wei Wang’ is a node that is common to five citations and ‘M. Rehan’ is pres-ent only in citation 4. As there are two co-authorships between C. Chen and Wei Wang, so a weight of ‘2’ is assigned tothe edge between them in Figure 3.\\n3.2.3. Graph structural clustering. The aim of applying the gSkeletonClu community detection algorithm here is to only\\ndetect hubs, outliers and clusters of nodes in the co-author’s graph [29]. The DISC algorithm then detects the potential(a)\\n(b) (c)\\nFigure 3. Co-author’s graph of Wei Wang sample citations: (a) an example of small citation’s data set, (b) information of nodes in\\nthe graph and (c) graph from the excerpt of bibliographic citations.Hussain and Asghar 6\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011homonyms and then resolves these homonyms by applying graph operations that satisfy some predefined conditions.\\nThe use of gSkeletonClu is done here for four reasons: (1) it detects outliers that are not possible with existing graph-\\nbased methods; (2) it makes algorithm scalable as its computational complexity is O(e), where eis the number of edges\\nin co-authors graph; (3) in contrast to other popular community detection algorithms, it is useful to identify overlappingcommunities in the co-author’s graph; and (4) no need to tune threshold parameters, it automatically computes theseparameters.\\nIn DISC, it is assumed that different homonyms have different communities in an academic social circle and different\\nhomonyms seldom work in the same institution or community [3,4]. So, they belong to different author communities. Acommunity is generated from each citation in the co-author’s graph and thus each community denotes the co-authorship\\nfor each citation that is the smallest social circle in the academic domain. For example, in ‘citation 4’, there are three\\nauthors – S. Huang, M. Rehan and Ajay Gupta, due to this citation three nodes 3, 4 and 5 and their relationships are con-structed as shown in Figure 3(c), and when we add more citations to this graph, it becomes wider and bigger. Finally,when we construct the graph of all sample citations in Figure 3(a), it looks as shown in Figure 3. With the help of co-authors graph, it is possible to infer a social circle of an author from one’s co-authors by finding shared communities.\\n3.2.4. Feature vector construction. After removing stop words and stemming with the help of the Porter [34] stemmer, we\\nconstruct feature vectors of titles; pseudocode of this is given in Algorithm 1 (Figure 4). DISC assumes that an author\\nlinked with multiple social circles contains mixed information for several authors if the similarity that is calculated usingJaccard similarity between feature vectors of two clusters is less than 0.2. This threshold is chosen empirically by varyingits value in small increments and finding the optimal value of it. DISC splits that node and its information into a numberof nodes that are present in non-overlapping communities.\\n3.2.5. DISC algorithm. DISC considers each non-overlapping community emanating from the same node as a different\\nsocial circle of an author if its feature vector similarity is less than 0.2. Complete pseudocode of DISC algorithm is given\\nin Algorithm 2 (Figure 5), and its details are described in subsequent paragraphs.\\nIn the co-author’s graph, an edge represents a co-author’s relationship between the two nodes (authors). The DISC\\nalgorithm is based on gSkeletonClu: Structural Clustering Algorithm for Networks to detect different non-overlappingcommunities in the co-author’s graph [29]. In this algorithm, when we use gSkeletonClu on a co-author’s graph it out-puts communities (clusters of nodes), hub nodes and outliers. The community consists of a set of nodes from this co-author’s graph. The node that is involved in many social circles in the co-author’s graph is called a hub node. The hubnode is the potential homonym that needs disambiguation. Outliers are nodes that only contribute one or a limited num-ber of publications with hub node and have no other co-authors. For a formal definition of the hub, outlier and cluster,please refer to the ‘Preliminaries’.Figure 4. An implicit text feature vector constructor algorithm.Hussain and Asghar 7\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011In a small co-author’s graph example, detected communities, hub and outlier can be seen in Figure 6. There are two\\nclusters of nodes (communities), one hub and one outlier. The first and second cluster consists of nodes <3, 4, 5 >and\\n<6, 7>, respectively. The hub is node <2>and the outlier is node <1>.\\nA hub node that has multiple non-overlapping communities contains mixed information for several homonyms. DISC\\nsplits the information about each author on the node containing the homonyms along the non-overlapping communities\\nFigure 5. Proposed DISC algorithm.Hussain and Asghar 8\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011if its feature vector similarity is less than 0.2. This part starts from the line 7 of ‘DISC algorithm’, where the hub node\\npublications list is retrieved. Similarly, publications of all cluster nodes (community) are retrieved and saved in a list(lines 13–15). Now, for each homonym, the intersection of hub publications and community publications are found. Anew node is created in the co-authorship graph for each community detected that has the same name as that of the hubnode and has an identity one more than in the current nodes of the co-authorship graph. This newly created node has thepublication list that is found at line 16. Likewise, for all communities detected in the co-author’s graph, new nodes arecreated.\\nCo-authors graph and hub node publications are updated after every new node is inserted into the graph. In graph\\nupdate, some edges to the hub node to existing communities are removed and some new edges to newly created nodesare created (line 18–20). The same procedure is repeated for all outliers, as is done in the case of all communities (lines27–41). This whole process is pictorially shown in Figure 7, where hub node <2>is split into two new nodes <2A,\\n2B>, as there are two clusters of nodes (communities) that have feature vector similarity less than 0.2. In this way, the\\nnamesake’s problem is solved using community detection algorithm.\\n3.2.6. DISC complexity analysis. DISC first step is ambiguous author blocking step that has a time complexity of O(B|S|),\\nwhere Bis the number of blocks and Sis the average size of blocks. Graph G(V, E) can be constructed in O(|V|+|E|).\\nAccording to Huang et al. [29], graph structural clustering can be done in O(|E|). Text feature vectors and other graph\\ncomparisons are done only in blocks found in the first step. These steps take negligible time as compared with othersteps, so we ignore time complexities of these steps. In the above three steps, the blocking step dominates the wholecomputational complexity. Hence, the DISC has a complexity of O(B|S|).\\n4. Performance evaluation of DISC\\nIn this section, the performance of DISC is compared with graph-based and non-graph-based methods using Arnetminer\\ndata set in the form of clustering evaluation metrics. All experiments were performed on the Intel/C21064-bit Core /C228i5-\\n5200U 2 ×2.2 GHz processors with 8 GB of RAM.\\n4.1. Data set: Arnetminer\\nTo evaluate the DISC performance, we used two data sets – Arnetminer-S and Arnetminer-L. Tables 1 and 2 illustrate\\nthe overall statistics of both the data sets.\\nThese data sets are extracted from Arnetminer which was originally created by Wang et al. [11]. They manually\\nchecked, verified and labelled all these citation records. This collection has been used in many previous studies withslight variations [4,8,11] and is available online. In Arnetminer-S, we grouped all those ambiguous authors which havecitation records less than or equal to 50 and in the Arnetminer-L more than 50 citation records. The small data set iscomposed of 950 citation records associated with 344 distinct authors belonging to 39 ambiguous groups, approximatelyFigure 6. An example of detected communities, hub and outlier in Wei Wang’s sample graph after clustering.Hussain and Asghar 9\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/01655515187610112.76 citation records per author, as shown in Table 1. Whereas Arnetminer-L is composed of 3189 citation records asso-\\nciated with 657 distinct authors belonging to 25 ambiguous groups, approximately 4.85 citation records per author, asgiven in Table 2. For instance, in Arnetminer-S number of ‘Hiu Yu’ variants is 20, while in Arnetminer-L number of\\n‘Lei Wang’ variants is 111 and so forth. Both data sets are publicly available and can be accessed https://github.com/\\nkashak79/DISC_AND_Data_Set.\\n4.2. Evaluation metrics\\nWe used nine well- known clustering metrics – namely, average clu ster purity (ACP), average author purity (AAP),\\nK-metric, pairwise precision (PP), pai rwise recall (PR), pairwise-F 1 (PF1), cluster precision (CP), cluster recall (CR)\\nand cluster-F1 (CF1) – to measure the effectiveness of DISC [4,27,28].Figure 7. Example of homonyms resolution.\\nTable 1. The Arnetminer-S data set\\nName Auth. Record Name Auth. Record Name Auth. Record\\nAjay Gupta 8 31 B. Wilkinson 1 17 Bob Johnson 3 4\\nBin Zhu 15 45 Cheng Chang 5 23 Michael Lang 4 16\\nCharles Smith 4 7 Paul Wang 6 15 Fei Su 4 37\\nMichael Siege 6 50 David Cooper 7 18 Robert Allen 8 21Gang Luo 8 42 S. Huang 15 16 Hui Fang 8 42\\nJ. Guo 9 12 Hui Yu 20 30 Xiaoyan Li 6 31\\nYan T ang 11 31 Lei Fang 7 17 Ping Zhou 17 33\\nXiaoming Wang 14 33 Yue Zhao 9 36 Lei Jin 6 16\\nPaul Brown 8 20 Peter Phillips 3 13 Thomas T aylor 3 4David Nelson 10 15 F . Wang 14 15 Z. Wang 35 43\\nHong Xie 6 10 J. Yin 7 18 Jianping Wang 5 35\\nJohn Hale 3 36 Kuo Zhang 4 16 M. Rahman 7 15\\nMichael Smith 16 27 Richard T aylor 11 27 Y oung Park 8 17\\nAuth. denotes number of distinct authors and record represents citations records associated with that author.Hussain and Asghar 10\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011K-metric is defined as the geometric mean of the ACP and the AAP. The purity of the system-generated clusters, as\\ncompared with the ground truth clusters is measured by the ACP, so it measures the amount of data misclassified into\\nthe wrong clusters by checking whether the generated clusters include only the publication records belonging to the ref-\\nerence clusters. The level of fragmentation of the system-generated clusters into multiple clusters is evaluated by AAP.\\nACP, AAP, and K-metric are expressed in equation (4)\\nACP=1\\nNXI\\ni=1XJ\\nj=1nij2\\nni,AAP=1\\nNXI\\ni=1XJ\\nj=1nij2\\nnj,K=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nACP *AAPp\\nð4Þ\\nHere, Ndenotes the size of the citations in the collection, Jis the number of gold standard reference clusters manually\\ngenerated and Iis the number of clusters automatically generated by the DISC. Also, njis the number of elements in clus-\\nterjandnijis the number of elements belonging to both clusters iandj. PF1 is the harmonic mean of PP and PR. The\\nfraction of citation records corresponding to the same author in the system-generated clusters are known as PP and frac-\\ntion of citation records associated with the same author in the gold standard reference clusters are called as PR. The PP,\\nPR and PF1 measures are expressed in equation (5), where C(n; i) denotes the number of combinations of ielements from\\nnelements. Other parameters including i, j, n iandnijare defined as before in equation (4)\\nPP=PR\\nr=1PS\\ns=1C(nrs,2)\\nPR\\nr=1C(nr,2),PR=PR\\nr=1PS\\ns=1C(nrs,2)\\nPS\\ns=1C(ns,2),PF1=2(PP*PR)\\nPP+PRð5Þ\\nCF1 is the harmonic mean of CP and CR, where CP is the fraction of the generated clusters that are equal to the refer-\\nence clusters and CR is the fraction of correctly retrieved clusters from the reference clusters. The CP, CR and CF1 mea-\\nsures are given in equation (6)\\nCP=I∩J\\nI,CR=I∩J\\nJ,CF1=2(CP*CR )\\nCP+CRð6Þ\\n4.3. Comparison methods\\nThree graph-based and two non-graph-based AND methods which were related to our method are used to compare with\\nDISC [4,8,11,35].\\nOn graph-based name disambiguation called GHOST (GrapHical framewOrk for name diSambiguaTion) [8], we mod-\\nelled the relationships among publications using undirected graphs. They solved name disambiguation by iteratively find-\\ning valid paths, computing similarities, clustering with the help of affinity propagation algorithm and, in the last, using\\nuser feedback as a complementary tool to enhance the performance. They used a number of ambiguous authors (‘K’) a\\npriory and does not handle the sole author’s cases. In ADANA [11], they made a pairwise factor graph that is used to\\nintegrate several types of features as well as user feedbacks into a unified model. They defined three types of feature\\nfunctions. In document pair feature functions, they found, known relationships from publications. In correlation feature\\nfunction, they extracted some hidden features with the help of known functions and in the constraint-based feature func-\\ntions, the user is involved in finding the unknown features. In the last step, they exploited active selection of the userTable 2. The Arnetminer-L data set\\nName Auth. Record Name Auth. Record Name Auth. Record\\nBin Li 58 178 Jie Yu 6 66 Rakesh Kumar 9 95\\nDavid Brown 24 60 Jose M. Garcia 2 83 Sanjay Jain 5 196\\nEric Martin 5 84 Kai Zhang 23 65 Wei Xu 47 149\\nFeng Pan 14 70 Lei Chen 38 190 Wen Jao 10 482\\nGang Chen 43 169 Lei Wang 111 299 X. Zhang 37 58Hao Wang 45 158 Li Shen 8 65 Yang Yu 19 69\\nJi Zhang 16 62 Lu Liu 16 57 Y ong Chen 24 83\\nJie T ang 6 66 Manuel Silva 4 73 Yu Zhang 71 221\\nAuth. denotes number of distinct authors and record represents citations records associated with that author.Hussain and Asghar 11\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011corrections in an interactive mode to improve the disambiguation performance after some preliminary clustering results.\\nShin et al. [4] proposed a framework which they called Graph Framework for Author Disambiguation (GFAD) in whichJohnson cycle algorithm [36], and syntactic similarity was used to find non-overlapping cycles in a graph. To compare\\nDISC with GFAD, we implemented the disambiguation algorithm as given in Shin et al. [4], with their proposed para-\\nmeters. In Wang et al. [11], shortest valid path length, whereas in Shin et al. [4], largest non-overlapping cycles are usedfor clustering. Heuristic-based hierarchical clustering (HHC) is a heuristic-based unsupervised method that uses citation\\nfeatures for AND. HHC showed better results compared with other unsupervised and supervised methods [35]. We com-\\npared the performance of DISC with HHC using co-authors and titles heuristics which we call HHC-CT and HHC withall the features of publications (co-authors, titles and venue) which we denote with HHC-All. Both HHC-All and HHC-\\nCT needs similarity thresholds for the paper title and publication venue. Therefore, we evaluated the performance of\\nHHC by varying threshold values and then selected values on which the performances are maximised. We set the thresh-old values for paper title and publication venue as 0.2 and 0.3, respectively. They used hierarchical clustering for both\\nHHC-CT and HHC-All. For further details on HHC, please refer to Cota et al. [35].\\n4.4. DISC performance evaluation\\nDISC performance is evaluated using nine well-known clustering metrics on both Arnetminer-S and Arnetminer-L datasets. Table 3 shows average ACP, AAP, K-metric, PP, PR, PF1, CP, CR and CF1 of DISC. It achieves on average an\\nACP of 99%, K and PF1 are 92% and 84%, respectively, and CF1 of 74% on Arnetminer-S, as depicted in Figure 8.Table 4 lists the complete results of each ambiguous author group in the Arnetminer-L using nine clustering metrics.\\nACP and PP of DISC in the majority of cases are approximately equal to 100% but in some cases such as ‘X. Zhang’\\nand ‘Z. Wang’ is low. DISC achieves on average an ACP of 99%, K and PF1 of 86% and 80%, respectively, and CF1 of\\n57% on Arnetminer-L, as shown in Figure 9. In the case of ‘Sanjay Jain’, CF1 is as low as 7%. In this case, there are five\\nground truth clusters, but DISC found that there are 24 clusters. We carefully examine the citations of ‘Sanjay Jain’ andfound that he has multiple affiliations and worked on multiple research domains in his career with an entirely different\\nset of co-authors.\\n4.4.1. DISC comparison with graph-based methods. In Figure 10, DISC performance is compared with three graph-based\\nmethods – GHOST, ADANA and GFAD using Arnetminer-S. It shows overall better results than these methods and\\nachieves 12%, 8% and 5% higher in K-metric; 32.1%, 27.6% and 4.2% higher in CF1; and 3.7%, 6.3% and 1.2% higher\\nin PF1 as compared with GHOST, ADANA and GFAD, respectively. DISC performance is better as compared with\\nmethods due to the reason that GHOST, ADANA and GFAD are unable to detect outlier authors that only share one co-author with hub node, whereas DISC is able to identify these authors.\\nIn Figure 11, DISC performance is compared with the same methods using Arnetminer-L data set and again it shows\\noverall better results than these methods. It achieves 3.5%, 8% and 3.5% higher in K-metric; 10.5%, 8.8% and 5.3%\\nhigher in CF1; and 5%, 5% and 0% higher in PF1 as compared with GHOST, ADANA and GFAD, respectively.\\n4.4.2. DISC comparison with non-graph-based methods. In Figure 12, DISC performance is compared with two non-graph-\\nbased methods, HHC-CT and HHC-All using Arnetminer-S. DISC shows overall better results and achieves 7% and 16%\\nhigher in K-metric, 2.4 and same% higher in CF1 and 10.4% and 5.7% higher in PF1 as compared with HHC-CT and\\nHHC-All, respectively. DISC performance is better as compared with HHC-CT and HHC-All due to two reasons. First, it\\ndetects outlier authors that only share one co-author with hub node, and second, it does not do wrong merges on the basisof venue similarity.\\nComplete details of ground truth clusters and DISC-generated clusters of Arnetminer-S are shown in Table 5, whereas\\ntotals of the ground truth clusters and DISC-generated clusters are summarised in Table 6. About 77.5% clusters are\\nwithin the range (ground truth clusters ±3) in Arnetminer-S, that’s why DISC shows better CF1 than these methods.\\nSimilar results are achieved in the case of Arnetminer-L, where 73.2% clusters are within the range (ground truthclusters ±3).\\nWe also observe that DISC performs better on Arnetminer-S than Arnetminer-L. We suspect that it may be due to the\\nhigher ratio of authors with only one citation record of 23.2% on the Arnetminer-S than that of 5.1% on the Arnetminer-\\nl, which exerts a negative influence on the performance of DISC. DISC performance is better than all compared methods\\nexcept PF1 that is same to GFAD on Arnetminer-L, despite the fact that GHOST and ADANA used the predefined num-ber of clusters ‘K’.Hussain and Asghar 12\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011DISC is unable to identify when two different authors with the same name have different co-authors with the same\\nname which in AND literature are called very ambiguous authors [1]. However, analysis of both data sets shows that only\\n0.31% authors are very ambiguous authors, particularly some Asians names (Chinese and Korean).\\nTo show the scalability of DISC, comparison of running time of all methods on Arnetminer-S and Arnetminer-L is\\nshown in Table 7. All experiments were performed on a personal computer with Intel/C210Core/C228i5-5200U CPU @\\n2.20 GHz and 8 Gigabyte memory. All methods were implemented using Python. GFAD was the slowest among all thecompared on both data sets. Its most time-consuming stage is the cycle finding in the co-author’s graph. GHOST, HHC-\\nCT and DISC are the fastest on Arnetminer-S and Arnetminer-L, respectively. The reason why HHC-CT outperforms in\\nrunning time on Arnetminer-S is that there are simple comparisons between co-authors and title similarities of thepapers. GHOST is performing equally well to DISC as it mostly compares two or three path length nodes in this graph,\\nhowever, on larger path lengths such as Arnetminer-L GHOST takes more time than DISC. The running time of DISC\\nwas statistically tied with GHOST and HHC-CT on Arnetminer-S. DISC is about 11–14 times faster than GFAD andoverall comparable than all other methods.Table 3. The performance evaluation of DISC for each ambiguous group on the Arnetminer-S\\nName ACP AAP K PP PR PF1 CP CR CF1\\nAjay Gupta 1 0.64 0.80 1 0.59 0.74 0.31 0.62 0.42\\nBarry Wilkinson 1 1 1 111111\\nBin Zhu 1 0.85 0.92 1 0.72 0.83 0.68 0.87 0.76\\nBob Johnson 1 1 1 111111\\nCharles Smith 1 1 1 111111\\nCheng Chang 1 0.79 0.89 1 0.70 0.82 0.38 0.60 0.46\\nDavid Cooper 1 0.92 0.96 1 0.90 0.95 0.75 0.86 0.80\\nDavid Nelson 1 0.93 0.97 1 0.88 0.93 0.82 0.90 0.86\\nF . W a n g 1 1 1111111\\nFei Su 0.95 0.71 0.83 0.98 0.77 0.86 0.25 0.50 0.33Gang Luo 1 0.95 0.98 1 0.93 0.96 0.78 0.88 0.82\\nHong Xie 1 0.87 0.93 1 0.50 0.67 0.75 0.86 0.80\\nHui Fang 1 0.70 0.83 1 0.55 0.71 0.42 0.62 0.50\\nHui Yu 1 0.97 0.98 1 0.95 0.97 0.91 0.95 0.93\\nJ. Yin 0.95 0.52 0.70 1 0.20 0.33 0.91 0.95 0.93J. Guo 1 0.92 0.96 1 0.67 0.80 0.82 0.90 0.86\\nJianping Wang 1 0.63 0.80 1 0.56 0.72 0.50 0.80 0.62\\nJohn Hale 1 0.52 0.72 1 0.49 0.66 0.11 0.33 0.17Kuo Zhang 1 0.91 0.95 1 0.91 0.95 0.60 0.75 0.67\\nLei Fang 1 0.90 0.95 1 0.77 0.87 0.75 0.86 0.80\\nL e i J i n 1 1 1111111\\nM. Rahman 1 0.79 0.89 1 0.43 0.60 0.67 0.86 0.75\\nMichael Lang 1 0.73 0.86 1 0.56 0.72 0.50 0.75 0.60Michael Siege 1 0.83 0.91 1 0.83 0.91 0.36 0.67 0.47\\nMichael Smith 1 0.76 0.87 1 0.35 0.52 0.62 0.81 0.70\\nPaul Brown 1 1 1 111111\\nPaul Wang 1 0.91 0.95 1 0.90 0.95 0.71 0.83 0.77\\nPeter Phillips 1 0.77 0.88 1 0.74 0.85 0.20 0.33 0.25Ping Zhou 1 0.97 0.98 1 0.98 0.99 0.89 0.94 0.91\\nRichard T aylor 1 0.73 0.86 1 0.49 0.65 0.64 0.82 0.72\\nRobert Allen 1 1 1 111111\\nS. Huang 1 1 1 111111\\nThomas T aylor 1 1 1 111111\\nXiaoming Wang 0.97 0.86 0.92 1 0.94 0.97 0.62 0.71 0.67\\nXiaoyan Li 1 0.94 0.97 1 0.92 0.96 0.71 0.83 0.77\\nYan T ang 1 0.85 0.92 1 0.80 0.89 0.64 0.82 0.72\\nY oung Park 1 0.85 0.92 1 0.77 0.87 0.70 0.88 0.78\\nYue Zhou 1 0.66 0.81 1 0.52 0.68 0.46 0.67 0.55Z. Wang 0.93 0.91 0.92 0.57 0.40 0.50 0.78 0.80 0.79\\nDISC: disambiguating homonyms using graph structural clustering; ACP: average cluster purity; AAP: average author purity; K: K-metric; PP: pairwi se\\nprecision; PR: pairwise recall; PF1: pairwise-F1; CP: cluster precision; CR: cluster recall; CF1: cluster-F1.Hussain and Asghar 13\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/01655515187610115. Conclusion and future work\\nGraph structural clustering–based community detection algorithms are not used, to the best of our knowledge, in the\\nAND algorithms. In this article, we have proposed DISC which solves homonyms using only co-authors and titles that\\nare surely present in almost all bibliographic data sets. DISC is a graph-based algorithm that neither requires a costly\\ntraining data nor a priori information about how many ambiguous authors are there nor any web searches nor any expertknowledge. DISC pre-processes the bibliographic metadata and constructs the co-author’s graph in which an author is\\nrepresented by a vertex and co-author’s relationship with an edge. Potential homonyms are those vertices that are hub\\nnodes and can be split using the information of feature vectors of the titles of the publications of the clusters. We haveshown how blocking step reduces the disambiguation time, how a graph structural clustering algorithm can be used toTable 4. The performance evaluation of DISC for each ambiguous group on the Arnetminer-L\\nName ACP AAP K PP PR PF1 CP CR CF1\\nBin Li 0.96 0.84 0.90 0.94 0.77 0.85 0.72 0.84 0.78\\nDavid Brown 1 0.83 0.91 1 0.82 0.90 0.69 0.92 0.79\\nEric Martin 1 0.80 0.89 1 0.85 0.92 0.36 0.80 0.50\\nFeng Pan 1 0.66 0.81 1 0.54 0.70 0.45 0.71 0.56\\nGang Chen 1 0.69 0.83 1 0.47 0.64 0.60 0.86 0.70Hao Wang 1 0.69 0.83 1 0.55 0.71 0.46 0.71 0.56\\nJi Zhang 1 0.87 0.93 1 0.92 0.96 0.62 0.81 0.70\\nJie T ang 1 0.68 0.83 1 0.64 0.78 0.42 0.83 0.56\\nJie Yu 1 0.88 0.94 1 0.87 0.93 0.56 0.83 0.67\\nJose M. Garcia 1 0.61 0.78 1 0.58 0.74 0.14 0.50 0.22Kai Zhang 1 0.84 0.91 1 0.82 0.90 0.55 0.74 0.63\\nLei Chen 1 0.67 0.82 1 0.51 0.68 0.59 0.84 0.70\\nLei Wang 0.94 0.86 0.90 0.85 0.78 0.82 0.73 0.84 0.78\\nLi Shen 1 0.74 0.86 1 0.73 0.84 0.40 0.75 0.52\\nLu Liu 1 0.79 0.89 1 0.69 0.82 0.56 0.88 0.68Manuel Silva 1 0.72 0.85 1 0.66 0.80 0.33 0.75 0.46\\nRakesh Kumar 1 0.65 0.81 1 0.73 0.84 0.19 0.56 0.29\\nSanjay Jain 1 0.73 0.86 1 0.80 0.89 0.04 0.20 0.07Wei Xu 0.98 0.74 0.85 0.99 0.65 0.79 0.53 0.72 0.61\\nWen Jao 0.99 0.89 0.94 0.99 0.90 0.94 0.24 0.70 0.36\\nX. Zhang 0.86 0.91 0.89 0.57 0.77 0.65 0.75 0.73 0.74\\nYang Yu 1 0.76 0.87 1 0.63 0.77 0.52 0.79 0.62\\nY ong Chen 1 0.65 0.80 1 0.44 0.61 0.45 0.71 0.55Yu Zhang 0.98 0.69 0.82 0.96 0.52 0.67 0.50 0.72 0.59\\nDISC: disambiguating homonyms using graph structural clustering; ACP: average cluster purity; AAP: average author purity; K: K-metric; PP: pairwi se\\nprecision; PR: pairwise recall; PF1: pairwise-F1; CP: cluster precision; CR: cluster recall; CF1: cluster-F1.\\nFigure 8. DISC average ACP , AAP , K-metric, PP , PR, PF1, CP , CR and CF1 on Arnetminer-S.Hussain and Asghar 14\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011Figure 9. DISC average ACP , AAP , K-metric, PP , PR, PF1, CP , CR and CF1 on Arnetminer-L.\\nFigure 10. Average K-metric, PF1 and CF1 of the DISC, ADANA, GHOST and GFAD on Arnetminer-S.\\nFigure 11. Average K-metric, PF1 and CF1 of the DISC, ADANA, GHOST and GFAD on Arnetminer-L.Hussain and Asghar 15\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011detect and disambiguate homonyms and how title-based feature vector similarity can be used to reduce false splitting of\\nthe homonyms. DISC performance was tested on two Arnetminer data sets and it showed better results than three graph-based and two non-graph-based methods. DISC delivered a simple but effective and scalable solution to an author name\\nambiguity problem as it utilised the powers of graphs. However, it is unable to detect very ambiguous author cases where\\ntwo different authors with the same name work with two different co-authors with the same name. In the future, we planto analyse email address and affiliation of authors (if available) to overcome this limitation.Figure 12. Average K-metric, PF1 and CF1 of the DISC, HHC-CT and HHC-All on Arnetminer-S.\\nTable 5. Comparisons of ground truth clusters and DISC-generated clusters on Arnetminer-S\\nName GTC DGC Name GTC DGC Name GTC DGC\\nAjay Gupta 8 12 B. Wilkinson 1 3 Bob Johnson 3 3\\nBin Zhu 15 18 Cheng Chang 5 8 Michael Lang 4 3\\nCharles Smith 4 8 Paul Wang 6 6 Fei Su 4 2Michael Siege 6 8 David Cooper 7 9 Robert Allen 8 6\\nGang Luo 8 8 S. Huang 15 17 Hui Fang 8 10\\nJ. Guo 9 11 Hui Yu 20 25 Xiaoyan Li 6 5\\nYan T ang 11 8 Lei Fang 7 5 Ping Zhou 17 14\\nXiaoming Wang 14 20 Yue Zhao 9 11 Lei Jin 6 11Paul Brown 8 12 Peter Phillips 3 4 Thomas T aylor 3 8\\nDavid Nelson 10 14 F . Wang 14 12 Z. Wang 35 43\\nHong Xie 6 8 J. Yin 7 9 Jianping Wang 5 8\\nJohn Hale 3 6 Kuo Zhang 4 7 M. Rahman 7 6\\nMichael Smith 16 19 Richard T aylor 11 14 Y oung Park 8 9\\nDISC: disambiguating homonyms using graph structural clustering; GTC: ground truth clusters; DGC: DISC-generated clusters.\\nTable 6. Summary of ground truth clusters and DISC-generated clusters\\nData Set Ground truth clusters DISC-generated clusters\\nArnetminer-S 341 410\\nArnetminer-L 641 802\\nDISC: disambiguating homonyms using graph structural clustering.Hussain and Asghar 16\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011Declaration of conflicting interests\\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship and/or publication of this article.\\nFunding\\nThe author(s) received no financial support for the research, authorship and/or publication of this article.\\nNotes\\n1. http://dblp.uni-trier.de\\n2. http://www.medline.com3. http://citeseerx.ist.psu.edu4. http://arxiv.org5. http://academic.research.microsoft.com6. http://scholar.google.com.pk7. http://www.lbd.dcc.ufmg.br/bdbcomp\\nReferences\\n[1] Ferreira AA, Gonc xalves MA and Laender AH. A brief survey of automatic methods for author name disambiguation. SIGMOD\\nRec2012; 41(2): 15–26.\\n[2] Han H, Xu W, Zha H et al. A hierarchical naive Bayes mixture model for name disambiguation in author citations. In:\\nProceedings of the 2005 ACM symposium on applied computing , Santa Fe, NM, 13–17 March 2005, pp. 1065–1069. New\\nYork: ACM.\\n[3] Tang J, Fong AC, Wang B et al. A unified probabilistic framework for name disambiguation in digital library. IEEE T Knowl\\nData En 2012; 24(6): 975–987.\\n[4] Shin D, Kim T, Choi J et al. Author name disambiguation using a graph model with node splitting and merging based on biblio-\\ngraphic information. Scientometrics 2014; 100(1): 15–50.\\n[5] On BW, Elmacioglu E, Lee D et al. Improving grouped-entity resolution using quasi-cliques. In: Proceedings of the 6th interna-\\ntional conference on data mining (ICDM’06) , Hong Kong, China, 18–22 December 2006, pp. 1008–1015. New York: IEEE.\\n[6] Torvik VI, Weeber M, Swanson DR et al. A probabilistic similarity metric for Medline records: a model for author name disam-\\nbiguation. J Assoc Inf Sci Tech 2005; 56(2): 140–158.\\n[7] Bhattacharya I and Getoor L. Collective entity resolution in relational data. ACM T Knowl Discov D 2007; 1(1): 5.\\n[8] Fan X, Wang J, Pu X et al. On graph-based name disambiguation. J Data Inf Qual 2011; 2(2): 10.\\n[9] Han D, Liu S, Hu Y et al. ELM-based name disambiguation in bibliography. World Wide Web 2015; 18(2): 253–263.\\n[10] Ferreira AA, Veloso A, Gonc xalves MA et al. Self-training author name disambiguation for information scarce scenarios. J\\nAssoc Inf Sci Tech 2014; 65(6): 1257–1278.\\n[11] Wang X, Tang J, Cheng H et al. ADANA: active name disambiguation. In: Proceedings of the 2011 IEEE 11th international\\nconference on data mining , Vancouver, BC, Canada, 11–14 December 2011, pp. 794–803. New York: IEEE.\\n[12] Santana AF, Gonc xalves MA, Laender AH et al. Incremental author name disambiguation by exploiting domain-specific heuris-\\ntics.J Assoc Inf Sci Tech 2017; 68: 931–945.\\n[13] Shen Q, Wu T, Yang H et al. NameClarifier: a visual analytics system for author name disambiguation. IEEE T Vis Comput Gr\\n2017; 23(1): 141–150.\\n[14] Liu Y, Li W, Huang Z et al. A fast method based on multiple clustering for name disambiguation in bibliographic citations. J\\nAssoc Inf Sci Tech 2015; 66(3): 634–644.\\n[15] Maguire EJ. Ethnicity sensitive author disambiguation using semi-supervised learning. In: Proceedings of the 7th international\\nconference on knowledge engineering and semantic web (KESW 2016) , Prague, 21–23 September 2016, vol. 649, pp. 272–287.\\nCham: Springer International Publishing.\\n[16] Liu W, Islamaj Do g˘an R, Kim S et al. Author name disambiguation for PubMed. J Assoc Inf Sci Tech 2014; 65(4): 765–781.Table 7. Summary of all methods running time on both data sets (s)\\nData set DISC ADANA GHOST GFAD HHC-CT HHC-All\\nArnetminer-S 22 86 21 257 18 25\\nArnetminer-L 38 124 57 548 40 64\\nDISC: disambiguating homonyms using graph structural clustering; ADANA: active name disambiguation for the name ambiguity.Hussain and Asghar 17\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011[17] Tran HN, Huynh T and Do T. Author name disambiguation by using deep neural network. In: Proceedings of the Asian confer-\\nence on intelligent information and database systems , Bangkok, Thailand, 7–9 April 2014, pp. 123–132. Cham: Springer\\nInternational Publishing.\\n[18] Levin M, Krawczyk S, Bethard S et al. Citation-based bootstrapping for large-scale author disambiguation. J Assoc Inf Sci Tech\\n2012; 63: 1030–1047.\\n[19] Kang IS, Na SH, Lee S et al. On co-authorship for author disambiguation. Inform Process Manag 2009; 45(1): 84–97.\\n[20] Wu H, Li B, Pei Y et al. Unsupervised author disambiguation using Dempster–Shafer theory. Scientometrics 2014; 101(3):\\n1955–1972.\\n[21] Onodera N, Iwasawa M, Midorikawa N et al. A method for eliminating articles by homonymous authors from the large number\\nof articles retrieved by author search. J Assoc Inf Sci Tech 2011; 62(4): 677–690.\\n[22] Zhu J, Yang Y, Xie Q et al. Robust hybrid name disambiguation framework for large databases. Scientometrics 2014; 98(3):\\n2255–2274.\\n[23] Schulz C, Mazloumian A, Petersen AM et al. Exploiting citation networks for large-scale author name disambiguation. EPJ\\nData Sci 2014; 3(1): 11.\\n[24] De Carvalho AP, Ferreira AA, Laender AH et al. Incremental unsupervised name disambiguation in cleaned digital libraries. J\\nInform Data Manag 2011; 2(3): 289–304.\\n[25] Peng HT, Lu CY, Hsu W et al. Disambiguating authors in citations on the web and authorship correlations. Expert Syst Appl\\n2012; 39(12): 10521–10532.\\n[26] Tang L and Walsh JP. Bibliometric fingerprints: name disambiguation based on approximate structure equivalence of cognitive\\nmaps. Scientometrics 2010; 84(3): 763–784.\\n[27] Ferreira AA, Machado TM and Gonc xalves MA. Improving author name disambiguation with user relevance feedback. J Inform\\nData Manag 2012; 3(3): 332–347.\\n[28] Pereira DA, Ribeiro-Neto B, Ziviani N et al. Using web information for author name disambiguation. In: Proceedings of the 9th\\nACM/IEEE-CS joint conference on digital libraries , Austin, TX, 15–19 June 2009, pp. 49–58. New York: ACM.\\n[29] Huang J, Sun H, Song Q et al. Revealing density-based clustering structure from the core-connected tree of a network. IEEE T\\nKnowl Data En 2013; 25(8): 1876–1889.\\n[30] Elliott S. Survey of author name disambiguation: 2004 to 2010. Libr Philos Pract 2010, http://digitalcommons.unl.edu/libphil\\nprac/473/ (2010, accessed October 2016).\\n[31] Smalheiser NR and Torvik VI. Author name disambiguation. Annu Rev Inform Sci 2009; 43(1): 1–43.\\n[32] Momeni F and Mayr P. Evaluating co-authorship networks in author name disambiguation for common names. In: Proceedings\\nof the international conference on theory and practice of digital libraries , Hannover, 5–9 September 2016, pp. 386–391. Cham:\\nSpringer.\\n[33] On B-W, Lee D, Kang J et al. Comparative study of name disambiguation problem using a scalable blocking-based framework.\\nIn:Proceedings of the 5th ACM/IEEE-CS joint conference on digital libraries, 2005 (JCDL’05) , Denver, CO, 7–11 June 2005,\\npp. 344–353. New York: IEEE.\\n[34] Porter MF. An algorithm for suffix stripping. Program 1980; 14(3): 130–137.\\n[35] Cota RG, Ferreira AA, Nascimento C et al. An unsupervised heuristic-based hierarchical method for name disambiguation in\\nbibliographic citations. J Assoc Inf Sci Tech 2010; 61(9): 1853–1870.\\n[36] Johnson DB. Finding all the elementary circuits of a directed graph. SIAM J Comput 1975; 4(1): 77–84.Hussain and Asghar 18\\nJournal of Information Science, 2018, pp. 1–18 /C211The Author(s), DOI: 10.1177/0165551518761011', 'A survey of author name disambiguation techniques:\\n2010 –2016\\nIJAZ HUSSAIN and SOHAIL ASGHAR\\nDepartment of Computer Science, COMSATS Institute of Information Technology, Islamabad 45550, Pakistan;\\ne-mail: ijazhussain7979@hotmail.com, Sohail.Asg@gmail.com\\nAbstract\\nDigital libraries content and quality of services are badly affected by the author name ambiguity\\nproblem in the citations and it is considered as one of the hardest problems faced by the digital\\nlibrary researchers. Several techniques have been proposed in the literature for the author name ambigu-\\nity problem. In this paper, we reviewed some recently presented author name disambiguation techniques\\nand give some challenges and future research directions. We analyze the recent advancements in this\\nﬁeld and classify these techniques into supervised, unsupervised, semi-supervised, graph-based and\\nheuristic-based techniques according to their problem formulation that is mainly used for the author\\nname disambiguation. A few surveys have been conducted to review different techniques for the author\\nname disambiguation. These surveys highlighted only the methodology adopted for author namedisambiguation but did not critically review their shortcomings. This survey provides a detailed review\\nof author name disambiguation techniques available in the literature, makes a comparison of these\\ntechniques at an abstract level and discusses their limitations.\\n1 Introduction\\nIn this electronic era, digital libraries ’(DLs) importance in academics is growing in leaps and bounds due\\nto numerous factors such as cuts in budget for traditional libraries, nearly unlimited storage space at a\\nmuch lower cost, ease of use, no physical boundary, round the clock availability and advances in\\ninformation technology (Song et al., 2007; Palfrey, 2016; Weiss, 2016). DLs, for example, DBLP\\n1,\\nMEDLINE2, CiteSeer3arXiv4, MAS5, Google Scholar6, and BDBComp7are being extensively used by\\nthe researchers to ﬁnd scholarly literature for their research and discovery (Nicholson & Bennett, 2016).\\nIn addition to the literature search facility, these DLs also provide some useful analysis and information\\nfunctionality that is being used for better decision making by funding agencies and academic institutions\\nfor grants and individual ’s promotion decisions. It is presumed that these DLs contain and provide high-\\nquality content to its users, however, they failed to provide (Lee et al., 2007). Christen (2006) and Ferreira\\net al. (2012) in their studies stated that the main sources of errors in DLs are the typographical, scanning\\nand data conversion, ﬁnd and replace, copy and paste, meta data, imperfect citation-gathering software,\\ndisparate citation formats, ambiguous author names, the decentralized generation of content (i.e., by\\n1http://dblp.uni-trier.de\\n2http://www.medline.com\\n3http://citeseerx.ist.psu.edu\\n4http://arxiv.org\\n5http://academic.research.microsoft.com\\n6http://scholar.google.com.pk\\n7http://www.lbd.dcc.ufmg.br/bdbcompThe Knowledge Engineering Review , Vol. 32, e22, 1 –24. © Cambridge University Press, 2017\\ndoi:10.1017/S0269888917000182\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .means of automatic harvesting) and abbreviations of publication venue titles, etc. Among these sources of\\nerrors, a great attention is paid to the ambiguous author names from the research community due to its\\ninherent dif ﬁculty. Speci ﬁcally, name ambiguity arises when a set of citation records contains ambiguous\\nauthor names and it may appear in two different forms, in the ﬁrst form the same author name may appear\\nunder distinct names called synonyms, and in the second form distinct author names may have similar\\nnames referred to as homonyms (Shin et al., 2014). Author name ambiguity problem is closely related to\\nother research ﬁelds like entity disambiguation (Bhattacharya & Getoor, 2007; Murnane et al., 2013;\\nChisholm & Hachey, 2015; Krzywicki et al., 2016; Oramas et al., 2016; Zhu et al., 2016), instance\\nuniﬁcation (Aswani et al., 2006), authority control Carrasco et al. (2016), Web appearance disambiguation\\n(Bekkerman & McCallum, 2005), name disambiguation (On et al., 2005; Ferreira et al., 2012; Shin et al.,\\n2014) object distinction (Zhu & Li, 2013), semantic matching (Giunchiglia & Shvaiko, 2003), record\\nlinkage (Christen, 2006; Kum et al., 2014), name variant problem (Maguire, 2016), name aliasing problem\\n(Scholtes et al., 2016) and global names architecture (Pyle, 2016).\\nGenerally, author name ambiguity is resolved by means of different publication attributes such as\\nco-authors, title words, keywords, af ﬁliations, references, abstract words, venues and publication years\\n(Onet al., 2005; Elliott, 2010; Ferreira et al., 2010; Esperidião et al., 2014). However, each DL is not\\nproviding all these attributes, they only provide scant information about these attributes and manualannotation is not possible at such a large scale. Furthermore, in recent years an ever increasing amount of\\npublication data are being accepted and imported by DLs that makes names ambiguity problem more\\nsevere than in the past (Wang et al., 2011).\\nThe problem of author name ambiguity is illustrated with the help of DBLP. Recently, when we search\\nfor an author name ‘C Chen ’in DBLP (a renowned DL), we get more than 20,000 different author names\\nhaving same names or its variants, 137 different author names with exactly the same name ‘C Chen ’.\\nMethods that resolve name ambiguity problem in DLs are called author name disambiguation (AND).\\nWe reviewed some of these techniques according to the de ﬁned inclusion/exclusion principal given in section\\n2.2 in this survey and their key characteristics are analyzed, such as the proposed methodology, which type ofclustering or classi ﬁcation is used, applied similarity measures, metrics used for measuring the performance,\\nuncertainty handled or not, used data set, capabilities of the techniques, evidence used and limitations of these\\ntechniques. We classi ﬁed these AND techniques into ﬁve categories according to the applied AND technique.\\nDetails of these categories are given in section ‘A Classi ﬁcation for Author Name Disambiguation Techniques ’.\\n1.1 Why need another AND survey?\\nOther reviews/surveys on AND can be found in the literature (Torvik & Smalheiser, 2009; Elliott, 2010;\\nFerreira et al., 2012). Torvik and Smalheiser (2009) refuted the need of a universal author name identi ﬁ\\ner or\\nmanual disambiguation for AND. They reviewed some AND techniques since 2007. Another description of\\ntechniques during 2004 –2010 was done by Elliott (2010). She reassessed some individual efforts, some\\nmanual efforts and some projects that are working to resolve author name ambiguity problem like Authority,\\nLCAF and The Names Project. A brief survey on AND was presented by Ferreira et al. (2012). They classi ﬁed\\nthe AND techniques in to author assignment methods and author grouping methods. In author assignment\\nmethod a reference is directly assigned to an author using some type of machine learning (ML)-based model,\\nwhile in author grouping methods a similarity either prede ﬁned or proposed is used to group the citations.\\nHowever, their proposed taxonomy is not comprehensive enough and is insuf ﬁcient in the current literature.\\nWe propose a comprehensive taxonomy of AND techniques according to the used methodology —ML\\ntechniques (Wang et al., 2011; Huynh et al., 2013; Imran et al., 2013; Tran et al., 2014; Han et al., 2015; Seol\\net al., 2016) that have been further subdivided into three categories, and non-ML techniques (Fan et al., 2011;\\nWang et al., 2011; Tang et al., 2012; Shin et al., 2014) into two subcategories. Moreover, these all surveys\\nreviewed the techniques up to the year 2012. There has been an extensive research effort in recent years on the\\nsolution of author name ambiguity problem. Figure 1 shows the increased trend of AND methods8within the\\nlast 12 years. So, there is a need for a survey that incorporates recent techniques.\\n8Indexed by Google Scholar on October 1st, 2016I. HUSSAIN AND S. ASGHAR 2\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .A comparison of techniques from the past 6 years is done in the ‘Selective Author Name Dis-\\nambiguation Techniques ’. Some open challenges and future research directions are given in ‘Current\\nChallenges and Future Research Directions ’. Finally, we conclude the survey with a summary of presented\\ntechniques in ‘Conclusions ’.\\n2M e t h o d s\\nTo conduct this AND survey, we adopted the systematic literature review methodology proposed by\\nKofod-Petersen (2012). They devised a three-step methodology —planning, conducting and reporting.\\n2.1 Research questions\\nTo achieve the aim of this review following research questions are thus posed as follows:\\n1. What are the existing techniques to AND?\\n2. What methodologies are followed in these AND techniques?\\n3. What implications will these ﬁndings have, when creating new AND systems?\\n4. What are the limitations of these AND techniques?\\n2.2 Research strategy\\nIn search of the answers to the research questions identi ﬁed in section ‘Research Questions ’, relevant data\\n(papers) for AND was collected using online DLs: ACM, DBLP, Google Scholar, IEEE Xplore,\\nSpringerLink, CiteSeer and ISI Web of knowledge. The ﬁrst keyword that we used for data retrieval is\\n‘author name disambiguation ’. Subsequently, we chose the keywords from these set of initial papers and\\nfound iteratively more papers. Keywords used in these searches were, for example, DL, name dis-\\nambiguation, clustering, classi ﬁcation, synonyms, homonyms, citation analysis, bibliographic citation,\\ncommon names, data integration, link discovery, mixed and split citations, and namesake resolution.\\nFurthermore, we also used ‘AND ’and‘OR’of these keywords. We followed the literature search strategy\\nof Moher et al. (2009). After this search strategy, we found 159 different papers. Then, we found 16 more\\npapers by manually reading and noting the references of ﬁrst set of papers and then retrieving those papers.\\nWe collected a set of 175 papers, out of which we selected only 31 papers that ful ﬁlls our designed\\nselection criteria. So, ﬁnally 31 articles form the data set to answer the posed research questions in thisFigure 1 Number of author name disambiguation publications in last 12 yearsA survey of AND techniques: 2010 –2016 3\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .survey. We strictly focused on recent works (past 6 years ’only) that used author name ambiguity and we\\ndo not included works that used only MEDLINE/PubMed data sets and also not those that mainly focused\\non other research areas like record linkage, authority control, entity resolution and instance uni ﬁcation,\\netc., as our selection criteria. The details of every step can be shown in Figure 2.\\n2.3 Comparison criteria\\nWe compared these AND techniques according to the following quality criteria. These quality criteria may\\nnot fully characterize AND techniques but with the help of these characteristics, we may be able to\\nsomehow compare the performance of several AND techniques. We brie ﬂy discuss one by one these\\nquality criteria as follows.\\na. Capability: AND techniques have two main problems: homonyms and synonyms. If a method handles\\nboth of these problems than it is better than the method that only deals with the single problem.\\nb. Evidence: It is related to the requirement of the attributes used in the method, either the primary\\nevidence is required only or secondary evidence is also needed. The primary evidence means attributes\\nreadily available in citation records and secondary means auxiliary attributes that are not present in\\ncitation record and needs further processing.\\nc. Uncertainty: In AND, missing data about some attributes is called uncertainty. A technique is\\nconsidered good if it is robust to these uncertainties.\\nd. Preliminary: This characteristic gives detail about the requirements of the human intervention like some\\ntechniques require user feedback, others require setting the threshold. Many techniques require Web\\ndata, some need complete data, others require total ordering of rules, and still others require citations\\nshould be greater than two. Techniques that require no preliminary data are good techniques.\\ne. No. of Ambiguous Authors ‘K’: Number of ambiguous authors ‘K’is known in advance or not. Some\\ntechniques estimate the number of unknown authors. Those techniques are better which do not require\\nIdentification Screening Eligibility IncludedRecords identified through ACM, DBLP, \\nGoogle Scholar, IEEE Xplore, SpringerLink,\\nCiteSeer and ISI web of knowledge \\n(159)Additional records identified\\nthrough other sources\\n(16)\\nRecords screened\\nTitle and abstract\\n(141)Records excluded\\n(40)\\nFull-text articles assessed for\\neligibility \\n(101)\\nStudies included in\\nquantitive synthesis\\n(31)Full-text articles excluded\\n(69)\\nReasons:\\n30 are older than 2010\\n26 not related to author\\nname disambiguation\\n11used MEDLINE/PubMed\\ndata set\\n2 are low quality papersRecords removed\\n(34)Records after duplicate removed\\n(175)\\nFigure 2 Literature review and selection processI. HUSSAIN AND S. ASGHAR 4\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .number of ambiguous authors in advance because in real world we have no clue about how many\\nambiguous authors in a DL.\\nf. Limitations: Limitations are some issues or drawbacks of the presented techniques.\\n2.4 Data sets and evaluation metrics\\nIn AND three data sets —DBLP9, Arnetminer10and BDBComp11—are used frequently and considered as\\nbench mark data sets. They are publicly available and can be downloaded from respective websites. How-\\never, some researchers used other data sets such as Microsoft Academic Search, KISTI, ArXive, and\\nMEDLINE. DBLP is the most used collection that is composed of 8442 citation records associated with 480\\ndistinct authors belonging to 14 ambiguous groups, which means an average of ~18 citation records per\\nauthor, as shown in Table 1. The original version of this collection was created by Han et al. (2004), and, with\\nslight variations, it has been used in performance evaluation for various author disambiguation methods (Peng\\net al., 2012; Ferreira et al., 2014; Shin et al., 2014; Wu et al., 2014; Zhu et al., 2014; Han et al., 2015; Liu &\\nTang, 2015). Han et al. (2004) created this collection by collecting bibliographic citation records from DBLP\\nand authors homepages. After that, they transformed author names to abbreviated forms consisting of the ﬁrst\\nname initial and the last name, and clustered the bibliographic citation records into ambiguous groups, each of\\nwhich corresponds to the authors with the same abbreviated name.\\nAn excerpt of the Arnetminer data set and its statistics are given in Table 2 and more details are found\\nonline. The original version of this collection was created by Wang et al. (2011). Then, Tang et al. (2012)\\nmanually checked, labeled and included more ambiguous authors to expand this data set. It is used with\\nslight variations in many AND studies (Tang et al., 2012; Shin et al., 2014; Wu et al., 2014). Subsets of\\nthis data set have also been used in other works (Han et al., 2004, 2005, 2015; Ferreira et al., 2010).\\nBDBComp is relatively a small data set of 363 records belonging to 184 distinct authors, but it is very\\ndifﬁcult to disambiguate as some authors have only one citation record. BDBComp data set statistics are\\ngiven in Table 3 and this collection has also been frequently used in AND studies (Levin & Heuser, 2010;De Carvalho et al., 2011).\\nMetrics are important to judge the quality, performance, ef ﬁciency or progress of a process, product or\\nplan. The number of Man of the matches or batting average of a cricket player is a well used metrics to rank\\ncricket players, and surely for event sponsors to rate players. In all disciplines, metrics play pivotal role to\\naccess the performance. All Sciences including bibliometrics, scientometrics or informetrics use different\\nquality metrics. However, for comparison we are using the following metrics to measure the performance\\nof our proposed system with that of baseline methods.\\nMostly, three metrics —K-metric, pairwise-F1 (PF1), and cluster-F1 (CF1) are used to measure the\\neffectiveness of AND techniques (Pereira et al., 2009; Shin et al., 2014).\\n2.4.1 K-metric\\nK-metric is de ﬁned as the geometric mean of the average cluster purity (ACP) and the average author\\npurity (AAP). ACP evaluates the purity of the algorithm-generated clusters with respect to the gold\\nstandard reference clusters, so it measures the amount of data misclassi ﬁed into the wrong clusters by\\nchecking whether the generated clusters include only the publication records belonging to the referenceclusters. AAP evaluates the level of splitting author information into several clusters, so it checks how\\nfragmented the generated clusters are. ACP, AAP and K-metric are expressed in the following equation:\\nACP=\\n1\\nNXR\\nr=1XS\\ns=1n2\\nrs\\nnr;AAP=1\\nNXS\\ns=1XR\\nr=1n2\\nrs\\nns;K=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nACP ´AAPp\\n(1)\\nhere, Ndenotes the size of the citations in the collection, Sthe number of gold standard reference clusters\\nmanually generated, and Rthe number of clusters automatically generated by the Proposed Algorithm.\\n9http://dblp.uni-trier.de\\n10https://aminer.org/\\n11http://www.lbd.dcc.ufmg.br/bdbcompA survey of AND techniques: 2010 –2016 5\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Table 2 The Arnetminer data set\\nNames Aut. Rec. Name Aut. Rec. Name Aut. Rec.\\nAjay Gupta 11 93 Lu Liu 17 58 Mark Davis 6 24\\nBin Zhu 15 49 Cheng Chang 5 27 Michael Lang 4 17\\nCharles Smith 4 7 David Brown 25 61 Lei Chen 36 192Michael Siege 6 54 David Cooper 7 18 Ning Zhang 31 125\\nDavid Wilson 5 67 R. Ramesh 9 46 Paul Wang 7 16\\nEric Martin 5 85 Fei Su 4 37 Robert Allen 9 24Yu Zhang 72 236 Gang Luo 9 47 S Huang 13 14Sanjay Jain 4 216 Hui Fang 8 42 J. Guo 10 13\\nHui Yu 21 32 Jie Tang 6 66 Ji Zhang 99 293\\nXiaoyan Li 6 33 Yang Yu 19 71 Wen Jao 9 487Jie Yu 9 32 John F. McDonald 2 34 X. Zhang 40 62\\nBo Liu 65 306 Lei Wang 109 307 Yan Tang 6 27\\nKai Zhang 28 82 Lei Fang 7 17 Wei Xu 47 153Bin Li 65 306 Ping Zhou 18 36 Xiaoming Wang 14 41\\nM. Wagner 14 71 Rakesh Kumar 10 96 Lei Jin 6 16\\nPaul Brown 7 26 Shu Lin 2 76 Li Shen 6 65Peter Phillips 3 13 Thomas D. Taylor 3 4\\nAut. denotes the number of distinct authors and Rec. represents citation records associated with that author.\\nTable 3 The BDBComp data set\\nS No. Ambiguous group Total records Distinct authors\\n1 A. Oliveira 52 16\\n2 A. Silva 64 32\\n3 F. Silva 26 204 J. Oliveira 48 185 J. Silva 36 17\\n6 J. Souza 35 11\\n7 L. Silva 33 188 M. Silva 21 16\\n9 R. Santos 20 16\\n10 R. Silva 28 20Table 1 The DBLP data set\\nS No. Name No. of authors No. of citation records\\n1 A. Gupta 26 577\\n2 A. Kumar 14 2443 C. Chen 61 800\\n4 D. Johnson 15 368\\n5 J. Lee 100 14176 J. Martin 16 112\\n7 J. Robinson 12 171\\n8 J. Smith 31 9279 K. Tanaka 10 280\\n10 M. Brown 13 153\\n11 M. Jones 13 25912 M. Miller 12 41213 S. Lee 86 1458\\n14 Y. Chen 71 1264\\nTotal 480 8442I. HUSSAIN AND S. ASGHAR 6\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Also, nsis the number of elements in cluster sandnrsthe number of elements belonging to both\\ncluster rand cluster s.\\n2.4.2 PF1\\nPF1 is de ﬁned as the harmonic mean of pairwise precision (PP) and pairwise recall (PR). PP is the fraction\\nof publication records corresponding to the same author in the algorithm-generated clusters and PR is the\\nfraction of publication records associated with the same author in the gold standard reference clusters. The\\nPP, PR and PF1 measures are expressed in Equation (2), where C(n,r) denotes the number of combina-\\ntions of relements from nelements: Cðn;rÞ=n!\\nr!ðn/C0rÞ!;n≥r. Other parameters including r,s,nsandnsrare\\ndeﬁned as before in Equation (1).\\nPP=PR\\nr=1PS\\ns=1Cðnrs;2ÞPR\\nr=1Cðnr;2Þ;PR=PR\\nr=1PS\\ns=1Cðnrs;2ÞPS\\nr=1Cðns;2Þ;PF/C01=2´PP´PR\\nPP+PR(2)\\n2.4.3 Cluster F1 (CF1)\\nCF1 is de ﬁned as the harmonic mean of cluster precision (CP) and cluster recall (CR), where CP is the\\nfraction of the generated clusters that are equal to the reference clusters and CR is the fraction of correctly\\nretrieved clusters from the reference clusters. The CP, CR and CF1 measures are given in the following\\nequation:\\nCP=R\\\\S\\nR;CR=R\\\\S\\nS;CF/C01=2´CP´CR\\nCP+CR(3)\\n2.5 Contributions\\nWe reviewed these selected AND techniques, their prominent features, such as capabilities, limitations and\\nbrieﬂy included some open challenges that needs researchers attention. We deliberately not include AND\\nworks that are older than 6 years now and have been discussed in some other surveys. We believe that this\\narticle will be useful for future researchers who are going to carry out research in AND domain. Moreover,\\nour contribution complements the existing surveys by presenting: (a) an overall look of more recent AND\\ntechniques, (b) classi ﬁcation of AND techniques based on presented solution method, (c) complete\\nsynthesis of the AND techniques, and some open challenges and future research directions related to AND\\nare also discussed.\\n3 A Classi ﬁcation of author name disambiguation techniques\\nAND techniques can be classi ﬁed in many ways but we classi ﬁed them into: ML-based techniques (Wang\\net al., 2011; Huynh et al., 2013; Imran et al., 2013; Tran et al., 2014; Han et al., 2015; Seol et al., 2016)\\nthat has been further divided into three subcategories, and non-ML-based techniques (Fan et al., 2011;\\nWang et al., 2011; Tang et al., 2012; Shin et al., 2014) into two subcategories. The proposed classi ﬁcation\\nof selected AND techniques is shown in Figure 3.\\nML techniques construct models based on prior observations which can then be used to predict the class\\nof unseen data (Provost & Kohavi, 1998). The model is constructed using a learning process that mines\\nvaluable information about the data using the previous observations. ML methods usually receive a set ofcitations data for training and learns a model. The learned model is then applied to unseen data to attempt\\nto guess the correct values. ML methods used in AND are of three types as described in the following\\nparagraphs.\\n3.1 Supervised AND techniques\\nTheﬁrst ML techniques are supervised techniques in which labeled training data are manually created and\\ninputted to the classi ﬁer. The data consist of pairs of the form 〈A\\ni,Bi〉, where Aiis input feature vector and\\nBiis correctly labeled output class. The objective of learning function in supervised learning is to map\\ninput attributes to correct output class value. With the help of training data, a classi ﬁcation model is trainedA survey of AND techniques: 2010 –2016 7\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .and validated using some validation technique such as ‘k’fold cross-validation. Here ‘k’can be any\\nnumber from 2 to N. Then the trained model is used to predict the output of unseen data. For example, the\\ninput can be a set of publication features like co-authors, title words, year of publications and venue\\ninformation and the output can be the true author name class. True authors are target authors and false\\nauthors are homonym/synonyms authors of the target authors.\\n3.2 Unsupervised AND techniques\\nThe second ML techniques are unsupervised techniques in which the learner is provided with input data,\\nwhich has not been labeled. The aim of the learner is to ﬁnd the intrinsic patterns in the data that can be\\nused to determine the correct output value for new data instances. The assumption here is that there is a\\nstructure to the input space, such that certain patterns occur more often than others, and we want to see\\nwhat generally happens and what does not. In statistics, this is called density estimation . A variety of\\nunsupervised learning algorithms has been used in AND. In these techniques, some prede ﬁned similarity\\nmeasures or some similarity functions are used for forming and ﬁnding the clusters of ambiguous author\\nnames. Key challenges to this type of technique are how to ﬁnd the number of clusters and the value of the\\nsuitable threshold for similarity.\\n3.3 Semi-supervised AND techniques\\nThe main disadvantage of the supervised technique is that they require a large amount of labeled training\\ndata. Similarly, ﬁnding number of hidden clusters and similarity threshold in unsupervised techniques is\\ndifﬁcult. To overcome these drawbacks of supervised and unsupervised techniques, semi-supervised\\ntechniques are introduced. In these techniques, authors want to achieve good accuracy by using a small\\namount of labeled training data in conjunction with unlabeled data. In nearly all semi-supervised techni-\\nques one assumption made about the data consistency that data close to each other or having similar\\nstructure are likely to have the same label. Some good results have been reported in the literature for this\\ntype of techniques and in this regard, a variety of algorithms has been developed for AND in literature\\n(Imran et al., 2013; Zhao et al., 2013; Maguire, 2016).\\n3.4 Graph-based AND techniques\\nThese are only some techniques Tang and Walsh (2010) and Shin et al. (2014), which can be purely called\\ngraph-based techniques. Otherwise, the majority of techniques such as, Fan et al. (2011) and Wang et al. (2011)AND Taxonomy\\nNon Machine Learning \\nTechniquesMachine Learning TechniquesUn-Supervised \\nML TechniquesSemi-\\nSupervised ML \\nTechniquesSupervised ML \\nTechniques\\nGraph-based\\nTechniques Heuristic-\\nbased \\nTechniquesNaïve BayesRandom Forest\\nBoosted TreeExtreme Learning Machine\\nDeep Neural Network\\nLogistic Regression Support Vector MachineDecision Tree Dempster Shafer Theory+ \\nShannon Entropy\\nAgglomerative \\nClustering\\nApproximate Structure Equivalence Markov Random FieldsHierarchical Clustering\\nWeb Page Genre IdentificationLogistic Regression + \\nCommunity DetectionSupport Vector Machine \\n+ Community Detection\\nBoosted Tree + \\nAgglomerative Clustering\\nWeb and Author\\nCo-relation\\nGraph Partitioning & Merging\\nGraph Node Splitting &\\nMerging \\nAffinity Propagation ClusteringMatch FunctionsPairwise Factor Graph\\nString Processing Meta Path-based Ranking\\nFigure 3 Proposed author name disambiguation taxonomy. ML =machine learningI. HUSSAIN AND S. ASGHAR 8\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .form a graph and then ﬁnd some similarity in them. Subsequentl y, they cluster these results by using ML\\ntechniques to disambiguate the author names. However, we have classi ﬁed both these techniques under the\\numbrella of graph-based techniques. Some authors say that graphs are a natural representation of author name\\nambiguity problem and it takes into consideration the semantics of the problem. They represent author names as\\nnodes and their co-author relationships as edges between them. After constructing graph some graph-based\\nsimilarity measures are applied for disambiguation.\\n3.5 Heuristic-based AND techniques\\nWhen exact solutions are not possible or it is too slow to get the exact solution then heuristic-based\\ntechniques come into the scene and give solution quickly. It is worth noting here that these solutions may\\nnot be an optimal but they are near to an optimal. These methods are sometimes called short cuts, the rule\\nof thumb or observation based. A heuristic function searches the solution space on the basis of available\\ninformation and decides to go to the branch that approximates the exact solution. These methods also at the\\nend may use ML techniques to resolve the ambiguity of author names.\\n4 Selective author name disambiguation techniques\\nIn this section, selected AND techniques are elaborated, compared and their ﬁndings are presented.\\n4.1 Supervised AND techniques\\nWang et al. (2012) presented a boosted tree classi ﬁcation method for name disambiguation that comprises\\nof four steps. In the ﬁrst step, name and af ﬁliation ﬁltering are done by matching ﬁrst initial and last name\\nand af ﬁliation matches, whereas similarity scores for six different publication features are calculated in the\\nsecond step. Then author name screening is done using false rate and in the last stage boosted tree classi ﬁer\\nis applied to the manually crafted data set of 100 authors having 4253 citations in it. It cannot classify high\\nfalse rate authors and requires manual checking. A deep neural network-based approach is proposed by\\nTran et al. (2014) to automatically learn features and disambiguate the author names from any data set.\\nHowever, they tested their proposed solution with Vietnamese ’s data set that they prepared and used in\\ntheir earlier study. The architecture of this solution has two main components. In the ﬁrst component, data\\nis taken as input and data representations are computed. These data representations can be prepared in\\nmany ways but authors ’used string matching technique. According to their claim, these computations can\\nbe done on any data set automatically. The second component take the basic feature set as its input and\\nlearn the features in it is hidden layers to disambiguate the author name. The last layer of the feed forward\\ndeep neural network computes the probabilities to ﬁnd whether two instances of the author name in a pair\\nbelongs to the same author name or not. They utilize the multi-column deep neural network technique to\\nimprove the generalization capabilities of the system that is similar to ensemble method bagging. Optimal\\nno of hidden layers and no of units is a complex task and requires skill and experience.\\nTwo extreme learning machine-based algorithms for AND are proposed by Han et al. (2015). First is\\none classi ﬁer for each name (OCEN) and the second is one classi ﬁer for all names (OCAN). In OCEN for\\neach name a classi ﬁer is trained with the help of some attributes and when an unseen paper that is written\\nby these ambiguous authors ’is given to the classi ﬁer, it tells the identity of that author. They use a list of\\nauthor names, title words of papers and venue title words as attributes for OCEN and extract features fromthese three attributes. Then, they reduce the dimensionality of features via principal component analysis.\\nIn the end, they formulate and solve the optimization problem using extreme learning machine method. In\\nOCAN, they train a classi ﬁer that can predict whether the same author name in any given pair of entries in\\nthe bibliography referred to the same entity or two different entities. The idea behind this strategy is that an\\nentity pair provides an abstraction from the concrete names. So, this classi ﬁer is not concerned with any\\nspeciﬁc name, which enables it to disambiguate all names. In this strategy, they use similarity of the two\\nentities. They formulate the feature vector of similarities between author names, title words, and venue title\\nwords. They create enhanced feature extraction, which exploit additionally the information in the rela-\\ntionship of the entries referring to the same author name. Finally, they apply extreme learning machineA survey of AND techniques: 2010 –2016\\n9\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .method on the proposed problem and ﬁnd the solution. They compare its performance with support vector\\nmachine classi ﬁer and conclude that it has similar or better generalization performance. Missing data case\\nis not handled in this strategy. If a single author or two homonyms share the same or similar title it would\\nfail to distinguish.\\nA discrimination function that predicts true authors (target authors) and false authors (homonyms)\\nusing logistic regression on Web of Science data set is used by Onodera et al. (2011). They extract true\\nauthor papers from 629,000 retrieved papers by using two stage ﬁltering. In the ﬁrst stage, they remove the\\nretrieved papers if either their af ﬁliation addresses has low similarity score to those of its source papers or\\nthere is no citation relationship between the venue of the retrieved papers and that of source papers. During\\nthe second stage, retrieved papers are manually judged and on the basis of this judgment a discrimination\\nfunction of logistic regression is de ﬁned. The salient discrimination predictors are common co-author(s),\\nthe similarity of address, the similarity of title words, and citation relationships in venues between the\\nretrieved and source papers. This method is not good for papers whose subject ﬁelds and af ﬁliation\\naddresses vary a little or do not vary at all. Five supervised ML techniques: Random Forest, Support\\nVector Machine, k-Nearest Neighbors, C4.5 (decision tree) and naive Bayes are proposed by Huynh et al.\\n(2013) for solving Vietnamese author name ambiguity problem. They use Levenshtein similarity for\\ncalculating similarity between features. They propose a set of features from publications ’data set that can\\nbe used to assist training classi ﬁers. They test their model on three different data sets from the online DLs.\\nIt requires very speci ﬁc neat and clean data set for the training of the models.\\nTable 4 shows the comparison of supervised AND techniques.\\n4.2 Unsupervised AND techniques\\nWuet al. (2014) proposed algorithms for author name disambiguation that uses Dempster –Shafer theory\\n(DST) in combination with Shannon entropy (SE). In the ﬁrst step, some high-level features like af ﬁliation,\\nvenue, contentisim, co-authors, citations, Web correlation and their co-relation similarities are calculated.\\nIn next step, these features are fused using DST and SE. On the basis of this information, belief and\\nplausibility of each author is calculated. They get a matrix of pairwise correlation of papers. Each entry in\\nthis matrix is linked to a belief function and a plausibility function. Finally, they apply the DST-based\\nhierarchical agglomerative clustering algorithm for author name disambiguation. In the process of clus-\\ntering they use three different convergence conditions for clustering algorithm namely: pre-set number of\\nclusters, the number of available evidence and distance between clusters.\\nCognitive maps of psychology and structural equivalence of network analysis-based knowledge\\nhomogeneity scores are used to recognize the ambiguous author name in bibliographic, particularly\\ncommon surnames in China, is presented by Tang and Walsh (2010). Their basic assumption is that every\\nauthor has its particular set of the knowledge base at a particular time. During that time, two authors having\\nthe same knowledge base are considered the same author whereas two authors having different knowledge\\nbase are considered two distinct authors. Finding true structural equivalence in the real world is rare.\\nHence, approximate structural equivalent (ASE) is used instead of equivalence, such that authors within a\\nstructurally equivalent cluster are more similar to each other than to those outside of that cluster. If these\\nstructurally equivalent records contain author names with the same (or similar) family name and ﬁrst\\ninitial, these similar authors are taken to be the same authors. To ﬁnd the ASE, a knowledge homogeneity\\nsimilarity (KHS) score is calculated that is based on the summation of shared references, the forwardcitations of each reported reference, and the minimum number of references reported by the two articles.\\nRarer references are given higher weights. Once the KHS matrix is constructed, hierarchical clustering\\nwith single linkage is done to distinguish the groups. These methods do not perform well when references\\nof a citation are not available.\\nA uni ﬁed probabilistic framework to address the name disambiguation problem speci ﬁcally homonyms\\nin DLs is proposed by Tang et al. (2012). They formalize the disambiguation problem using Markov\\nrandom ﬁelds, in which the data are cohesive on both local attributes and relationships. In this approach,\\nthey suggest an algorithms that automatically estimate the number of unknown ambiguous authors and the\\nrequired unknown parameters. The basic idea behind this work is that a similar content and similarI. HUSSAIN AND S. ASGHAR 10\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Table 4 Supervised author name disambiguation techniques comparison\\nRef. Similarity Methodology Performance metric Uncer. Data set Evidence Capability Limitations\\nWang et al.\\n(2012)Cosine, eigen\\ndecompositionBoosted tree\\nclassi ﬁcationPrecision, recall and\\nmisclassi ﬁcation\\nerrorNo Not standard Author, venues, keyword,\\ntitle word, abstract,subject categoryHomonyms How to decide the splitting point\\nand how to control the size ofthe tree\\nHanet al. (2015) TF-IDF ELM-based\\nclassi ﬁcationMean and SD of test\\naccuracyNo Self-designed\\nDBLPAuthors, co-authors, title\\nwords, venueHomonyms It is dif ﬁcult to decide network\\nstructure and its parameters\\nTran et al.\\n(2014)Jaccard, Jaro,\\nLevenshtein, Smith-\\nWaterman, Jaro-\\nWinkler, Mogne-ElkanDeep neural\\nnetworkAccuracy, error rate Yes IEEE Explore\\nVietnamese\\nauthors, MAS,\\nand ACMAuthor name, af ﬁliation,\\nco-author, interest\\nkeywordsBoth It requires retraining the model if\\nsome parameters are changed.\\nMany models for each author\\nOnodera et al.\\n(2011)Self-de ﬁned Logistic\\nregressionPrecision, recall,\\naccuracyNo WOS Co-authors, af ﬁliation,\\ncitation relationship, title\\nwords, co-citations, yearof publication, countryHomonyms Cannot solve transitivity\\nproblem. Fails if subject ﬁelds\\nand af ﬁliation do not vary\\ntoo much\\nHuynh et al.\\n(2013)Jaccard Random forest,\\nSVM, K-NN,\\ndecision trees,BayesAccuracy No ACM Vietnamese\\nDS, IEEE, MASAuthors, co-author names,\\nauthors af ﬁliations,\\nkeyword similarityBoth Cannot handle outliers\\nFrom left to right: Ref. is the references of the paper, Similarity used, what Methodology followed, Performance Metrics, Uncer. refers to either Unc ertainty handled or not, Data set used,\\nEvidence used, Capability of the method and Limitations.A survey of AND techniques: 2010 –2016 11\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .relationship belongs to the same author. They claim that their approach can achieve better performance in\\nname disambiguation than baseline methods because the approach takes advantage of interdependencies\\nbetween paper assignments. In the ﬁrst step, they assign six attributes (Title word, venue name, Publica-\\ntions’year, Abstract, Authors, References) to each paper that they acquire from the online DLs. They\\ndeﬁneﬁve types of relationships namely, co-pub venue, co-author, co-citation, constraints and time of\\nco-authorship among these papers, and gave unknown weights to these relationships which they ﬁnd in the\\nlater stages. They transform the content-based information and structure-based information into the hidden\\nMarkov random ﬁelds (HMRF) as feature functions. They de ﬁne an objective function as the maximum a\\nposteriori con ﬁguration of the HMRF. They use Bayesian information criterion to estimate the true\\nnumber of authors. To ﬁnd the unknown parameters they devised an algorithm that ﬁrst initializes and\\nassigns random values to unknown parameters and then updates these values for each function values to\\noptimize the objective function.\\nAnother algorithm that not only disambiguates author name problem but it also reconstructs the h-index\\nof the authors is proposed by Schulz et al. (2014). They apply this algorithm on a large scale data set of\\nWeb of Science. This algorithm consists of two main steps. In the ﬁrst step, they calculate the pairwise\\nsimilarity between all papers on the basis of number of shared co-authors, self-citations, common refer-\\nences and the number of papers citing both publications. Calculating these pairwise similarities betweenpapers, they construct a link between those publications that have similarity score greater than some\\nthreshold. Their aim is to ﬁnd a distinct author that they ﬁnd by knowing each connected component.\\nThese connect links form a number of clusters and a new similarity between these clusters is calculated.\\nA link between these clusters is constructed if this similarity is above a threshold as in the previous step.\\nThen these connected clusters are merged. These clusters are the set of papers by a unique author. They\\noptimize and validate the disambiguated authors by re-constructing their h-index. Information about\\nh-index is necessary for this system, but new researchers have no h-index information and this system does\\nnot disambiguate them well.\\nA technique named ‘Fast Multiple Clustering ’is presented by Liu et al. (2015) in which three-step\\nclustering is used to disambiguate author names. In the ﬁrst step, co-authors are used to ﬁnding clusters of\\nthe authors in which multiple relations such as paper related to authors and papers related to other papers\\nare found. On the basis of these relations, the related papers are clustered in different clusters. Further,\\nbigger clusters are formed on the basis of similarity between title words. Finally, venue information is used\\nto cluster the papers of the authors who tend to publish in the same venue but may have different title\\nwords. Authors claim that this technique is better in PF1 and faster as compared to three existing tech-\\nniques. Another advantage of their proposed technique is that it can be used for almost all bibliographic\\ndatabases.\\nComparison on different quality criteria of unsupervised AND techniques is given in Table 5.\\n4.3 Semi-supervised AND techniques\\nA hybrid name disambiguation framework that not only used the traditional information, co-authors, but\\nalso Web page genre information is proposed by Zhu and Li (2013). This framework consist of two main\\nsteps: Web page genre identi ﬁcation and re-clustering model. In Web page genre identi ﬁcation, returned\\npages are classi ﬁed either home page of an author or not. Those records that found at the authors ’home\\npage belonged to him and are disambiguated. The remaining ambiguous records are disambiguated usingco-authors ’information. Some records that neither disambiguated from co-authors ’names nor from Web\\ninformation are sent to re-clustering model. Then they build a graph ‘G’using all citation records in which\\neach vertex represents a citation record and each edge denotes the same co-authors relationship or they are\\nof the same domain. If there are many links present between two vertices, then they are considered to be\\nrelated to each other. They transfer this graph into a similarity matrix by using multi-dimensional scaling\\nalgorithm. This algorithm detect similarities among objects. They construct two-dimensional matrices for\\nco-authorship and topic-relationship and calculate the distance between two vertices with the help of\\nEuclidean distance. If the distance between citations is less than a speci\\nﬁc threshold then they are con-\\nsidered by the same author. One author might have more than one citation record on one personal page andI. HUSSAIN AND S. ASGHAR 12\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Table 5 Unsupervised author name disambiguation techniques comparison\\nReferences Clustering Similarity MethodologyPerformance\\nmetric Uncer. Data set Evidence Capability Limitations\\nWuet al. (2014) Hierarchical\\nagglomerativeJC, LD, Cosine Dempster –Shafer\\nTheory with Shanon\\nEntropyK, Pairwise-\\nF1Yes DBLP Co-author, Af ﬁliations, Venues,\\nContent Similarity, Citations,\\nWeb co-relationsBoth Low accuracy\\nSchulz et al. (2014) 2-Step\\nagglomerativeOwn de ﬁned 2-step agglomerative\\nclusteringPrecision,\\nh-index\\nerrorYes WoS Author, shared citations, self-\\ncitationsBoth H-index is not known for new authors\\nTang and Walsh\\n(2010)Hierarchical\\nagglomerativeKnowledge\\nhomogeneity score(KHS), Self-de ﬁned\\nsimilarityApproximate Structure\\nequivalence-basedKHSAccuracy No Limited\\n(owndesigned)Author name, references Both Homogeneity threshold varies from subject to\\nsubject\\nTang et al. (2012) – Cosine Uni ﬁed Probabilistic\\nframework with\\nMarkov random ﬁeldsPairwise\\naccuracy,\\nprecision,F1No Self-\\ndesignedAuthors, title words, abstracts,\\nreference, year, venueHomonym If relationship information is not available then\\nthis method would not perform well\\nDe Carvalho et al.\\n(2011)Hierarchical Fragment comparison,\\ncosine, TF-IDFHeuristic based K metric Yes BDBComp,\\nsyntheticAuthors, title words, venues Both It fails if similar author name but does not have\\nsimilarity, not have co-authors and not match\\nwith any existing cluster\\nLiuet al. (2015) Hierarchical Cosine Hierarchical clusters of\\nrelations between\\nauthors and papersPP, PR, PF1 No DBLP Authors, Title words, Venues Both It fails if authors with the same name have\\nsimilar research ﬁelds\\nFrom left to right: Reference of the paper, Type of clustering, Similarity used, what Methodology followed, Performance metrics, Uncer. refers to ei ther Uncertainty handled or not, Data Set\\nused, Evidence used, Capability of the method and Limitations of the methods.A survey of AND techniques: 2010 –2016 13\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .have other different citation records on another personal page that represent as two distinct authors which\\ncannot be fully handled by this approach.\\nLevin et al. (2012) presented a semi-supervised two-stage method to disambiguate authors in DLs. In\\ntheﬁrst stage, they used citation-based rules along with some other simple rules to create labeled training\\ndata automatically. Then these initial clusters are being used as a bootstrap to learn the similarity metrics\\nfor the second stage of agglomerative clustering. They used a large number of old as well as new featuresfor measuring the similarity between pairs of publications and solve the optimization problem to ﬁnd\\ndisambiguated authors. They evaluate this model on a data set collected from the Thomson Reuters Web of\\nKnowledge. Ferreira et al. (2014) proposed an enhancement to their previously proposed technique in\\n2010 named SAND (self-training associative name disambiguation) that consists of three steps. Pure but\\nfragmented clusters are generated in the ﬁrst step using co-authors heuristic. Representative clusters from\\ntheﬁrst step are used for the training of the model. In the last step, ﬁnding associative name disambiguator\\nand training of the model that detects the appearance of the new authors. Remaining clusters that are not\\nused for training of the model are used for testing purpose. They test SAND on DBLP and BDBComp data\\nsets and achieve better results than two supervised and two unsupervised approaches. Finding the pure\\nclusters in the ﬁrst phase is a tough task.\\nA semi-supervised approach for AND that utilizes Microsoft academic search data are proposed by\\nZhao et al. (2013). They pre-processed data set and found many useful features. They construct a co-\\nauthor-based bibliographic network and apply community detection algorithm. Support vector machine\\nand some other ML models are used to handle uncertainty in the data. They boost the performance of these\\nmodels with the help of a self-taught procedures. Lastly, they merge the results from different models and\\nachieved 0.98717 mean F-score on Microsoft academic search data set provided by KDD cup 2013. Very\\nrecently, an ethnicity sensitive method that mainly comprises of three parts is presented by Maguire\\n(2016). In the ﬁrst part phonetic-based blocking for similar author signatures is done. After blocking,\\nsupervised ML-based linkage function is proposed that exploited the ethnicity sensitive information.\\nAuthor names are divided into seven groups of ethnicity as White, Black, American Indian or AlaskaNative, Chinese, Japanese, Asian or Paci ﬁc Islanders and others using. For a pair of given names the\\nprobability of the ethnic group for both the names is estimated and on the basis of this information the\\norigin of the author is predicted. Finally, hierarchical agglomerative clustering is done on the basis of a\\ndistance between two pairs of publications ’linkage function. Web co-relations ’and author co-relations ’-\\nbased approach to measure similarities between publications for AND is presented by Peng et al. (2012).\\nThey use two assumptions, citations on the same Web page are related to the same author and citations\\nwith same rarer authors belong to the same author. They measure these both type of co-relations by\\npairwise similarity metrics by using modi ﬁed sigmoid function, cosine metric and name popularity metric.\\nGurney et al. (2012) developed an algorithm that address the issues of discarded records due to null data\\nﬁelds and their resultant effects on recall, precision, and F-measure. They implement a dynamic approach\\nto similarity calculations based on all available data ﬁelds. They use the Tani Moto coef ﬁcient for simi-\\nlarity calculations of (a) title words (b) abstract words (c) last names and ﬁrst initials of co-authors (d) cited\\nreferences in whole-string form (e) normalized author keywords (f) normalized indexed keywords\\n(g) normalized research addresses (h) venue. They also include average author contributions and age\\ndifference between publications, both of which has meaningful effects on overall similarity measurements,\\nresulting in signi ﬁcantly higher recall and precision of returned records. Logistic regression is then applied\\ntoﬁnd the unknown weights to all the parameters. In the ﬁnal stage, authors are found with the help of\\nBlondel community detection algorithm. This algorithm take into account the weighted edges of a network\\nand assign each node to a speci ﬁc community based on the surrounding nodes and its edge weights.\\nLogistic regression predicts the probability whether two publications are from the same author or not, butthe community detection algorithm works on the entire interconnected network of nodes or publications\\nand identi ﬁes the communities of papers belonging to unique authors. The results are presented from a test\\ndata set of heterogeneous catalysis publications and demonstrate signi ﬁcantly high average F1 scores and\\nsubstantial improvements compared with previous stand-alone techniques. Null combination code (NC) is\\nthe presence or absence of shared meta data in a citation. It is a numeric string where each number refers to\\naﬁeld present in the citation. For example, NC code 124 represents that title, abstract and author assignedI. HUSSAIN AND S. ASGHAR 14\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .keywords are present. A zero in this code means respective ﬁeld is not present in the citation. This method\\nrequires pre-checking of records and calculation of NCcode manually that is a tedious job.\\nA solution for AND is proposed by Imran et al. (2013) that can also be used as a wrapper service for\\nDLs. They use selected features of an author and citation record to disambiguate author names using\\nunsupervised techniques. First of all, they collect citation records of an ambiguous author name in which\\nboth the mixed citations and split citations exist. They make clusters of their citations on the basis of theirdisciplines and then divide these clusters into smaller ones using co-authors information. Afterward, these\\nsmall clusters are fused together on the basis of remaining evidence. In this phase, two clusters are fused\\nonly if their distance is reduced to a certain threshold limit. Meanwhile, they interact with users to further\\npurify the retrieved clusters, so users ’feedback directly in ﬂuences the results of the disambiguation. Wang\\net al. (2014) proposed a uni ﬁed semi-supervised framework which is capable of solving both homonyms\\nand synonyms. This framework uses semi-supervised approach to the solution of author names where\\nthere are no training data are provided. Multi-aspect similarity indicator and a support vector machine are\\napplied to fuse the attributes. In the ﬁnal step, a self-taught method is presented to resolve the ambiguities\\nin the authors to enhance the performance of the disambiguation system. A comparative analysis of semi-\\nsupervised AND techniques is done in Table 6.\\n4.4 Graph-based AND techniques\\nTwo multi-level scalable algorithms for AND are proposed by On et al. (2012). First is a multi-level graph\\npartitioning (MGP) algorithm, and the second is a multi-level graph partitioning and merging (MGPM)\\nalgorithm. Input data are represented in the form of af ﬁnity graph, where each entity denote the node and\\nrelationship between two nodes represented the edge. Edge weights between two entities are calculated\\nusing the term frequency inverse document frequency of all word tokens. These graphs are then level bylevel approximately partitioned into the smaller graphs using MGP schemes. Three steps are followed for\\nthe MGP algorithm: scaling-down, partitioning and scaling-up. During the scaling-down, the size of the\\ngraph is repeatedly decreased; in the clustering, the smallest graph is partitioned and in the scaling-up,\\npartitioning is successively re ﬁned to the larger graph. MGP has somewhat low accuracy as compared to\\nMGPM. To overcome this drawback they propose MGPM algorithm that is faster but more accurate graph\\npartitioning method. The MGPM algorithm is also a MGP algorithm but it allows multi-resolutions at\\nvariant levels. After every stage, the algorithm decide to go next level only if the partitioning step produces\\nmore inter-cluster edges than intra-cluster edges. Otherwise, the MGPM algorithm stop at the current level.\\nIn the end, the MGPM algorithm generate different levels of each branch and various resolutions for eachleaf node. After dividing graph into smaller sub-graphs, they combine the sub-graphs together if two sub-\\ngraphs has the biggest normalized cuts. They repeatedly merge the sub-graphs until the number of sub-\\ngraphs is equal to the given number of clusters ‘K’. The drawback of this method is that it assumed\\nprede ﬁned no of clusters.\\nCollaboration network of authors along with syntactic similarity between author names to disambiguate\\nauthor names in three different subsets of DL data sets is presented by Levin and Heuser (2010). They\\nassume that two syntactically similar authors are same if there are a close relationship and small distance\\nbetween them. They make a graph by considering two type of nodes; author nodes and paper nodes, edges\\nare relationships between papers and their writers. They de ﬁne multiple metrics to measure the closeness\\nand distance between authors. Finally, they de ﬁneﬁve matching functions based on these metricsone for\\nsyntactic matching and fours for relationship matching. Their research results depict that this approachsigniﬁcantly improve the performance of just syntax-based similarity measures. A graph-based method\\ncalled graphical framework for name disambiguation is proposed by Fan et al. (2011). In it they model the\\nrelationships among publications using undirected graphs. Authors are represented with a vertex and an\\nedge shows a coauthor ’s relationship in this graph. Then, they solve homonyms problem by iteratively\\nﬁnding valid paths, computing similarities, clustering with the help of af ﬁnity propagation algorithm and\\nin the last step using user feedback as a complementary tool to enhance the performance. They simulate\\nusing PubMed and DBLP data sets, and results reveal that their proposed approach is better both inA survey of AND techniques: 2010 –2016 15\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Table 6 Semi-supervised author name disambiguation techniques comparison\\nRef. Similarity Methodology Performance metric Uncer. Data set Evidence Capab. Limitations\\nZhuet al. (2014) Multi-dimensional\\nscalingWeb page genre identi ﬁcation based\\ngraph re-clusteringF1 Robust DBLP Explicit web genre Both Slow as using web information\\nGurney et al. (2012) Tani Moto coef ﬁcient Logistic regression & community\\ndetection algorithmPrecision, recall,\\nF-measureYes Self-designed\\nWoSTitle words, abstracts, keywords,\\ncitations, difference in years of\\npublication and average author\\ncontributionHomonyms Requires too many parameters\\nImran et al. (2013) Levenshetien Heuristic-based, unsupervised and\\nadaptivePrecision, recall,\\nF-1No DBLP Authors, af ﬁliations, title words, venues,\\nhome pageBoth It involves user on multiple\\nstages\\nZhao et al. (2013) Cosine, TF-IDF, LDA,\\ntanimotoCommunity detection and SVM\\nalgorithmsPrecision, recall,\\nF-1Yes MAS Title words, venue, keywords,\\nafﬁliations, publication yearBoth Complex rules and topic\\nmodeling needed\\nMaguire (2016) Cosine, Jaro-Winkler,\\nTF-IDFFirst gradient boosted tree applied\\non similar authors and then\\nagglomerative clustering is\\nperformedPairwise precision,\\nrecall, F-1No INSPIRE Authors, af ﬁliations, title words, venues,\\nabstract, keywords, collaborations,\\nreferences, subject, year differenceBoth Not applicable to all DL ’s\\ndata set\\nPeng et al. (2012) Modi ﬁed sigmoid\\nfunction, cosine, name\\npopularity measureWeb and authorship correlations Pairwise precision,\\nrecall, F1No DBLP Authors, title words, venues Both Inherently slow as using web\\ninfo\\nLevin et al. (2012) TF-IDF, cosine Self-citation clustering rules with\\nother rulesPairwise precision,\\nrecall, F-1No WoK Thomson\\nReutersTitle word, venue, authors, addresses,\\nafﬁliations, subject categories,\\ncitations languages, year,\\ncombinations of theseBoth Too much processing for large\\nset of features\\nFerreira et al.\\n(2014)Cosine, Euclidean First pure clusters of data are found\\nand model is trained, tested on\\nthemK, pairwise-F1 No DBLP,\\nBDBComp,\\nSyGARAuthors, title words, venues Both Threshold selection is a big\\nissue\\nWang et al. (2014) Cosine, Tanimoto,\\nTF-IDFSimilar authors share co-authors and\\nhave high topic similarityF1 No MAS Title words, af ﬁliation, keyword, venue Both Fails in case of sole authors\\nFrom left to right: Ref. is the references of the paper, Similarity used, what Methodology followed, Performance metrics, Uncer. represents either U ncertainty handled or not, Data set used,\\nEvidence used, Capab. refers to the Capability of the method and Limitations.\\nDL=digital libraries.I. HUSSAIN AND S. ASGHAR 16\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .precision and recall than DISTINCT —a baseline approach. This approach does not handle outliers and fail\\nin the case of sole authors.\\nIn a study by Liu and Tang (2015), a problem and knowledge domains-based bi-relational network\\n(BRN) framework for AND is proposed. The problem domain is used to construct basic BRN and\\nknowledge domain is exploited to elaborate the covert aspects of the network. Subsequently, they use a\\nrandom walk with a restart to ﬁnd the closeness between BRN nodes and af ﬁnity propagation algorithm for\\nclustering the obtained results from the previous step. A graph-based method —ADANA is proposed by\\nWang et al. (2011), in which they modeled pairwise factor graph that can be used to integrate several types\\nof features as well as user feedbacks into a uni ﬁed model. They de ﬁne three types of feature functions:\\ndocument pair, correlation and constraint-based feature functions. In document pair feature functions, they\\nfound known relationships from publications. In correlation feature functions, they found some hidden\\nfeatures with the help of known functions and in the constraint-based feature functions, the user is involved\\ninﬁnding the unknown features. Finally, they exploit active selection of the users ’corrections in an\\ninteractive mode to improve the disambiguation performance after some preliminary clustering results.\\nThey exploit some additional information such as af ﬁliation, references, etc. that is not present in\\nevery DL.\\nA topological –collaborative approach is presented to solve homonyms in DLs by Amancio et al. (2015)\\nin which they used topological features of the collaborative graph along with weighted collaboration\\npatterns among authors. In this technique, the ﬁrst step is the formation of the weighted network among\\nauthors according to the strength of their collaborations and then characterized the network using several\\ntopological features such as neighborhood degree, neighborhood strength, clustering coef ﬁcient, average\\nshortest path length, hierarchical measurements and locality index. Fuzzy K-NN is used in the classi ﬁer to\\ndisambiguate different homonyms. Authors validated proposed technique on a data set extracted from the\\narXiv repository and concluded that topological features enhance the accuracy of their results. Among\\nthese topological features, the average shortest path length is the most prominent feature for dis-\\nambiguation. A graph framework for author name disambiguation (GFAD) is presented by Shin et al.\\n(2014) that exploit authors, co-authors and paper title words information. They claim that this framework\\nis robust and domain independent because it requires only author names, co-author names and paper titles\\ninformation that is surely available in all DLs. They model the author name and co-author names in an\\nundirected graph, where a vertex denoted an author name and an edge indicated a co-author relationship.\\nHomonyms are resolved by splitting the vertex that is common to multiple non-overlapping cycles so that\\neach cycle corresponded to a unique author. The heteronymous name problem is handled by merging\\nmultiple author vertices into one by identifying those vertices that actually represent a single author with\\ndifferent names. Moreover, unlike other related studies, outlier issues are also handled in GFAD. It remove\\noutliers by comparing similarity among outliers with the help of cosine similarity. Experiments are con-\\nducted on two real world data sets of DBLP and Arnetminer. This method fails when there is less or no title\\nword similarity in two papers written by sole authors and also fails in the case of very ambiguous authornames. Furthermore, as it used Johnson (1975) algorithm for cycle ﬁnding, so it is computationally too\\nexpensive. All these graph-based methods in this section are compared in Table 7.\\n4.5 Heuristic-based AND techniques\\nINDi a solution for the existing cleaned DLs is presented by De Carvalho et al. (2011). Their proposed\\nsolution utilize similarity among bibliographic records and group the new records to authors with similar\\ncitation records in the DL or to new authors when the similarity evidence is not strong enough. Some\\nparticular heuristics are used for checking whether references of new citation records belong to pre-\\nexisting authors of the DL or if they belong to new ones (i.e., authors without citation records in the DL),\\navoiding running the disambiguation process in the entire DL. They run simulations on BDBComp\\ncollection and synthetic DSs to assess the effectiveness of their method. They compare its performance\\nwith an unsupervised method. They use synthetic data for incremental disambiguation purpose that gene-\\nrated data from existing data and does not portray the real world situation where many new authors also\\npublish.A survey of AND techniques: 2010 –2016 17\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Table 7 Graph-based author name disambiguation techniques comparison\\nRef. Graph Similarity Methodology Performance metric DS Evidence Capab. Limitations\\nOnet al. (2012) Co-authorship TF-IDF Graph partitioning and merging Precision, recall, F1 4 Small to large DS Documents Both Number of Clusters Required\\nShin et al. (2014) Co-authorship LCS, Cosine Graph Node Splitting & Merging K-metric, Pairwise-\\nF1, Cluster-F1DBLP & arnetminer Co-authors, title words Both Unable to disambiguate sole\\nauthors\\nFanet al. (2011) Co-authorship User de ﬁned Af ﬁnity propagation clustering Pairwise accuracy,\\nprecision F1Small standard DS Co-author graphs, user feedback Homonyms Slow due to user feedback\\nLevin and Heuser\\n(2010)Author social graph Levenshtein,\\ntrigramSyntactic relationship match function Accuracy,\\nprecision, F1Cora, BDBComp,\\nDBLPAuthors, title words Both If no relationship or low\\nsyntactic similarity then fails\\nWang et al. (2011) Pairwise factor\\ngraph (PFG)TF-IDF,\\ncosinePFG and interaction of user to enhance the\\nperformancePrecision, recall,\\nF-measurePublication, web\\npage and news\\nstoriesCitations, Co-authors,\\nco-venues, co-af ﬁliations, co-\\naff-occur, title word-sim, co-\\nhomepageHomonyms Not scalable due to huge\\nnumber of path calculations\\nLiu and Tang\\n(2015)Bi-relational Graph-based\\nclosenessBi-relational network is created, closeness\\nbetween nodes is Found for clustering– Citations Authors, title words, venues Both Knowledge base is required that\\nis not available in some DL\\nFrom left to right: Ref. is the references of the paper, Similarity used, what Methodology followed, Performance metrics, Data set (DS) used, Evidenc e used, Capab. refers to the Capability of the\\nmethod and method Limitations.\\nDL=digital libraries.I. HUSSAIN AND S. ASGHAR 18\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .A name matching framework for author name disambiguation for Microsoft Academic Search data set\\nis proposed by Chin et al. (2014) and it realize two implementations. This method has six stages in total. In\\ntheﬁrst stage, they decide that either the name is a Chinese name or non-Chinese. They group Korean,\\nSingaporean and Vietnamese names into Chinese. They built two dictionaries of Chinese names. In the\\nsecond stage, they clean and preprocess the citations data. In the third stage, they make blocks of similarauthor names. Author name blocks are created using a dictionary of (key, value) pairs. In this blocking\\nstrategy, a name is used as a key and the number of its occurrence as a value. For instance, an author name\\n‘Ajay Kumar Gupta ’has following keys. ‘Ajay’,’Kumar ’,’Gupta ’,’Ajay Kumar ’,’Ajay Gupta ’,’Kumar\\nGupta ’and‘Ajay Kumar Gupta ’. In this blocking scheme, the order of the names does not matter. During\\nthis stage, they take care of low recall and high false positives that both might degrade the performance of\\nthe overall AND system at later stages. In the fourth stage, they identify duplicates in the blocks. After\\nidenti ﬁcation, they split some names that are wrongly included in the blocking stage. In the ﬁfth stage, they\\nlink the names of authors to their identi ﬁers. Lastly, they merge the results of two predictions by using\\nbackground information and a ﬁltering process. This merging step boosts the F1 score.\\nSantana et al. (2015) proposed a combination of the domain-speci ﬁc heuristics to disambiguate authors\\nin bibliographic databases. The authors assume that a researcher ’s publication pro ﬁle may be characterized\\nby the terms of one ’s citations. The co-authors form the collaborations networks and title and venue terms\\nportray one ’s research interests. Using these three domain-speci ﬁc heuristics a similarity function is\\nproposed and if the similarity between two compared authors is greater than a speci ﬁed threshold than\\nthese authors are included in the training data set. It is also assumed that if there is a low similarity between\\na citation and its most similar group then this is a new author which is not already present in the training\\ndata set. In the ﬁnal step optimal threshold for different parameters are estimated using the standard\\nprocedure of cross-validation. Ranking-based name matching algorithm and a system called RankMatch is\\nproposed by Liu and Tang (2015). RankMatch has four steps. The ﬁrst step is preprocessing in which some\\ndata cleansing activities are done like noisy ﬁrst and last name and mistakenly separated or merged names\\nare identi ﬁed. The next stage is r-step in which recall of the system is enhanced. The next stage is the p-step\\nthat improves the precision of the system. In this step, they calculate different meta path-based similarity\\nmeasures. Then these ranking-based measures combine with string-based measures are used to remove the\\nconﬂictory names from the results of the previous stage. In the last, post-processing stage, the uncon\\nﬁdent\\nnames are removed from the system despite the fact that names are compatible and they might have\\nacceptable meta path similarity between names. They achieved good F1 score on MAS data set. Com-\\nparison on different quality criteria of heuristic AND techniques is given in Table 8.\\n5 Critical discussion\\nSupervised AND techniques require representative labeled training data. Creating labeled training data is\\nresource and time consuming and accurate labeling is often hard to achieve. For example, obtaining\\nsufﬁcient training data for AND in bibliography might require weeks to months to collect and then label\\nthat data. In some cases even in manual labeling, the data may not be correct due to lack of bibliographic\\nevidence. On the other hand, According to Ferreira et al. the performance of supervised AND techniques isTable 8 Heuristic-based author name disambiguation techniques comparison\\nRef. Similarity Heuristic Data set Limitations\\nChin et al. (2014) String comparison String processing MAS It only ﬁnds duplicates. Not\\ntell the No. of ambiguous authors\\nLiu and Tang (2015) Levenshtein,\\nSoundexMeta path-based\\nrankingMAS It fails on sole authors cases\\nDe Carvalho et al. (2011) String Comparison Publication features BDBComp No real world data set is used\\nSantana et al. (2015) Own de ﬁned Publication features DBLP, BDBComp,\\nKISTIFinding optimal threshold is a\\ndifﬁcult task\\nFrom left to right: Ref. is the Reference, Similarity used, what Heuristic followed, Data set used and Limitations.A survey of AND techniques: 2010 –2016 19\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .relatively better than unsupervised techniques Ferreira et al. (2014). In a supervised study by Wang et al.\\n(2012), author af ﬁliation history is required that is usually not available in DLs. In Han et al. (2015), their\\nproposed technique require a large number of training examples to train their model that is not available for\\na new or a rising researcher. Similarly, Tran et al. (2014) present a technique that requires retraining of the\\nmodel if some parameters are changed or new citation have been inserted in the DL.\\nUnsupervised AND techniques do not require any training data set. However, unsupervised learning\\nalgorithms also have some drawbacks, some of them are as under: (a) it is dif ﬁcult to decide which type of\\nclustering best suits to the problem at hand, (b) no of clusters (K) is not known in advance, (c) when to stop\\nthe clustering process is not known and (d) these techniques are not as ef ﬁcient as supervised ones.\\nThe technique proposed by Tang and Walsh (2010) used homogeneity score for clustering and its\\nthreshold vary from subject to subject which is very dif ﬁcult to ﬁnd. Tang et al. (2012) technique performs\\npoorly when relationship information is not available.\\nSemi-supervised AND techniques also require some training data as well as a number of clusters in\\nadvance. However, semi-supervised techniques like Zhu et al. (2014) and Peng et al. (2012) require Web\\ninformation that slows down their progress in DLs. Whereas, Imran et al. (2013) proposed method need\\nusers feedback for higher accuracy of the proposed system.\\nGraph-based AND techniques require fewer attributes than other techniques. These techniques has a\\nrigorous mathematical foundation and considered the semantics of the citations for disambiguation. GFAD\\nis unable to disambiguate very ambiguous author cases and authors having no common authors\\namong them.\\nHeuristic-based AND techniques are sometimes very ef ﬁcient and give a solution in a very straight\\nforward manner. However, it is possible these techniques might sometimes not give an accurate solution.\\nThese techniques do not have a rigorous mathematical foundation and consider some previous heuristic\\nlike coauthor inclusion, string processing or meta path-based similarity in AND. Liu and Tang (2015)\\nproposed method fail in the case of sole authors.\\n6 Current challenges and future research directions\\nIn spite of the development of a variety of AND techniques, a little effort has been focused on some of the\\nchallenges of the author name ambiguity problem. These challenges are the main reason behind the\\nunreliable AND solutions. We discuss some of them in following paragraphs.\\nSole author cases: Majority AND techniques considered that ambiguous author name can be dis-\\nambiguated by knowing his or her co-authors identity. However, these studies failed or produced inferior\\nresults when there were citations that only written by the sole author. This case even became more worst\\nwhen these sole authors had published only one or a few papers (Han et al., 2004; On et al., 2006; Levin &\\nHeuser, 2010; De Carvalho et al., 2011; Fan et al., 2011; Shin et al., 2014). Some new features or\\ntechniques need to be explored to handle these types of authors such as reference similarity, af ﬁliation\\ninformation or some hidden topics of the publications.\\nImbalance distribution of publications: According to a study by Torvik and Smalheiser (2009), 46% of\\nthe authors have written only one publication in their entire career. The authors ’distributions of pub-\\nlications follow power law distribution. According to this law, many authors have only a few publications\\nand a few authors have many publications. This imbalance citation distribution more hardens the AND\\nproblem.\\nScalability: The majority of existing AND techniques are only applied to speci ﬁc and limited scale data\\nset. When they are applied on a larger scale either they loose performance or even fail to disambiguate.\\nDue to the rapid increase in publications by academics in recent years, scalability of the AND techniques is\\nalso a challenging issue that needs attention and resolution. Schloss Dagstuhl Leibniz-Zentrum fr Infor-\\nmatik GmbH announced a project titled ‘Scalable Author Disambiguation for Bibliographic Databases\\n2015–2018’12in which scalability is the main focus of their project. In this regard new AND systems\\nshould be validated on huge data sets.\\n12https://www.dagstuhl.de/ueber-dagstuhl/projekte/autoren-disambiguierungI. HUSSAIN AND S. ASGHAR 20\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .No standard representation of names: Presently, every publisher has his own set of rules for author\\nname representation which makes AND more challenging and dif ﬁcult to resolve. Some writes as\\n‘FirstName, LastName ’, others write as ‘LastName, FirstName ’, still others write as ‘FirstNameInitial,\\nLastName ’. To make some cases of name ambiguity somewhat simpler and easier publishers should come\\nto an agreement on the standardization of representing names and then follow this standard. Some ongoing\\nefforts, such as ORCID, Scopus Author Identi ﬁer or Web of Science Researcher Id might eliminate this\\nissue (Arunachalam & Madhan, 2016; LaFlamme, 2016).\\nInterdisciplinary work: Many existing data sets that are used for AND techniques belong to Computer\\nScience. But, in this contemporary era, the boundaries between different knowledge areas are blurring\\nand interdisciplinary research is gaining popularity. Every discipline has its own pattern of a number of\\nco-authors like Medical Sciences have a lot of publications with sole authors, whereas Physics have\\npublications with a lot of co-authors. As recently reported in Nature news that ‘Physics paper sets record\\nwith more than 5,000 authors ’13. Multidisciplinary articles authored by many persons from multiple\\ninstitutions (nationwide or worldwide) may cause ‘multiple entity disambiguations ’problems.\\nIncremental disambiguation: It is constantly needed to insert new citation records in disambiguated DL\\nand each reference should be assigned to their respective ‘true’authors. One possible solution to this\\nproblem could be to re-run disambiguation technique after every insertion of new citation records into thedisambiguated DL. This solution is not possible due to two reasons: ﬁrst, DL usually composed of\\nthousands, sometimes even in several millions of records. Second, this could be applied only in the case of\\nunsupervised techniques. It is not possible to apply this strategy in supervised techniques because every\\ntime, we cannot retrain the classi ﬁer.\\nNew authors: A huge number of new researchers present their work in different venues. Nearly all AND\\ntechniques use static data or existing data. No one has considered the real world situation in which many\\nnew authors are publishing. AND techniques should be capable of doing correct predictions about these\\nnew authors publications that have not yet published any work (Ferreira et al., 2015). So, there is also a\\nroom for working on this kind of solution that takes into account the new authors situation in DLs. Thismight be a good future research direction for new AND techniques. A related work on creating a synthetic\\ndata set for this situation has been the subject of a study conducted by Ferreira et al. (2012) but still this is\\nan approximation to the situation and only creates synthetic data of existing authors in the DL data set.\\n7 Conclusions\\nA substantial review of a broad range of existing author name disambiguation techniques has been\\npresented, compared and classi ﬁed into categories on the basis of used techniques. Some representative\\nmethods of all categories are included in this survey. It was observed in the surveyed techniques that the\\nvast majority of the techniques resolved the ambiguity of author names by using some prede ﬁned simi-\\nlarity measures or by proposing new similarity functions and then clustering the results of these similarities\\nin citations data. Some methods disambiguate by using graph-based or community detection-based\\ntechniques. These AND techniques have been classi ﬁed in to ﬁve categories: supervised, unsupervised,\\nsemi-supervised, graph-based and heuristic-based. Supervised techniques have usually better performance\\non their training data set than other technique but they require costly training data for each class. Thesetechniques required that complete representation data of all target classes should be present in the training\\ndata set. Selection of suitable similarity measures and clustering technique in unsupervised techniques is a\\ndifﬁcult task. Semi-supervised works well in the case of a limited number of target ambiguous authors but\\nfails when the number of ambiguous authors increases. Graph-based methods are new and need some\\nmaturity and rigorous proof. Heuristic-based solutions not always work; sometimes they give very bad\\nresults than other techniques.\\nSome open challenges and future research directions that need some more attention from researchers\\nhave been discussed. The new and sole authors cases in citations were reviewed in the context of AND.\\n13http://www.nature.com/news/physics-paper-sets-record-with-more-than-5-\\n000-authors-1.17567A survey of AND techniques: 2010 –2016 21\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Similarly, the need for a standardized name for publishers is identi ﬁed. Scalability and interdisciplinary\\nresearch issues have been examined in detail.\\nDuring this review, we identi ﬁed that we cannot directly compare the performance of one method to other\\ndue to several reasons. One most important reason is there is no standard data set available on which all\\ntechniques can be compared, some used DBLP, others used Arnetminer, and still, others used MAS or CiteSeer\\nor BDBCOMP data set. Finally, some used their own de ﬁned, like Chinese, Korean or German data sets.\\nSimilarly, everyone used different p erformance metrics for comparisons. The majority of studies used a data set\\nof Computer Science ﬁeld, whereas now research trends have been revolutionized and new mixed ﬁelds are\\nemerging that made author name ambiguity problem even harder than it was in the previous decade. This survey\\nwill de ﬁnitely provide new avenues for researchers t o advance and discover some AND techniques.\\nAcknowledgment\\nTheﬁrst author is partially supported by a grant of the Higher Education Commission (HEC), Pakistan.\\nReferences\\nAmancio, D. R., Oliveira, O. N. Jr & Costa, L. D. F. 2015. Topological-collaborative approach for disambiguating\\nauthors names in collaborative networks. Scientometrics 102(1), 465 –485.\\nArunachalam, S. & Madhan, M. 2016. Adopting orcid as a unique identi ﬁer will bene ﬁt all involved in scholarly\\ncommunication. The National Medical Journal of India 29(4), 227 –234.\\nAswani, N., Bontcheva, K. & Cunningham, H. 2006. Mining information for instance uni ﬁcation. In International\\nSemantic Web Conference , 329 –342. Springer.\\nBekkerman, R. & McCallum, A. 2005. Disambiguating web appearances of people in a social network. In Proceed-\\nings of the 14th International Conference on World Wide Web , 463 –470. ACM.\\nBhattacharya, I. & Getoor, L. 2007. Collective entity resolution in relational data. ACM Transactions on Knowledge\\nDiscovery from Data (TKDD) 1(1), 5.\\nCarrasco, R. C., Serrano, A. & Castillo-Buergo, R. 2016. A parser for authority control of author names in biblio-\\ngraphic records. Information Processing & Management 52(5), 753 –764.\\nChin, W.-S., Zhuang, Y., Juan, Y.-C., Wu, F., Tung, H.-Y., Yu, T., Wang, J.-P., Chang, C.-X, Yang, C.-P., Chang, W.-C.\\nHuang, K.-H., Kuo, T.-M., Lin, S.-W., Lin, Y.-S., Lu, Y.-C., Su, Y.-C., Wei, C.-K., Yin, T.-C., Li, C.-L., Lin, T.-W.,\\nTsai, C.-H., Lin, S.-D., Lin, H.-T. & Lin, C.-J. 2014. Effective string processing and matching for author dis-ambiguation. The Journal of Machine Learning Research 15(1), 3037 –3064.\\nChisholm, A. & Hachey, B 2015. Entity disambiguation with web links. Transactions of the Association for Com-\\nputational Linguistics 3, 145 –156.\\nChristen, P. 2006. A comparison of personal name matching: techniques and practical issues. In Sixth IEEE Inter-\\nnational Conference on Data Mining-Workshops (ICDMW ’06), 290 –294. IEEE.\\nDe Carvalho, A. P., Ferreira, A. A., Laender, A. H. & Gonçalves, M. A. 2011. Incremental unsupervised name\\ndisambiguation in cleaned digital libraries. Journal of Information and Data Management 2(3), 289.\\nElliott, S. 2010. Survey of author name disambiguation: 2004 to 2010. Library Philosophy and Practice 473,\\nhttp://digitalcommons.unl.edu/libphilprac/473/ .\\nEsperidião, L. V. B., Ferreira, A. A., Laender, A. H., Gonçalves, M. A., Gomes, D. M., Tavares, A. I. & de Assis, G. T.\\n2014. Reducing fragmentation in incremental author name disambiguation. Journal of Information and Data\\nManagement 5(3), 293.\\nFan, X., Wang, J., Pu, X., Zhou, L. & Lv, B. 2011. On graph-based name disambiguation. Journal of Data and\\nInformation Quality (JDIQ) 2(2), 10.\\nFerreira, A. A., Gonçalves, M. A. & Laender, A. H. 2012. A brief survey of automatic methods for author name\\ndisambiguation. Acm Sigmod Record 41(2), 15 –26.\\nFerreira, A. A., Gonçalves, M. A. & Laender, A. H. 2015. Automatic methods for disambiguating author names in\\nbibliographic data repositories. In Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries ,\\n297–298. ACM.\\nFerreira, A. A., Veloso, A., Gonçalves, M. A. & Laender, A. H. 2010. Effective self-training author name dis-\\nambiguation in scholarly digital libraries. In Proceedings of the 10th Annual Joint Conference on Digital Libraries ,\\n39–48. ACM.\\nFerreira, A. A., Veloso, A., Gonçalves, M. A. & Laender, A. H. 2 014. Self-training author name disambiguation for\\ninformation scarce scenarios. Journal of the Association for Information Science and Technology 65(6), 1257 –1278.\\nGiunchiglia, F. & Shvaiko, P. 2003. Semantic matching. The Knowledge Engineering Review 18(3), 265 –280.\\nGurney, T., Horlings, E. & Van Den Besselaar, P. 2012. Author disambiguation using multi-aspect similarity indi-\\ncators. Scientometrics 91(2), 435 –449.I. HUSSAIN AND S. ASGHAR 22\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Han, D., Liu, S., Hu, Y., Wang, B. & Sun, Y. 2015. Elm-based name disambiguation in bibliography. World Wide Web\\n18(2), 253 –263.\\nHan, H., Giles, L., Zha, H., Li, C. & Tsioutsiouliklis, K. 2004. Two supervised learning approaches for name\\ndisambiguation in author citations. In Proceedings of the 2004 joint ACM/IEEE conference on Digital Libraries,\\n2004 , 296 –305. IEEE.\\nHan, H., Xu, W., Zha, H. & Giles, C. L. 2005. A hierarchical naive bayes mixture model for name disambiguation in\\nauthor citations. In Proceedings of the 2005 ACM Symposium on Applied Computing , 1065 –1069. ACM.\\nHuynh, T., Hoang, K., Do, T. & Huynh, D. 2013. Vietnamese author name disambiguation for integrating publications\\nfrom heterogeneous sources. In Asian Conference on Intelligent Information and Database Systems , 226 –235.\\nSpringer.\\nImran, M., Gillani, S. & Marchese, M. 2013. A real-time heuristic-based unsupervised method for name dis-\\nambiguation in digital libraries. D-Lib Magazine 19(9), 1.\\nJohnson, D. B. 1975. Finding all the elementary circuits of a directed graph. SIAM Journal on Computing 4(1), 77 –84.\\nKofod-Petersen, A. 2012. How to do a structured literature review in computer science. Document released as a guide\\nto performing a Structured Literature Review at NTNU. https://pdfs.semanticscholar.org/\\nf9e7/b1f645ddeddfbf702558f554dd316a7692ae.pdf .\\nKrzywicki, A., Wobcke, W., Bain, M., Martinez, J. C. & Compton, P. 2016. Data mining for building knowledge\\nbases: techniques, architectures and applications. Knowledge Engineering Review 31(2), 97 –123.\\nKum, H.-C., Krishnamurthy, A., Machanavajjhala, A., Reiter, M. K. & Ahalt, S. 2014. Privacy preserving interactive\\nrecord linkage (ppirl). Journal of the American Medical Informatics Association 21(2), 212 –220.\\nLaFlamme, M. 2016. On the problem of the namesake. Cultural Anthropology 31(1), 1 –3.\\nLee, D., Kang, J., Mitra, P., Giles, C. L. & On, B.-W. 2007. Are your citations clean? Communications of the ACM\\n50(12), 33 –38.\\nLevin, F. H. & Heuser, C. A. 2010. Evaluating the use of social networks in author name disambiguation in digital\\nlibraries. Journal of Information and Data Management 1(2), 183.\\nLevin, M., Krawczyk, S., Bethard, S. & Jurafsky, D. 2012. Citation-based bootstrapping for large-scale\\nauthor disambiguation. Journal of the American Society for Information Science and Technology 63(5),\\n1030–1047.\\nLiu, Y., Li, W., Huang, Z. & Fang, Q. 2015. A fast method based on multiple clustering for name disambiguation in\\nbibliographic citations. Journal of the Association for Information Science and Technology 66(3), 634 –644.\\nLiu, Y. & Tang, Y. 2015. Network based framework for author name disambiguation applications. International\\nJournal of u-and e-Service, Science and Technology 8(9), 75 –82.\\nMaguire, E. J. 2016. Ethnicity sensitive author disambiguation using semi-supervised learning. In Proceedings of the\\nKnowledge Engineering and Semantic Web: 7th International Conference , KESW 2016 649, 272. Springer, 21 –23\\nSeptember 2016.\\nMoher, D., Liberati, A., Tetzlaff, J. & Altman, D. G. 2009. Preferred reporting items for systematic reviews and meta-\\nanalyses: the prisma statement. Annals of Internal Medicine 151(4), 264 –269.\\nMurnane, E. L., Haslhofer, B. & Lagoze, C. 2013. Reslve: leveraging user interest to improve entity disambiguation on\\nshort text. In Proceedings of the 22nd International Conference on World Wide Web , 1275 –1284. ACM.\\nNicholson, S. W. & Bennett, T. B. 2016. Dissemination and discovery of diverse data: do libraries promote their\\nunique research data collections? International Information & Library Review 48(2), 85 –93.\\nOn, B.-W., Elmacioglu, E., Lee, D., Kang, J. & Pei, J. 2006. Improving grouped-entity resolution using quasi-cliques.\\nInSixth International Conference on Data Mining (ICDM ’06), 1008 –1015. IEEE.\\nOn, B.-W., Lee, D., Kang, J. & Mitra, P. 2005. Comparative study of name disambiguation problem using a scalable\\nblocking-based framework. In Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries ,\\n344–353. ACM.\\nOn, B.-W., Lee, I. & Lee, D. 2012. Scalable clustering methods for the name disambiguation problem. Knowledge and\\nInformation Systems 31(1), 129 –151.\\nOnodera, N., Iwasawa, M., Midorikawa, N., Yoshikane, F., Amano, K., Ootani, Y., Kodama, T., Kiyama, Y.,\\nTsunoda, H. & Yamazaki, S. 2011. A method for eliminating articles by homonymous authors from the large\\nnumber of articles retrieved by author search. Journal of the American Society for Information Science and\\nTechnology 62(4), 677 –690.\\nOramas, S., Espinosa-Anke, L., Sordo, M., Saggion, H. & Serra, X. 2016. Elmd: an automatically generated entity\\nlinking gold standard dataset in the music domain. In Proceedings of the 10th International Conference on\\nLanguage Resources and Evaluation, LREC .\\nPalfrey, J. 2016. Design choices for libraries in the digital-plus era. Daedalus 145(1), 79 –86.\\nPeng, H.-T., Lu, C.-Y., Hsu, W. & Ho, J.-M. 2012. Disambiguating authors in citations on the web and authorship\\ncorrelations. Expert Systems with Applications 39(12), 10521 –10532.\\nPereira, D. A., Ribeiro-Neto, B., Ziviani, N., Laender, A. H., Gonçalves, M. A. & Ferreira, A. A. 2009. Using web\\ninformation for author name disambiguation. In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital\\nLibraries ,4 9–58. ACM.A survey of AND techniques: 2010 –2016 23\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .Provost, F. & Kohavi, R. 1998. Guest editors ’introduction: on applied research in machine learning. Machine\\nLearning 30(2), 127 –132.\\nPyle, R. L. 2016. Towards a global names architecture: the future of indexing scienti ﬁc names. ZooKeys 550, 261–281.\\nSantana, A. F., Gonçalves, M. A., Laender, A. H. & Ferreira, A. A. 2015. On the combination of domain-speci ﬁc\\nheuristics for author name disambiguation: the nearest cluster method. International Journal on Digital Libraries\\n16(3–4), 229 –246.\\nScholtes, J. C. & Maes, F. P. E., et al. 2016. System and method for authorship disambiguation and alias resolution in\\nelectronic data. US Patent 9,264,387.\\nSchulz, C., Mazloumian, A., Petersen, A. M., Penner, O. & Helbing, D. 2014. Exploiting citation networks for large-\\nscale author name disambiguation. EPJ Data Science 3(1), 1.\\nSeol, J.-W., Lee, S.-H. & Kim, K.-Y. 2016. Author disambiguation using co-author network and supervised learning\\napproach in scholarly data. International Journal of Software Engineering and Its Applications 10(4), 73 –82.\\nShin, D., Kim, T., Choi, J. & Kim, J. 2014. Author name disambiguation using a graph model with node splitting and\\nmerging based on bibliographic information. Scientometrics 100(1), 15 –50.\\nSong, Y., Huang, J., Councill, I. G., Li, J. & Giles, C. L. 2007. Ef ﬁcient topic-based unsupervised name dis-\\nambiguation. In Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries , 342 –351. ACM.\\nTang, J., Fong, A. C., Wang, B. & Zhang, J. 2012. A uni ﬁed probabilistic framework for name disambiguation in\\ndigital library. IEEE Transactions on Knowledge and Data Engineering 24(6), 975 –987.\\nTang, L. & Walsh, J. P. 2010. Bibliometric ﬁngerprints: name disambiguation based on approximate structure\\nequivalence of cognitive maps. Scientometrics 84(3), 763 –784.\\nTorvik, V. I. & Smalheiser, N. R. 2009. Author name disambiguation in medline. ACM Transactions on Knowledge\\nDiscovery from Data (TKDD) 3(3), 11.\\nTran, H. N., Huynh, T. & Do, T. 2014. Author name disambiguation by using deep neural network. In Asian\\nConference on Intelligent Information and Database Systems , 123 –132. Springer\\nWang, J., Berzins, K., Hicks, D., Melkers, J., Xiao, F. & Pinheiro, D. 2012. A boosted-trees method for name\\ndisambiguation. Scientometrics 93(2), 391 –411.\\nWang, P., Zhao, J., Huang, K. & Xu, B. 2014. A uni ﬁed semi-supervised framework for author disambiguation in\\nacademic social network. In International Conference on Database and Expert Systems Applications ,1–16.\\nSpringer.\\nWang, X., Tang, J., Cheng, H. & Philip, S. Y. 2011. Adana: active name disambiguation. In 2011 IEEE 11th\\nInternational Conference on Data Mining , 794 –803. IEEE.\\nWeiss, A. 2016. Examining massive digital libraries (mdls) and their impact on reference services. The Reference\\nLibrarian 57(4), 286 –306.\\nWu, H., Li, B., Pei, Y. & He, J. 2014. Unsupervised author disambiguation using Dempster-Shafer theory. Sciento-\\nmetrics 101(3), 1955 –1972.\\nZhao, J., Wang, P. & Huang, K. 2013. A semi-supervised approach for author disambiguation in KDD CUP 2013.\\nInProceedings of the 2013 KDD CUP 2013 Workshop , 10. ACM.\\nZhu, J., Yang, Y., Xie, Q., Wang, L. & Hassan, S.-U. 2014. Robust hybrid name disambiguation framework for large\\ndatabases. Scientometrics 98(3), 2255 –2274.\\nZhu, L., Ghasemi-Gol, M., Szekely, P., Galstyan, A. & Knoblock, C. A. 2016. Unsupervised entity resolution on\\nmulti-type graphs. In International Semantic Web Conference , 649 –667. Springer.\\nZhu, Y. & Li, Q. 2013. Enhancing object distinction utilizing probabilistic topic model. In 2013 International\\nConference on Cloud Computing and Big Data (CloudCom-Asia) , 177 –182. IEEE.I. HUSSAIN AND S. ASGHAR 24\\nhttps://doi.org/10.1017/S0269888917000182Downloaded from https://www.cambridge.org/core . University of Florida , on 11 Jan 2018 at 11:59:46 , subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms .', \"Intelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nLUCID: Author Name Disambiguation using Graph\\nStructural Clustering\\nIjaz Hussain\\nDepartment of Computer Science\\nCOMSATS Institute of Information Technology\\nIslamabad, Pakistan 78000Sohail Asghar\\nDepartment of Computer Science\\nCOMSATS Institute of Information Technology\\nIslamabad, Pakistan 78000\\nAbstract —Author name ambiguity may occur in two situations\\nwhen multiple authors have the same name or the same author\\nwrites her name in multiple ways. The former is called homonym\\nand the later is called synonym. Disambiguation of these am-\\nbiguous authors is a non-trivial job because there is a limited\\namount of information available in citations data set. In this\\npaper, a graph structural clustering algorithm “LUCID: Author\\nName Disambiguation using Graph Structural Clustering” is\\nproposed which disambiguates authors by using community\\ndetection algorithm and graph operations. In the ﬁrst phase,\\nLUCID performs some preprocessing tasks on data set and\\ncreates blocks of ambiguous authors. In the second phase co-\\nauthors graph is built and “SCAN: A Structural Clustering\\nAlgorithm for Networks” is applied to detect hubs, outliers,\\nand clusters of nodes (author communities). The hub node that\\nintersects with many clusters is considered as a homonym and\\nresolved by splitting across this node. Finally, the synonyms are\\ndisambiguated using proposed hybrid similarity function. LUCID\\nperformance is evaluated using a real data set of Arnetminer.\\nResults show that LUCID performance is overall better than\\nbaseline methods and it achieves 97% in terms of pairwise\\nprecision, 74% in pairwise recall and 82% in pairwise F1.\\nKeywords —Author name disambiguation ; Structural clustering\\n;Co-authors graph ; Homonym resolver; Synonyms resolver\\nI. I NTRODUCTION\\nCurrently, Digital Libraries (DLs) such as DBLP1, MED-\\nLINE2, Cite Seer3arXiv4, MAS5, Google Scholar6, and BD-\\nBComp7are a big source of scholarly information retrieval.\\nHowever, the majority of DLs are unable to correctly identify\\nthe oeuvre of an author due to author name ambiguity problem.\\nThis may happen in two different ways when multiple authors\\nhave the same name or the same author writes his name\\nin multiple ways. The former is called homonym and the\\nlater is called synonym. For instance, When we searched\\nDBLP (Digital Bibliography & Library Project) a name “Ajay\\nGupta”, it returned multiple authors with exactly the same\\nname “Ajay Gupta”. Similarly, when we searched a name “A.\\nGupta” it returned greater than two hundred likely matches.\\nSome bad effects of the author name ambiguity are wrong\\nsearch results and incorrect attributions. Systems which resolve\\n1http://dblp.uni-trier.de\\n2http://www.medline.com\\n3http://citeseerx.ist.psu.edu\\n4http://arxiv.org\\n5http://academic.research.microsoft.com\\n6http://scholar.google.com.pk\\n7http://www.lbd.dcc.ufmg.br/bdbcompthese ambiguities are known as Author Name Disambiguation\\n(AND) systems [5, 18].\\nSeveral AND systems were presented in the last few years\\nthat used many different techniques from machine learning to\\ndata mining [1, 2, 3, 4, 6, 10, 13, 15, 16, 18, 19, 22]. However,\\nsome of them only resolved the homonyms [4, 24], some\\nconcentrated only on synonyms [4, 15] and a few required\\nsome auxiliary information like afﬁliation, email or related\\nweb page, or needed some hidden information such as the\\nunknown number of ambiguous authors a priori or required\\nsome experts knowledge to estimate unknown parameters and\\nthresholds [6, 11, 16, 17, 18]. Still, some others heavily relied\\non training data to train the proposed model [22, 23].\\nIn this paper “LUCID: Author Name Disambiguation using\\nGraph structural Clustering” is proposed to resolve author\\nname ambiguity problems. LUCID starts working by prepro-\\ncessing the data set and then forms ambiguous author blocks\\n[6, 18], co-author’s graph is built from the preprocessed data\\nset. In the next step, “SCAN: A Structural Clustering Algo-\\nrithm for Networks” is applied to the co-author’s graph to ﬁnd\\nthe hubs, outliers, and clusters of nodes (author communities)\\n[27]. Hub nodes are considered as the homonyms and if\\nmultiple author communities are identiﬁed in the co-author’s\\ngraph then these are considered as different homonyms and\\nresolved by splitting these communities across the hub nodes.\\nSynonyms are ﬁrst identiﬁed and then resolved with the help\\nof proposed hybrid similarity. In this similarity, both compared\\nnames should be compatible and their syntax similarity should\\nalso be greater than a deﬁned threshold. Then the geodesic\\ndistance between these name nodes is found. If any two\\nvertices fulﬁll the both conditions of hybrid similarity then\\nthese vertices are fused into one node. Co-authors attribute\\nhas been considered more discriminative feature than other\\ncitation features in many earlier studies [4, 18, 19, 21]. LUCID\\nsimultaneously solves homonyms as well as synonyms. One of\\nthe prominent features of LUCID is that it only required co-\\nauthors attribute to fully resolve the author name ambiguity\\nproblems.\\nLUCID performance is evaluated by comparing it with two\\nstate-of-the-art unsupervised name disambiguation methods\\nHHC and GFAD [2, 18] using a data set of Arnetminer [20].\\nDespite only using co-authors attribute, LUCID performance\\nis overall better than baseline methods from the perspective of\\nrepresentative clustering evaluation metrics. In summary, the\\nmain contributions of this work are:\\nIEEE 1 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\n\\x0f A homonym resolver algorithm is proposed that uses\\ngraph-based community detection algorithm to detect\\nhomonyms and graph operations to resolve them.\\n\\x0f A synonym resolver algorithm is also presented that\\nuses proposed hybrid similarity to detect and resolve\\nsynonyms.\\n\\x0f Experiments on Arnetminer data set are performed to\\nvalidate the performance of LUCID.\\nRest of paper is structured as follows. In “Related Work”\\nsome related works to LUCID are discussed and their ca-\\npabilities and limitations are highlighted. “Proposed Method:\\nLUCID” describes in detail the working of LUCID. We\\nsimulated proposed algorithm on Arnetminer and compare it\\nto baseline methods in Experiments and Discussions. In the\\nend, we conclude with a discussion of conclusions and future\\nwork in “Conclusions”.\\nII. R ELATED WORK\\nSome AND methods used supervised techniques [7, 21,\\n23], other used unsupervised techniques [2, 16, 19, 26], still\\nother used semi-supervised techniques [11, 28], and graph\\noriented techniques [4, 12, 18].\\nA boosted tree method was proposed in [23] for author\\nname disambiguation. In the ﬁrst step, they ﬁltered authors\\nby name and afﬁliation matches. Then similarity scores for\\npublication attributes are computed and author screening is\\nperformed. Finally, boosted tree classiﬁer was applied to the\\nmanually crafted data set. In [7] two extreme learning machine\\nbased algorithms were proposed for author name disambigua-\\ntion. But this method requires training of a new classiﬁer\\nfor each ambiguous author name. Similarly, in [21] a deep\\nneural network approach was presented to automatically learn\\nfeature vectors and disambiguate those authors. They tested\\ntheir proposed solution with Vietnameses data set, while they\\nclaimed that their proposed method can equally useful for any\\ndata set. They utilized the multi-column deep neural networks\\nto improve the generalization capabilities of the system that is\\nvery similar to ensemble method bagging.\\nA simple two-step algorithm for AND using heuristic-\\nbased hierarchical clustering method was proposed by [2].\\nThey resolved both homonyms and synonyms by exploiting\\nsimilarities in publication attributes combined with some other\\nheuristics. Initially, they clustered citations on the basis of\\nsome common co-author names. They created fragmented\\nbut very pure clusters. These clusters were then pair-wise\\ncompared and merged if they have a similarity greater than\\nsome predeﬁned threshold in their title and venue. This process\\nwas iterative and successive iterations had a synergetic effect\\non disambiguation of authors, as more titles and venues were\\ncombined in these merged clusters. This process ﬁnished when\\nno more fusions between clusters were possible or a number\\nof speciﬁed iterations was reached. This method may produce\\ndifferent clusters for each run. A discrimination function in\\nwas proposed [16] that predicted true authors and false authors\\nusing logistic regression on Web of Science data set. They\\nextracted true author papers from a large amount of retrieved\\npapers by using two stage ﬁltering. This method is not good\\nfor papers in which subject ﬁeld and afﬁliation address donot vary too much. Wu et al. in [26] proposed an algorithm\\nthat used Dempster-Shafer Theory (DST) in conjunction with\\nShannon Entropy (SE) for author disambiguation. In the ﬁrst\\nstep, some high-level features and their co-relation similarities\\nwere calculated. In next step, these features were fused using\\nDST and SE. In clustering stage, they used three different\\nconvergence conditions for clustering algorithm namely–the\\npreset number of clusters, the number of available evidence\\nand distance between clusters.\\nA uniﬁed probabilistic framework to address the\\nhomonyms in DLs was proposed by [19]. They formalized\\nthe disambiguation problem using Markov random ﬁelds, in\\nwhich the data were cohesive on both local attributes and\\nrelationships. They proposed algorithms that automatically\\nestimate the number of unknown ambiguous authors and the\\nrequired unknown parameters. A hybrid name disambiguation\\nframework was presented in [28] by zhu et al. which not\\nonly used the traditional information (co-authors) but also web\\npage genre information. This framework consisted of two main\\nsteps; web page genre identiﬁcation and re-clustering model. A\\nsemi-supervised approach was proposed in [11], which offered\\nan end to end solution for AND and it can also be used\\nas a wrapper service for DLs. They used selected features\\nof an author and citation record to disambiguate authors\\nusing unsupervised techniques. However, at some stages, they\\ninteracted with users to further purify the retrieved clusters.\\n“GHOST: GrapHical framewOrk for name diSambigua-\\nTion” was presented in [4] that ﬁrst modeled the relationships\\namong publications using undirected graphs. They solved\\nhomonyms problem by iteratively ﬁnding valid paths, comput-\\ning similarities, clustering with the help of afﬁnity propagation\\nalgorithm and in the last using user feedback as a comple-\\nmentary tool to enhance the performance. This approach does\\nnot handle outliers and fails in the case of solo authors. A\\ngraph framework for author name disambiguation “GFAD”\\nwas proposed in [18] which exploited co-authors and citation\\ntitles. They modeled an undirected graph by denoting an author\\nto a vertex and a co-author relation was indicated by an edge.\\nNamesake problem was resolved by splitting the vertex that\\nwas common to multiple non-overlapping cycles so that each\\ncycle correspond to a unique author. The heteronymous name\\nproblem was handled by merging multiple author vertices into\\none by identifying those vertices that actually represented a\\nsingle author with different names. This method is compu-\\ntationally too expensive as it used Johnson’s cycle detection\\nalgorithm. It removed outliers by comparing similarity among\\noutliers with the help of cosine similarity measure. LUCID is\\nsimilar to existing AND systems as it falls in the unsupervised\\ncategory [2, 3, 4, 18]. But different from other AND systems\\n[1, 11, 14], as it uses the graph structural clustering and graph\\noperations. Also in it, a hybrid similarity is proposed that uses\\nsyntax similarity as well as graph geodesics to disambiguate\\nsynonyms.\\nIII. P ROPOSED METHOD : LUCID\\nWe give some deﬁnitions for understanding the structural\\nclustering speciﬁcally related to SCAN algorithm that are\\nbeing utilized in LUCID. Then, LUCID is presented in subse-\\nquent paragraphs, whereas its architecture is shown in the Fig.\\n1.\\nIEEE 2 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nData Set Preprocessing Graph Constructor Homonyms Resolver Synonyms Resolver\\nFig. 1. Architecture of LUCID\\nA. Preliminaries\\nWe give these deﬁnitions here as used in [27] for the\\nreader’s convenience. Let G=fN;Lgbe a graph, where N is\\na set of nodes and L is set of links in this graph G.\\nDeﬁnition 1. CORE\\nCore nodes have a minimum of \\x16neighbors with a struc-\\ntural similarity that is greater than the speciﬁed threshold \\x0f.\\nClusters are grown from core nodes. Parameters \\x16and\\x0fare\\nthe determinants of the clustering. Let \\x0f2Rand\\x162N.\\nA noden2Nis called a core according to \\x0fand\\x16, if its\\n\\x0f-neighborhood contains at least \\x16nodes:\\nCORE\\x0f;\\x16(m),kN\\x0f(m)k\\x15\\x16 (1)\\nDeﬁnition 2. CLUSTERS\\nNow we can deﬁne a clustering of a network Gw.r.t. the\\ngiven parameters \\x0fand\\x16as all structure-connected clusters\\ninG. Let\\x0f2Rand\\x162N. A clustering P of network\\nG=<M;L> w.r.t.\\x0fand\\x16consists of all structure-connected\\nclusters w.r.t. \\x0fand\\x16inG, formally:\\nCLUSTERS \\x0f;\\x16(P),P=fC2NkCLUSTER \\x0f;\\x16(C)g\\n(2)\\nA node is either a member of a cluster, or it is isolated. If\\na node is not a member of any structure-connected clusters, it\\nis either a hub or an outlier, depending on its neighborhood.\\nDeﬁnition 3. HUB\\nLet\\x0f2Rand\\x162N. For a given clustering P, i.e.\\nCLUSTERS \\x0f;\\x16(P), if an isolated node n2Nhas neighbors\\nbelonging to two or more different clusters w.r.t. \\x0fand\\x16, it is\\na hub w.r.t. \\x0fand\\x16, and formally,\\nHUB\\x0f;\\x16(N), 8(C)2P:n =2C9(p;q)2\\x00(n) :\\n9(X;Y )2P:X6=Y:^p2X^q2Y(3)\\nDeﬁnition 4. OUTLIER\\nFor a given cluster P, i.e.CLUSTERS \\x0f;\\x16(P), an isolated\\nnoden2Nis an outlier if and only if all its neighbors either\\nbelong to only one cluster or do not belong to any cluster.OUTLIER \\x0f;\\x16(N), 8(C)2P:n =2C\\n:9(p;q)2\\x00(v) :9(X;Y )2P:X6=Y:^p2X^q2Y\\n(4)\\nWe implemented SCAN algorithm as proposed by [27] in\\nPython to detect hubs, outliers and clusters (author communi-\\nties) from the co-author’s graph. This step is given in LUCID\\nalgorithm at line 4. Interested readers are requested to read\\n[27] for further details.\\nB. Preprocessing and Graph Building Stage\\nAll the citations data set is extracted and pre-processed into\\ndifferent tokens, like authors, titles, and venues etc. In blocking\\nstage, similar names are grouped together if their similarity\\nis higher than some predeﬁned threshold value. We used\\nfragment comparison method as it was proved to be effective\\nfor name disambiguation in many previous studies [2, 6].\\nBlocking stage is important as it affects the computations in\\nthe later stages of the name disambiguation algorithms. The\\nBlocking stage returns ‘l’ blocks if given ‘m’ authors, it is\\nshown in Fig. 2 .\\nThe computation complexity is\\nO(m2) (5)\\nfor ‘m’ authors if we do not use blocking, whereas, blocking\\nconsiderably reduces computational complexity to\\nO(LjTj) (6)\\nWhere ‘L’ is the number of blocks and ‘T’ is the average size\\nof blocks.\\nIf we set a very high threshold for blocking then it produces\\nvery low recall but pure blocks. In contrast, if we set very low\\nthreshold then it produces high false positives. So, we sampled\\nnames data set and set this threshold to 0.8 that produces\\noptimal blocking of ambiguous author names.\\nCitations data set provides some basic features like names\\nof authors, titles, venues and year of publications. However, as\\nmentioned earlier in “Introduction”, the most inﬂuential feature\\namong these is the co-authors for solving the problem of author\\nambiguity [4, 18, 19, 21]. In LUCID, an author is represented\\nby a node that has its unique id for identiﬁcation, its name (not\\nIEEE 3 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nX\\n1. C  C h e n\\n…………..………….\\nM. Ijaz HussainY\\n………….\\n1001. Ijaz Hussain\\n…………….…………...\\nn. Chang Chen1789. C D Chen\\n2897. I Hussain\\n2898. H Ijaz \\nIjaz Hussain’s Block C Chen’s Block………….………….\\nFig. 2. An Example of Ambiguous Author Blocking\\nunique), and publications set where that author has appeared.\\nThe edges modeled the bidirectional relationship between two\\nauthors. Every citation that has ‘m’ number of authors(nodes)\\nexactly produces ‘E’ number of edges, given by Eq. 7. A toy\\nexample of citations data set, nodes and constructed graph is\\nshown in the Fig. 3. There are eight author nodes and ten edges\\nthat are created from ﬁve citations. Chang Chen is a node that\\nis common to four citations and in one citation it’s abbreviated\\nname “C Chen” is mentioned.\\nE=(m)\\x03(m\\x001)\\n2(7)\\nC. Homonyms Resolver Stage\\nIt is assumed here that different homonyms have different\\nsocial circles and different authors with the same name seldom\\nwork in the same organization or social circle [19, 18]. So, they\\nshould form quite different author communities. A community\\nin the co-author’s graph of LUCID is generated from each\\ncitation and thus each community denotes the co-authors for\\neach citation that is the smallest social circle in the academic\\ndomain. With the help of co-authors graph, it is possible to\\ninfer a social circle of an author from his/her co-authors by\\nﬁnding communities. The community gets bigger and wider\\nwhen more citations are processed. LUCID assumes that an\\nauthor linked with multiple social circles contains mixed\\ninformation for several authors that happened to have the\\nsame name. LUCID splits that node and its information into a\\nnumber of different nodes that are present in non-overlapping\\ncommunities. LUCID considers each non-overlapping commu-\\nnity emanating from the same node as a different social circle\\nof an author. LUCID community detection and split algorithm\\npseudo-code is described in Algorithm 1.\\nIn co-authors graph, an edge represents a co-authors re-\\nlationship between two authors (nodes). LUCID’s community\\ndetection algorithm is based on “SCAN: A Structural Cluster-\\ning Algorithm for Networks” to detect different communities\\nA\\nGCP B\\nD\\nFEPaper Id\\n1\\n2\\n3\\n4\\n5Bibliographic Citations Data \\nChang Chen : Author Name Disambiguation- A Review: SEIT, 2015: page 29-38\\nFarman Ali, Shakeel Ali, M.Rehan, Chang Chen: Graph based Author Name Disambiguation Framework: \\nOIR, 2014: page 329-339Chang Chen, F. Ali, M. Ibrahim : Non-linear Control for Hot Air Blower System: FIT,2012: page 78-87Chang Chen, Tasawar Ali : Frequent Graph Pattern Mining Comparisom: KDD, 2016: page 124-140\\n(a) An excerpt of bibliographic citations\\nNode Id Node Name Node Publications\\n1\\n876432\\n5C. Chen\\nShakeel Ali\\nM. RehanFarman AliM. IbrahimTasawar Ali\\nF.  AliP5\\nP4\\nP4P4P3P3P2P1-P4\\n(c) Graph constructed from the excerpt of bibliographic citations shown in (a) (b) Information of nodes in the graphM.Rehan, C. Chen: Graph utilization in Author Name Ambiguity Problem: Scientometrics, 2014: page \\n319-329\\nShort Name\\nP\\nCDBFEA\\nGChang ChenFig. 3. Co-authors Graph of Chang Chen Sample Citations\\nin the co-author’s graph [27]. When SCAN is applied on\\nco-authors graph it outputs communities (clusters of nodes),\\nhub node and outliers as deﬁned in “Preliminaries”. The\\ncommunity consists of a set of nodes from this co-authors\\ngraph. The node that is involved in many social circles in\\nthe co-author’s graph is called hub node. Hub node is the\\npotential homonym that needs disambiguation. Outliers are\\nnodes that only contribute one or a few publications with hub\\nnode. In a toy graph example, detected communities, hub and\\noutlier can be seen in Fig. 4. There are two clusters of nodes\\nAB\\nC D\\nE\\nF\\nGHubOutlier\\nP\\nCluster 1Cluster 2\\nFig. 4. An Example of Detected Communities, Hub and Outlier after using\\nSCAN on Chang Chen Co-authors Graph\\n(communities), one hub and one outlier. The ﬁrst and second\\ncluster consists of nodes hP, B, C, DiandhF, Girespectively.\\nThe hub is nodehAiand outlier is node hEi.\\nA hub node that has multiple non-overlapping communities\\ncontains mixed information for several homonym authors.\\nLUCID splits the information of each author on the node con-\\ntaining the homonyms along the non-overlapping communities.\\nThis part starts from the line 2 of Algorithm 1, where the hub\\nnode publications list is retrieved from the graph. Similarly,\\npublications of all cluster nodes (community) are retrieved\\nand saved in a list (line 3-5). Now, for each homonym the\\nintersection of hub publications and community publications\\nIEEE 4 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nAlgorithm 1: LUCID algorithm\\nData:Co\\x00authorsGraph (CG)\\nResult:DisambiguatedGraph (DG)\\nbegin\\n1 [Clusters;Hub;Outliers ] \\x00\\nApplySCAN (CG)\\n2HubPublications \\x00getPublications (Hub)\\n3 forcluster2Clusters do\\n4 fornode2cluster do\\n5 clusterPublications \\x00\\nclusterPublications[\\ngetPublications (node)\\n6HNiPublications \\x00\\nHubPublications\\\\clusterPublications\\n7HNiName \\x00getName (Hub)\\n8HNiId \\x00getLength (CG)\\n9InsertNode (HNiId;HNiName;HN iPublications )\\n10UpdateGraph (CG;cluster )\\n11HubPublications \\x00\\nHubPublicationsnclusterPublications\\n12UpdateGraph (CG)\\n13HubName \\x00HG:getName (Hub)\\n14 fornode2HG do\\n15allAuthorNames \\x00\\nallAuthorNames[HG:getName (node)\\n16comparison Name \\x00HubName\\n17nameTokens \\x00comparison Name:split ()\\n18allAuthorNames:delete (HubName )\\n19 forauthor2allAuthorNames do\\n20similarity \\x00\\nnameSimilarity (comparison Name;author )\\n21 ifsimilarity>threshold &\\nauthorcompatible then\\n22 similarNamesList:append (author )\\n23 forname2similarNamesList do\\n24geodesicDistance \\x00\\ngeodesicDistance (comparison Name;name )\\n25 ifgeodesicDistance<threshold then\\n26 mergeNodePubs  \\x00\\ncomparison NamePublications[\\nnamePublications\\n27 ifnameId<comparison NameId\\nthen\\n28 mergeNodeName  \\x00nameName\\n29 mergeNodeId \\x00nameId\\n30 else\\n31 mergeNodeName  \\x00\\ncomparison Name\\n32 mergeNodeId \\x00comparison Id\\n33 InsertNewNode \\x00\\n(mergeNodeId;mergeName;mergeNodePubs )\\n34 deleteNode (comparison Node )\\n35 UpdateGraph (HG)\\n36returnUpdatedGraph (DG)are found. A new node is created in the co-author’s graph for\\neach community detected that has the same name as that of\\nthe hub node and has identity one more than in the current\\nnodes in the co-author’s graph, this newly created node has\\nthe publication list that is found at line 6. Likewise, for all\\ncommunities detected in the co-author’s graph, new nodes\\nare created and the graph is updated after every new node\\nis inserted in the co-author’s graph. Publications of hub node\\nare also updated after every insertion. In graph update, some\\nedges to the hub node to existing communities are removed\\nand some new edges to newly created nodes are created (line\\n7-12). The same procedure is repeated for all outliers, as is\\ndone in the case of all communities. This whole process is\\npictorially shown in the Fig. 5.\\nB\\nC D\\nF\\nG\\nA1\\nA3Three homonymsA2 E\\nP\\nCluster 1Cluster 2\\nFig. 5. An Example of Homonyms Resolution\\nAs seen in it hub node ‘A’ is split into three new nodes h\\nA1, A2, A3i, as there are two clusters of nodes (communities)\\nand one outlier. In this way, the homonym problem is solved\\nusing community detection algorithm and graph operations.\\nD. Synonyms Resolver\\nThe second type of name ambiguity is the synonym that\\noccurs when an author has a similar name as explained in\\nthe subsequent paragraphs. In academic domain, authors often\\nﬁll out their names in different forms–abbreviation, sequence\\nchanging, an omission of some part of their name, etc. So,\\nthe same author may appear in literature with many different\\nforms of his name.\\nCo-authors graph indicates the author’s past and present\\nconnections. So, if apparently different researchers having\\nsimilar names and there is a small geodesic distance between\\nthem then it is probably the same author. In LUCID, the nodes\\nthat have similar names referred as the same person if they\\nhave a geodesic distance less than 3, meaning that they are\\nhaving very close relationship with the similar author. First,\\nLUCID searches the whole graph for nodes with similar author\\nnames. The names are considered to be similar if they have\\na syntactic similarity greater than a predeﬁned threshold and\\ncompatible names. The Jaro-Winkler similarity is used for this\\npurpose as it proved to be the best similarity measure for names\\n[25]. Compatible names are those names that are either fully\\npart of another name or they have the same initial of the ﬁrst\\nand last name. For example, a name “C Chen” is fully part\\nof another name “C Chen Chang” and a name “Lei Wang”\\nis not having the same initials as that of another name “Wei\\nWang”. Similarly, another pair of names is “A. Gupta” and\\nIEEE 5 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\n“Anuram Gupta” that have same initial. So, the ﬁrst and third\\npair of names are compatible and the second pair of names\\nis not compatible. Pseudo-code for searching similar names is\\ndescribed in Algorithm 1 (lines 19-22).\\nAfter ﬁnding similar names the geodesic distance between\\ntwo similar name nodes is found. Those author nodes whose\\ngeodesic distance is less than the threshold are considered the\\nsynonyms. Two synonym nodes are then merged into one node\\nhaving publication list of both of these two nodes and id of the\\nfull name is preserved in the graph whereas the other similar\\nname node is deleted from the graph. Pseudo-code for merging\\nsimilar names is described in Algorithm 1 (lines 23-35).\\nAn example of synonym merging is shown in Fig. 6. In\\nthis example, since node ‘P’ and node ‘A1’ have syntactic\\nsimilarity less than a threshold and are compatible names.\\nFurthermore, the geodesic distance between these nodes is\\nless than the threshold. So in this case, they are merged into\\none node ‘A1’and the other node ‘P’ is deleted from the co-\\nauthorship graph and the edges are updated.\\nNode 'P' and Node 'A1'  are synonyms, so merged  \\nB\\nC D\\nF\\nG\\nA1\\nA3A2 E\\nB\\nC D\\nF\\nG\\nA1\\nA3A2 E\\nP\\nFig. 6. An Example of Synonyms Resolution\\nIV. E XPERIMENTS AND DISCUSSIONS\\nFor the HHC and GFAD methods, we used the imple-\\nmentation of [2] and [18], respectively. All experiments were\\nperformed on the Intel (R) 64-bit Core (TM) i5-5200U 2x2.2\\nGHz processors with 8 GB of RAM.\\nA. Data set\\nA real world publications data set from Arnetminer is used\\nto measure the effectiveness of LUCID. The statistics of the\\nArnetminer data set are given in Table I and more details\\nare found online at Arnetminer. The original version of this\\ncollection was created by Wang et al. [24]. Then Tang et al.\\n[19] manually checked, labeled and included more ambiguous\\nauthors to expand this data set. It is used with slight variations\\nin many AND studies [19, 18, 26]. Subsets of this data set\\nhave also been used in many earlier studies [8, 9, 2, 6, 5].\\nIn LUCID, only co-authors information of this data set is\\nused for the complete solution of the author name ambiguity\\nproblem. LUCID considered two authors same if their full\\nname are identical.\\nB. Evaluation Metrics\\nPairwise Precision (PP), Recall (PR) and F1 (PF1) are used\\nto measure the effectiveness of LUCID. PF1 is the harmonicTABLE I. T HEARNETMINER DATA SET(AUTH. DENOTES NUMBER\\nOFDISTINCT AUTHORS AND REC. REPRESENTS CITATIONS RECORDS\\nASSOCIATED WITH THAT AUTHOR )\\nName Auth. Rec. Name Auth. Rec.\\nBin Li 8 31 B. Wilkinson 1 17\\nBin Zhu 15 45 Cheng Chang 5 23\\nCharles Smith 4 7 Paul Wang 6 15\\nMichael Siege 6 50 David Cooper 7 18\\nGang Luo 8 42 S Huang 15 16\\nJ. Guo 9 12 Hui Yu 20 30\\nYan Tang 11 31 Lei Fang 7 17\\nXiaoming Wang 14 33 Yue Zhao 9 36\\nPaul Brown 8 20 Peter Phillips 3 13\\nDavid Nelson 10 15 F. Wang 14 15\\nHong Xie 6 10 J. Yin 7 18\\nJohn Hale 3 36 Kuo Zhang 4 16\\nMichael Smith 16 27 Richard Taylor 11 27\\nmean of PP and PR. The fraction of citation records corre-\\nsponding to the same author in the system-generated clusters\\nare known as PP and fraction of citation records associated\\nwith the same author in the gold standard reference clusters\\nare called as PR. The PP, PR and PF1 measures are expressed\\nTABLE II. T HEPERFORMANCE EVALUATION OF LUCID FOR EACH\\nAMBIGUOUS GROUP ON THE ARNETMINER COLLECTION\\nName PP PR PF1 Name PP PR PF1\\nBin Li 1.00 0.64 0.80 Hui Yu 1.00 0.97 0.98\\nBarry Wilkinson 1.00 1.00 1.00 J. Yin 0.95 0.52 0.70\\nJ. Guo 1.00 0.85 0.92 S. Huang 1.00 0.63 0.80\\nBin Zhu 1.00 1.00 1.00 John Hale 1.00 0.52 0.72\\nCharles Smith 1.00 1.00 1.00 Kuo Zhang 1.00 0.91 0.95\\nCheng Chang 1.00 0.79 0.89 Lei Fang 1.00 0.90 0.95\\nDavid Cooper 1.00 0.92 0.96 Yue Zhao 1.00 1.00 1.00\\nDavid Nelson 1.00 0.93 0.97 Peter Phillips 1.00 0.79 0.89\\nF. Wang 1.00 1.00 1.00 Richard Taylor 1.00 0.73 0.86\\nYan Tang 0.95 0.71 0.83 Michael Siege 1.00 0.83 0.91\\nGang Luo 1.00 0.95 0.98 Michael Smith 1.00 0.76 0.87\\nHong Xie 1.00 0.87 0.93 Paul Brown 1.00 1.00 1.00\\nXiaoming Wang 1.00 0.70 0.83 Paul Wang 1.00 0.91 0.95\\nAverage 0.99 0.84 0.91\\nin Eq. 8, where C(n;i)denotes the number of combinations of\\nielements from nelements:C(n;i) =n!\\ni!(n\\x00i)!;n\\x15i. Here,\\nndenotes the size of the citations in the collection, jis the\\nnumber of gold standard reference clusters manually generated,\\nandiis the number of clusters automatically generated by the\\nLUCID. Also, njis the number of elements in cluster jand\\nnijis the number of elements belonging to both clusters iand\\nj.\\nPP =PI\\ni=1PJ\\nj=1C(nij;2)\\nPI\\ni=1C(ni;2); PR =PI\\ni=1PJ\\nj=1C(nij;2)\\nPJ\\ni=1C(nj;2); PF 1 =2\\x03PP\\x03PR\\nPP+PR\\n(8)\\nC. Baseline Methods\\nTwo unsupervised author name disambiguation methods–\\nheuristic based hierarchical clustering (HHC) [2] and author\\nname disambiguation using a graph model (GFAD) [18] were\\nused as baseline methods to compare with LUCID.\\nHHC is a heuristic based unsupervised method that uses\\ncitation features for author name disambiguation. HHC showed\\nthe best results as compared with other unsupervised and\\nsupervised methods [3]. We compared the performance of\\nLUCID with the HHC using all the features of citations (co-\\nauthors, titles, and venue) that they called HHC-All in their pa-\\nper. HHC-All needs similarity thresholds for the paper title and\\nIEEE 6 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nLUCID GFAD HHC80859095100 99\\n94\\n84 8485\\n7991\\n89\\n81Percentage (%)PP PR PF1\\nFig. 7. Average Pairwise Precision, Pairwise Recall and Pairwise F1 of the\\nthree techniques\\npublication venue. Therefore, we evaluated the performance of\\nHHC-All by varying threshold values and then selected values\\non which the performances are maximized. Optimal results are\\nachieved at 0.2 and 0.3 similarity threshold values for paper\\ntitle and publication venue respectively. For further details on\\nHHC, refer to [2]. To compare LUCID with GFAD, we use\\nthe disambiguation results of the GFAD as given in [18] as\\nit is because LUCID also uses the same data set and same\\nperformance metrics as GFAD.\\nD. Performance Evaluation\\nLUCID performance is evaluated using three well-known\\nclustering metrics on Arnetminer collection of citation records\\ndescribed in the section IV-B. It achieves on average PP of\\n99%, PR of 84% and PF of 91% on Arnetminer data set.\\nTable II lists the complete details of each ambiguous author\\ngroup in the Arnetminer collection. In general PP of LUCID\\nin the majority of cases are approximately equal to 100% but\\nin some cases such as “Yan Tang” and “J. Yin” is low. In\\nFig. 7, LUCID performance is compared with two baseline\\nmethods–HHC and GFAD. It shows overall better performance\\nthan baseline methods. It achieved 5 and 2% higher in PP and\\nPF1 than GFAD. Similarly, it achieved 15 and 11% higher in\\nPP and PF1 than HHC. GFAD performed better in PR by 1%\\nhigher than LUCID.\\nLUCID performance is overall better as compared to base-\\nline methods due to the reason that this takes the advantage\\nof semantic relationships between authors. GFAD is unable to\\ndetect outlier authors that only share one co-author whereas\\nLUCID is able to identify these authors. HHC only takes into\\naccount the syntax similarities of attributes not semantics.\\nComparison between ground truth clusters and LUCID\\ngenerated clusters is shown in Fig. 8. About 70% clusters are\\nwithin the range (Ground Truth Clusters \\x064). However, in the\\nmajority of cases, LUCID produced more clusters than the\\nground truth clusters. This is due to the fact that many authors\\nhave multiple afﬁliations and research interests in their entire\\ncareer and thus have different social circles.0 5 10 15 20 25020406080\\nAmbiguous authorNumber of clustersGTC\\nOC\\nFig. 8. Comparison of ground truth clusters and LUCID generated clusters\\n(GTC denotes Ground Truth Clusters and OC refers to LUCID generated\\nClusters)\\nLUCID performance is better than baseline methods in\\nall metrics except in PR where it is 1% lower than GFAD.\\nHowever, LUCID still has some limitations; (a) it is unable to\\nidentify when two different authors with the same name have\\ndifferent co-authors with the same name, (b) it also fails to\\ndisambiguate when two citation records do not share common\\nco-authors although they might be written by the same authors,\\n(c) it may not resolve authors correctly when an author changes\\nhis/her area of research and works with entirely different set\\nof co-authors.\\nV. C ONCLUSION\\nCommunity detection algorithms and hybrid similarity are\\nlittle used in the domain of author name disambiguation.\\nIn this paper, we have proposed LUCID that solves author\\nname ambiguity problem using only co-authors. LUCID is\\na graph-based method that does not require costly training\\ndata or a priori or hidden information (how many ambiguous\\nauthors) and web searches, solves both homonym and synonym\\nproblem. In this study, we present homonym resolver algorithm\\nand synonym resolver algorithm as our main contributions.\\nLUCID performance was tested on Arnetminer, a real\\nworld data set and it showed better results than two baseline\\nstudies HHC and GFAD. LUCID delivered a simple but effec-\\ntive solution to author name ambiguity problem as it utilized\\nthe powers of graph-based algorithms. However, it is unable\\nto detect very ambiguous author cases where two different\\nauthors with the same name work with two different co-authors\\nwith the same name and in the cases where a researcher has\\nmultiple different research interests and is working separately\\non these research interests with completely different set of\\nco-authors. In the future, we plan to analyze self-citations,\\nhidden concepts and email address of authors to overcome\\nthese limitations.\\nREFERENCES\\n[1] Indrajit Bhattacharya and Lise Getoor. Collective entity\\nresolution in relational data. ACM Transactions on\\nIEEE 7 jP a g eIntelligent Systems Conference 2017\\n7-8 September 2017 jLondon, UK\\nKnowledge Discovery from Data (TKDD) , 1(1):5, 2007.\\n[2] Ricardo G Cota, Anderson A Ferreira, Cristiano Nasci-\\nmento, Marcos Andr ´e Gonc ¸alves, and Alberto HF Laen-\\nder. An unsupervised heuristic-based hierarchical method\\nfor name disambiguation in bibliographic citations. Jour-\\nnal of the American Society for Information Science and\\nTechnology , 61(9):1853–1870, 2010.\\n[3] Ana Paula De Carvalho, Anderson A Ferreira, Alberto HF\\nLaender, and Marcos A Gonc ¸alves. Incremental unsuper-\\nvised name disambiguation in cleaned digital libraries.\\nJournal of Information and Data Management , 2(3):289,\\n2011.\\n[4] Xiaoming Fan, Jianyong Wang, Xu Pu, Lizhu Zhou, and\\nBing Lv. On graph-based name disambiguation. Journal\\nof Data and Information Quality (JDIQ) , 2(2):10, 2011.\\n[5] Anderson A Ferreira, Marcos Andr ´e Gonc ¸alves, and\\nAlberto HF Laender. A brief survey of automatic methods\\nfor author name disambiguation. volume 41, pages 15–\\n26. ACM, 2012.\\n[6] Anderson A Ferreira, Adriano Veloso, Marcos Andr ´e\\nGonc ¸alves, and Alberto HF Laender. Effective self-\\ntraining author name disambiguation in scholarly digital\\nlibraries. In Proceedings of the 10th annual joint confer-\\nence on Digital libraries , pages 39–48. ACM, 2010.\\n[7] Donghong Han, Siqi Liu, Yachao Hu, Bin Wang, and\\nYongjiao Sun. Elm-based name disambiguation in bibli-\\nography. World Wide Web , 18(2):253–263, 2015.\\n[8] Hui Han, Lee Giles, Hongyuan Zha, Cheng Li, and\\nKostas Tsioutsiouliklis. Two supervised learning ap-\\nproaches for name disambiguation in author citations. In\\nDigital Libraries, 2004. Proceedings of the 2004 joint\\nACM/IEEE conference on , pages 296–305. IEEE, 2004.\\n[9] Jianbin Huang, Heli Sun, Qinbao Song, Hongbo Deng,\\nand Jiawei Han. Revealing density-based clustering\\nstructure from the core-connected tree of a network.\\nIEEE Transactions on Knowledge and Data Engineering ,\\n25(8):1876–1889, 2013.\\n[10] Tin Huynh, Kiem Hoang, Tien Do, and Duc Huynh.\\nVietnamese author name disambiguation for integrating\\npublications from heterogeneous sources. In Asian Con-\\nference on Intelligent Information and Database Systems ,\\npages 226–235. Springer, 2013.\\n[11] Muhammad Imran, Syed Gillani, and Maurizio March-\\nese. A real-time heuristic-based unsupervised method\\nfor name disambiguation in digital libraries. D-Lib\\nMagazine , 19(9):1, 2013.\\n[12] Felipe Hoppe Levin and Carlos A Heuser. Evaluating the\\nuse of social networks in author name disambiguation\\nin digital libraries. Journal of Information and Data\\nManagement , 1(2):183, 2010.\\n[13] Yuechang Liu and Yong Tang. Network based framework\\nfor author name disambiguation applications. Interna-\\ntional Journal of u-and e-Service, Science and Technol-\\nogy, 8(9):75–82, 2015.\\n[14] Eamonn James Maguire. Ethnicity sensitive author\\ndisambiguation using semi-supervised learning. In\\nKnowledge Engineering and Semantic Web: 7th Interna-\\ntional Conference, KESW 2016, Prague, Czech Republic,\\nSeptember 21-23, 2016, Proceedings , volume 649, page\\n272. Springer, 2016.\\n[15] Byung-Won On, Ingyu Lee, and Dongwon Lee. Scalable\\nclustering methods for the name disambiguation prob-lem. Knowledge and Information Systems , 31(1):129–\\n151, 2012.\\n[16] Natsuo Onodera, Mariko Iwasawa, Nobuyuki Mi-\\ndorikawa, Fuyuki Yoshikane, Kou Amano, Yutaka\\nOotani, Tadashi Kodama, Yasuhiko Kiyama, Hiroyuki\\nTsunoda, and Shizuka Yamazaki. A method for elim-\\ninating articles by homonymous authors from the large\\nnumber of articles retrieved by author search. Journal\\nof the American Society for Information Science and\\nTechnology , 62(4):677–690, 2011.\\n[17] Hsin-Tsung Peng, Cheng-Yu Lu, William Hsu, and Jan-\\nMing Ho. Disambiguating authors in citations on the\\nweb and authorship correlations. Expert Systems with\\nApplications , 39(12):10521–10532, 2012.\\n[18] Dongwook Shin, Taehwan Kim, Joongmin Choi, and\\nJungsun Kim. Author name disambiguation using a\\ngraph model with node splitting and merging based on\\nbibliographic information. Scientometrics , 100(1):15–50,\\n2014.\\n[19] Jie Tang, Alvis CM Fong, Bo Wang, and Jing Zhang. A\\nuniﬁed probabilistic framework for name disambiguation\\nin digital library. IEEE Transactions on Knowledge and\\nData Engineering , 24(6):975–987, 2012.\\n[20] Li Tang and John P Walsh. Bibliometric ﬁnger-\\nprints: name disambiguation based on approximate struc-\\nture equivalence of cognitive maps. Scientometrics ,\\n84(3):763–784, 2010.\\n[21] Hung Nghiep Tran, Tin Huynh, and Tien Do. Author\\nname disambiguation by using deep neural network.\\npages 123–132, 2014.\\n[22] Pucktada Treeratpituk and C Lee Giles. Disambiguating\\nauthors in academic publications using random forests. In\\nProceedings of the 9th ACM/IEEE-CS joint conference on\\nDigital libraries , pages 39–48. ACM, 2009.\\n[23] Jian Wang, Kaspars Berzins, Diana Hicks, Julia Melkers,\\nFang Xiao, and Diogo Pinheiro. A boosted-trees method\\nfor name disambiguation. Scientometrics , 93(2):391–411,\\n2012.\\n[24] Xuezhi Wang, Jie Tang, Hong Cheng, and S Yu Philip.\\nAdana: Active name disambiguation. In 2011 IEEE 11th\\nInternational Conference on Data Mining , pages 794–\\n803. IEEE, 2011.\\n[25] William E Winkler. String comparator metrics and\\nenhanced decision rules in the fellegi-sunter model of\\nrecord linkage. 1990.\\n[26] Hao Wu, Bo Li, Yijian Pei, and Jun He. Unsupervised\\nauthor disambiguation using dempster-shafer theory. Sci-\\nentometrics , 101(3):1955–1972, 2014.\\n[27] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and\\nThomas AJ Schweiger. Scan: a structural clustering\\nalgorithm for networks. In Proceedings of the 13th\\nACM SIGKDD international conference on Knowledge\\ndiscovery and data mining , pages 824–833. ACM, 2007.\\n[28] Jia Zhu, Yi Yang, Qing Xie, Liwei Wang, and Saeed-Ul\\nHassan. Robust hybrid name disambiguation framework\\nfor large databases. Scientometrics , 98(3):2255–2274,\\n2014.\\nIEEE 8 jP a g e\", 'Turk J Elec Eng & Comp Sci\\n() : {\\nc⃝T\\x7fUB_ITAK\\ndoi:10.3906/elk-1702-293\\nTurkish Journal of Electrical Engineering & Computer Sciences \\nhttp://journals.tubitak.gov.tr/elektrik/\\nResearch Article \\nResolving namesakes using the author\\'s social network\\nIjaz HUSSAIN\\x03, Sohail ASGHAR\\nDepartment of Computer Science, Faculty of Information & Technology, COMSATS Institute\\nof Information Technology, Islamabad, Pakistan\\nReceived: 21.02.2017 \\x0f Accepted/Published Online: 12.11.2017 \\x0f Final Version: ..201\\nAbstract: Author name ambiguity may occur when multiple authors share the same name or diﬀerent name variations of\\na single author exist. This degrades search results and correct attributions in bibliographic databases. Existing solutions\\nrequire either the actual number of ambiguous authors or extra information that is collected from the Web. However,\\nin many scenarios, obtaining such auxiliary information is not possible or requires much extra eﬀort. An eﬀective\\nand scalable method, ASONET, is proposed that uses graph community detection algorithms and graph operations\\nto disambiguate namesakes. The citation dataset is preprocessed and ambiguous author blocks are formed. A graph\\nstructural clustering, gSkeletonClu, is applied to identify hubs, outliers, and clusters of nodes in a coauthor\\'s graph.\\nNamesakes are resolved by splitting these clusters across the hub if their feature vector similarity is less than a prede\\x0cned\\nthreshold. ASONET utilizes only coauthors and titles that are surely available in all bibliographic databases. To validate\\nthe ASONET performance, experiments are performed on two real-world datasets of Arnetminer and DBLP. The results\\ncon\\x0crm that ASONET is scalable and outperforms baselines.\\nKey words: Author name disambiguation, namesakes, graph structural clustering, community detection\\n1. Introduction\\nBibliographic databases conserve bibliographic citations and provide several services, such as receiving items\\nassociated with a particular author, multiple searches, browsing personalization, and building communities\\nwith certain educational \\x0celds [1]. One of the main challenges highlighted by Lee et al. in [1] is to have high-\\nquality contents in bibliographic databases come from several sources of errors. Among these sources of errors,\\nresearchers paid great attention to ambiguous author names due to their nontrivial solution [1{3]. The systems\\nthat resolve author name ambiguity from publications are called author name disambiguation (AND) systems.\\nAuthor name ambiguity occurs when multiple authors share the same name. In this case, it is diﬃcult to be\\ncertain about the accuracy of retrieved results. This phenomenon of citation merger, the case of two or more\\npersons having the same name (e.g., \\\\Wei Wang\"), is also known as mixed citations [3].\\nGenerally, ambiguity in author names is resolved by means of diﬀerent publication attributes such as\\ncoauthors, title, abstract, venue, and publication year [1,3,4]. However, each bibliographic database does not\\nprovide all these attributes; they provide only limited information and manual annotation is not possible on such\\na large scale. Furthermore, in recent years, large amounts of publication data are being created and accepted\\nby bibliographic databases that make the name ambiguity problem even more severe than it was in the past [1].\\nWhen we search for an author named \\\\Wei Wang\" in DBLP (a renowned bibliographic database), we get\\n\\x03Correspondence: ijazhussain7979@hotmail.com\\n1HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\n86 authors having the same name. The same situation occurs in almost all major bibliographic databases [3].\\nThis example is representative of the magnitude of the mixed citation problem that motivated us. Moreover,\\nDBLP and other bibliographic databases are frequently not informative enough in their metadata and lack\\npieces of important information such as an author\\'s aﬃliation, e-mail address, medical subject heading (present\\nonly in PubMed), and publication references [4].\\nSeveral AND methods have been proposed in the literature [2{25]. These methods have improved the\\nsituation somewhat, but there is still a need for improvement in current solutions. Supervised methods [2,5,6,20{\\n21,25] require much clean and representative data for the training of the model and give poor results when a\\nmodel is trained on noisy data or nonrepresentative data [1,3]. Most unsupervised AND methods assume that a\\nnumber of ambiguous authors/clusters \\\\K\" are known in advance [4,7,10,12]. Some techniques are not scalable\\nwhen the number of ambiguous authors increases [2,3,16]. Some require extra information from the Web or user\\nfeedback for disambiguation [7,16].\\nIn this paper, ASONET (\\\\resolving namesakes using the Author\\'s SOcial NETwork\") is proposed to\\nresolve namesakes, as shown in Figure 1. ASONET is a graph-based method in which a citation dataset\\nis preprocessed, blocks of the ambiguous author are created, and the coauthor\\'s graph is constructed using\\nthe preprocessed citation data. Then gSkeletonClu is used to identify hubs, outliers, and clusters of vertices\\n(communities) from the coauthor\\'s graph [26]. ASONET resolves namesakes by splitting these communities\\nacross the hub nodes if their title feature vector similarity is less than a prede\\x0cned threshold and using\\ngraph operations. However, distinct from existing techniques, we exploit the community detection algorithm in\\ncombination with graph operations to disambiguate namesakes. To the best of our knowledge, the ASONET\\nalgorithm is the \\x0crst that uses a gSkeletonClu community detection algorithm to disambiguate namesakes.\\nAuthor ‘A’\\nPubl/i.dotcat/i.dotons\\nAuthor ‘B’\\nPubl/i.dotcat/i.dotons\\nAuthor ‘C’\\nPubl/i.dotcat/i.dotons\\nRaw C/i.dottat/i.dotons Preprocess/i.dotng Graph Constructor Commun/i.dott y D etector &  Spl/i.dottter D /i.dotsamb/i.dotguated C/i.dottat/i.dotonsCluster 1\\nCluster 2Outl/i.doterD /i.dotsamb/i.dotguated Authors\\nFigure 1 .ASONET architecture.\\nASONET\\'s performance is evaluated by comparing it with three unsupervised graph-based AND methods\\nusing Arnetminer and DBLP datasets [3,4,7].\\n2. Related work\\nAccording to Ferreira et al. [1], AND methods can be categorized as author assignment methods [2,5,6,9,20,21]\\nand author grouping methods [3,4,7,8,11,12,18,24].\\nOnodera et al. proposed a methodology for targeting the author from false homonym authors [13]. They\\nused publication features such as aﬃliation, title, and coauthors for discriminating the oeuvre of an author.\\nZhu et al. proposed a hybrid name disambiguation framework that not only used the traditional information\\n(coauthors) but also Web page genre information [14]. This framework consists of two main steps: Web page\\n2HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\ngenre identi\\x0ccation and a reclustering model. In Web page genre identi\\x0ccation, the returned pages are classi\\x0ced\\nas either home page of an author or not. Those records found at the author\\'s homepage belong to him and\\nare disambiguated. The remaining ambiguous records are disambiguated using coauthors\\' information. Some\\nrecords that are not disambiguated from coauthors or from the Web information are sent to the reclustering\\nmodel. They then build a graph \\\\ G\" using all citation records, in which each vertex represents a citation record\\nand each edge denotes the same coauthor\\'s relationship. If there are many links present between two vertices,\\nthen they are considered to be related/close to each other. They transfer this graph into a similarity matrix by\\nusing a multidimensional scaling algorithm. This algorithm detects similarities among objects. They constructed\\nmany two-dimensional matrices for coauthorship and the topic relationship and calculated the distance between\\ntwo vertices with the help of the Euclidean metric. If the distance between citations is less than a speci\\x0cc\\nthreshold, then they are considered to be by the same author. This method is rather slow as it crawls results\\nfrom the author\\'s home page, identi\\x0ces their genre, and requires more computations. One other issue with this\\nmethod is how to identify that this is an author\\'s home page or not and it is a nontrivial task. Dempster{Shafer\\ntheory (DST) was fused with Shannon entropy (SE) for author name disambiguation in [12]. In the \\x0crst step,\\nhigh-level features such as Aﬃliation, Venue, ContentSim, Coauthor, Citation, and Webcorrelation and their\\ncorrelation similarities are calculated. In the next step, these features are fused using DST and SE. On the\\nbasis of this information, belief and plausibility of each author are calculated. Then they obtain a matrix\\nof pairwise correlations of papers. Each entry in this matrix is linked to a belief function and a plausibility\\nfunction. In the end, they apply a DST-based hierarchical agglomerative clustering (DSHAC) algorithm for\\nauthor disambiguation. In the process of clustering they use three diﬀerent convergence conditions for the\\nclustering algorithm: a preset number of clusters, number of available pieces of evidence, and distance between\\nclusters. This method has lower accuracy than other unsupervised methods. The dataset used is relatively small\\nand specialized so the method may not work well on other datasets and thus needs to be tailored to diﬀerent\\nsituations. In the clustering process, the main hindrance of this method is where to stop the clustering process.\\nThe authors use three convergence conditions: the preset number of clusters, number of pieces of available\\nevidence, and distance between clusters. However, in real-world settings these three conditions are not feasible.\\nFurthermore, this method suﬀers from over\\x0ctting and optimistic accuracy estimates.\\nCarvalho et al. proposed INDi as a good solution for the existing cleaned/disambiguated DLs in [15].\\nINDi utilizes similarity among bibliographic records and groups the new records for authors with similar citation\\nrecords in the DL or new authors when the similarity evidence is not strong enough. Some particular heuristics\\nare used for checking whether references of new citation records belong to preexisting authors of the DL or if\\nthey belong to new ones (i.e. authors without citation records in the DL), avoiding running the disambiguation\\nprocess in the entire DL. They run simulations on the BDBComp collection and synthetic DSs are used to\\nassess the eﬀectiveness of their method. They compare it with a state-of-the-art unsupervised method. Their\\nexperiments show gains of up to 19% when compared to a state-of-the-art method without the cost of having to\\ndisambiguate the whole DL at each new load (as done by supervised methods) or the need for any training (as\\ndone by supervised methods). This is an incremental method that disambiguates only new entries. Liu et al.\\nin [9] used a three-step clustering framework for name disambiguation. In the \\x0crst step, they obtained clusters\\nbased on common coauthors. Titles were then used to make bigger clusters from the \\x0crst step\\'s fragmented\\nclusters. Finally, they fused clusters based on venues. An ethnicity-sensitive method that mainly comprises\\nthree parts was presented in [15]. In the \\x0crst part, phonetic-based blocking for similar author signatures was\\ndone. A supervised machine learning-based linkage function was used that exploited the ethnicity-sensitive\\ninformation. Finally, hierarchical agglomerative clustering was done based on a distance between two pairs of\\n3HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\npublications\\' linkage functions. In [16], the authors proposed an algorithm that used both Web correlation and\\nauthor correlation-based approaches to measure similarities between publications.\\nADANA (Active nAme Disambiguation for the Name Ambiguity problem) was proposed in [7]. In this\\nmethod, the authors modeled a pairwise factor graph that can be used to integrate several features and user\\nfeedback into a uni\\x0ced model. They de\\x0cned three types of feature functions, i.e. document pair, correlation, and\\nconstraint-based features. In document pair feature functions, they found known relationships from publications.\\nIn the correlation feature function, they found some hidden features with the help of known functions, and in the\\nconstraint-based feature functions the user was involved in \\x0cnding the unknown features. Lastly, they exploited\\nactive selection of the user corrections in an interactive mode to improve the disambiguation performance after\\nsome preliminary clustering results. Some additional information was used for disambiguation, such as aﬃliation\\nand references, which is not present in every bibliographic database.\\nLi et al. in [18] proposed a categorical set similarity measure to disambiguate namesakes. They used the\\nauthor\\'s preference for speci\\x0cc venues with the help of a categorical distribution and calculated the probability to\\nestimate the true authors when two sets of authors belong to the same distribution. They used this probability\\nas the similarity measure for \\x0cnding that either of two sets belong to the same author. Vishnyakova presented\\na journal descriptor indexing (JDI) tool to resolve ambiguous authors [19]. It utilized titles, abstracts, and\\nMeSH terms as an input to the JDI and it returned the journal descriptors and semantic types as its output.\\nThis information was used as a feature for the supervised model. This model can only be used on MEDLINE,\\nnot on other bibliographic databases. Tran et al. in [20] suggested a deep neural network-based approach to\\nautomatically learn the weights of the features and disambiguate the authors. Determining optimal number of\\nhidden layers, data representations for the \\x0crst layer, and number of units is a complex task and requires skill\\nand experience. Incremental disambiguation methods were proposed by Qian et al. and Santana et al. that can\\nbe used on an already disambiguated database [22,23]. Torvik et al. presented a name disambiguation method\\nthat used much auxiliary information such as shared title words, journal name, coauthors, medical subject\\nheadings, language, e-mail, aﬃliations, and author name features, which are not available in many bibliographic\\ndatabases [24].\\nIn contrast to previous AND techniques, ASONET requires no costly training data, it uses only coauthors\\nand titles information, and it does not need to set a priori the number of ambiguous author clusters. A\\ncomparison between related methods with ASONET is done in Table 1.\\n3. Proposed methodology of ASONET\\nWe implemented the gSekeletonClu algorithm [26] in Python using the Networx package to detect hubs, outliers,\\nand clusters (communities) in the coauthor\\'s graph, as given in the ASONET algorithm. More details of\\ngSkeletonClu can be found in [26].\\n3.1. Preprocessing of dataset\\nThe raw citations are preprocessed and split into authors, titles, and venues terms. In blocking, ambiguous\\nauthors are grouped in candidate classes using a prede\\x0cned similarity threshold. The fragment comparison\\nmethod is used for this purpose because it proved to be very eﬀective in some earlier studies [4,6]. The blocking\\nstage is very crucial as it aﬀects the computations in the later stages of name disambiguation. The blocking\\nstage returns `b\\' number of blocks if given `a\\' number of authors.\\nThe computation complexity is O( a2) for ` a\\' authors if we do not use blocking, whereas blocking\\n4HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nTable 1 .Comparison between diﬀerent related methods.\\nRef. Authors  Scalable  Methodology  Dataset  Features \\n7 \\nADANA  Known  No User feedback -based \\nAND Publication, \\nWeb page, news \\nstories  Citation, Coauthor, \\nCovenue, Coaffiliation, \\nCoaffoccur, Titlesim, \\nCohomepage  \\n12 \\nDSHAC  Uses \\nknown/unknown  No Dempster –Shafer \\ntheory, Shann on \\nentropy  DBLP  Coauthor, Affiliation, \\nVenue, Content \\nsimilarity, Citations, \\nWeb co rrelations \\n14 \\nMMC  Unknown  Yes Logistic regression  Subset of WoS  Coauthor, Affiliation, \\nCitation relations, \\nTitles, Cocitations, Year, \\nAffiliation country  \\n15 \\nINDi Known  Yes Multidimensional \\nscaling  DBLP Uses Web genre \\nidentification -based \\ngraph clustering  \\n16 \\nModified \\nMNDF  Unknown  No Heuristic  BDBComp, \\nsynthetic  Author, Coauthors, \\nTitle , and Venue  \\n17 \\nASE  Known  No Web, authorship \\ncorrelations DBLP  Uses additional \\ninformation from Web  \\n20 \\nDNN Known  No Deep neural network  Vietnamese  Author name, \\nAffiliation, Coauthor  \\n21 \\nJDS Known  No Journal descriptor, \\nsemantics  MEDLINE  Titles, Abstracts , and \\nMedical Subject \\nHeading  \\n25 \\nNB/SVM  Unknown  No Similarity- based \\nclustering MEDLINE  Shared title words, \\nJournal, C oauthors, \\nMeSH, Language, e -\\nmail, Affiliations, \\nAuthors  \\nProposed  Unknown  Yes Graph structural \\nclustering, feature \\nvector  Arnetminer, \\nDBLP  Coauthors, Title  \\nconsiderably reduces computational complexity to O( BjSj), where ` B\\' is the number of blocks and ` S\\' is\\nthe average size of blocks. The similarity threshold controls the precision and recall of blocking. If we set the\\nsimilarity threshold too high, then recall is very low, but it creates very pure blocks. On the other hand, if we\\nset it very low, then high false positives are produced. For the selection of the threshold, we sampled names\\nand set this threshold to 0.7, which produces the optimal blocking of ambiguous names.\\n3.2. Graph construction\\nAfter blocking, each citation is converted into a set of ` n\\' authors in a graph G = (V,E) . Authors are extracted\\nfrom the citation dataset and each distinct author aiis mapped to a vertex v2V. Then, for each citation,\\nif there are ` a\\' number of authors then there are . number of edges created in the graph ` G\\'. In this way, the\\ncoauthor\\'s graph of all the citations in the dataset is formed. A small working example of the citation dataset,\\n5HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nvertices, edges, and constructed graph is shown in Figure 2. There are seven authors (nodes) and ten edges\\nthat are created from six citations. \\\\Wei Wang\" is a node that is common to \\x0cve citations and \\\\M. Rehan\" is\\npresent only in citation 4.\\nFigure 2 .Coauthor\\'s graph of Wei Wang sample citations.\\n3.3. Detecting author\\'s social network\\nThe ASONET algorithm detects the namesakes using the gSkeletonClu community detection algorithm [26]\\nand then resolves these namesakes by applying graph operations. The use of gSkeletonClu is done here for\\nfour reasons: 1) It detects outliers that are not possible with existing graph-based methods. 2) It makes the\\nalgorithm scalable as its computational complexity is much lower than that of other competing community\\ndetection methods, as given in Section 1. 3) In contrast to other popular community detection algorithms, it\\nis useful to identify overlapping communities in the coauthor\\'s graph. 4) There is no need to tune threshold\\nparameters; it automatically computes these parameters. In graph-based structural clustering algorithms, two\\nparameters are usually required to be chosen, \"and\\x16, where \"2 ℜ and\\x162N. A threshold \"is applied\\nto the computed structural similarity when assigning cluster membership. Core vertices are a special class of\\nvertices that have a minimum of \\x16neighbors with a structural similarity that is above a speci\\x0ced threshold.\\nTypical values for \\x16and\"are 2 and 0.7. From core vertices, we grow the similar group of clusters. In this\\nway, the parameters \\x16and\"determine the clustering of networks. In gSkeletonClu, the optimal values of both\\nthese parameters are calculated automatically. For further details, interested readers can refer to the work of\\nHuang et al. in [26].\\n6HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nASONET assumes that diﬀerent namesakes have diﬀerent communities in an academic social circle and\\ndiﬀerent namesakes seldom work in the same institution or community [3,4]. Thus, they belong to diﬀerent\\nauthor communities. A community is generated from each citation in the coauthor\\'s graph and thus each\\ncommunity denotes the coauthorship for each citation that is the smallest social circle in the academic domain.\\nFor example, in `citation 4\\' there are three authors, 3, 4, and 5. Due to this citation the upper three nodes\\nand their relationships are constructed as shown in Figure 2c and when we add more citations to this graph it\\nbecomes wider and bigger. Finally, when we construct the graph of all sample citations in Figure 2a, it looks\\nas shown in Figure 2. With the help of the coauthor graph, it is possible to infer the social circle of an author\\nfrom one\\'s coauthors by \\x0cnding shared communities.\\n3.4. Feature vector construction\\nAfter removing stop words and stemming those words, we construct feature vectors of titles. ASONET assumes\\nthat results for an author linked with multiple social circles contain mixed information for several authors if\\nthe similarity between feature vectors of two clusters is less than 0.2. This threshold is chosen empirically by\\ndiscretizing the search space and then varying its value in small increments of 0.01 in the range of 0.1 to 0.3\\nand \\x0cnding the optimal value of it. It is worthwhile to mention here that this step is needed only once for\\nall datasets. ASONET splits that node and its information into several diﬀerent nodes that are present in\\nnonoverlapping communities.\\nFeature vector example : Suppose we have two titles of papers T1 and T2.\\nT1: Disambiguating homonyms using graph-based community detection algorithms.\\nT2: Graph semantic similarity for author name disambiguation.\\nAfter removing stop words and stemming, we obtain the following titles, T1\\' and T2\\'.\\nT1\\': disambiguate homonym graph community detect algorithm\\nT2\\': graph semantic similar author name disambiguate\\nLet us have a look at the total vocabulary now: disambiguate, homonym, graph, community, detect,\\nalgorithm, semantic, similar, author, name.\\nVocabulary size is 10 words, so the vector will have 10 dimensions.\\nLet us \\x0ct our title documents into vectors:\\nD1 : [ 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 ]\\nD2 : [ 1, 0, 1, 0, 0, 0, 1, 1, 1, 1 ]\\nThere are two words in common between the two title vectors of the papers out of a total 10 words, so\\ntheir title similarity is 0.2.\\n3.5. ASONET algorithm\\nIn the coauthor\\'s graph, an edge represents a coauthor\\'s relationship between the two nodes (authors). The\\nASONET algorithm (Algorithm 1) is based on gSkeletonClu, a structural clustering algorithm for networks,\\nto detect diﬀerent nonoverlapping communities in the coauthor\\'s graph [26]. In this algorithm, when we use\\ngSkeletonClu on a coauthor\\'s graph, it outputs communities (clusters of nodes), hub nodes, and outliers.\\nThe community consists of a set of nodes from the coauthor\\'s graph. The node that is involved in many\\nsocial circles in the coauthor\\'s graph is called a hub node. The hub node is the potential homonym that needs\\ndisambiguation. Outliers are nodes that only contribute one or a limited number of publications with the hub\\nnode and have no other coauthors. In the example coauthor\\'s graph, detected communities, hub, and outlier\\n7HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nAlgorithm 1 ASONET algorithm\\nData : Coauthors Graph (CG)  \\nResult : Disambiguated Graph (DG)  \\n1 begin \\n2 Author s ← PreprocessData (CitationsData)  \\n3 CG ← BuildGraph (Authors)  \\n4 [ClustersList, Hub, Outliers]  ← applygSkeletonClu (CG)  \\n5 HubCitations  ← get.Citations (Hub)  \\n6 FVHub ← get.FV (Hub)  \\n7 k, l ← 0 \\n8 for cluster  ∈  ClustersList  do \\n9 clusterCitations  ← 0 \\n10 FVC  ←  get.FV (Cluster)  \\n11 FV Similarity ←  sim (FVC , FVHub) \\n12 if FV Similarity < 0.2  then  \\n13 for vertex ∈ cluster do \\n14 clusterCitations  ←   clusterCitations  U getCitations  (vertex)  \\n15 end \\n16 NSkCitations  ←  HubCitations  I clusterCitations  \\n17 NSkName  ←  getName (Hub)  \\n18 NSkId ←  getLength (CG)  \\n19 CG.InsertNewNode (NSkId, NSkName, NSkCitations)  \\n20 UpdateGraph (CG, outlier)  \\n21 HubCitations  ←  HubCitations \\\\ clusterCitations  \\n22  k ←  k + 1  \\n23  else \\n24 cluster ←  cluster + 1  \\n25  end \\n26 end \\n27 for outlier ∈ outliers do \\n28 outlierCitations ←  0 \\n29 FVO ←  get.FV (outlier)  \\n30 FV Similarity ←  sim (FVO , FVHub) \\n31 if FV Similarity < 0.2  then  \\n32 OlCitations  ←  HubCitation s I outlierCitations \\n33 OlName  ←  getName (Hub)  \\n34 OlId ←  getLength (CG)  \\n35 CG.InsertNewNode (OlId, OlName, OlCitations)  \\n36 UpdateGraph (CG, outlier)  \\n37 HubCitations  ←  HubCitations \\\\ outlierCitations \\n38 l ←  l + 1  \\n39 else \\n40 outlier  ←  outlier + 1  \\n41 end \\n42 end \\n43 end \\n8HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\ncan be seen in Figure 3. There are two clusters of nodes (communities), one hub, and one outlier. The \\x0crst and\\nsecond clusters consist of nodes <3, 4, 5 >and<6, 7>, respectively. The hub is node <2>and the outlier is\\nnode <1>. A hub node that has multiple nonoverlapping communities contains mixed information for several\\nnamesakes. ASONET splits the information about each author on the node containing the namesakes along\\nthe nonoverlapping communities if its feature vector similarity is less than 0.2. This part starts from line 7 of\\nthe ASONET algorithm, where the hub node publications list is retrieved. Similarly, publications of all cluster\\nnodes (community) are retrieved and saved in a list (lines 13{15). Now, for each homonym, the intersection\\nof hub publications and community publications is found. A new node is created in the coauthorship graph\\nfor each community detected that has the same name as that of the hub node and has an identity one more\\nthan in the current nodes of the coauthorship graph. This newly created node has the publication list that is\\nfound at line 16. Likewise, for all communities detected in the coauthor\\'s graph, new nodes are created. The\\ncoauthor\\'s graph and hub node publications are updated on each new node insertion into the graph. The same\\nprocedure is repeated for all outliers, as is done in the case of all communities (lines 27{41). This whole process\\nis pictorially shown in Figure 4, where hub node <2>is split into two new nodes, <2A, 2B >, as there are two\\nclusters of nodes (communities) that have feature vector similarity less than 0.2. In this way, the namesake\\'s\\nproblem is solved using the community detection algorithm.\\n24\\n3 5\\n6 71Cluster 1\\nCluster 2Hub Outl/i.doter\\n24\\n3 5\\n6 71Cluster1\\nCluster2Hub Outlier\\nFigure 3 .Example of detected communities, hub, and\\noutlier in Wei Wang\\'s sample. graph after clustering.Figure 4 .An example of namesake resolution.\\n3.6. ASONET complexity analysis\\nThe initial stage of ASONET is to create ambiguous author blocks that can be created in O (B jSj), where `B\\'\\nis the number of blocks and `S\\' is the average size of the blocks. According to Huang et al., graph \\\\G (V, E)\"\\ncan be constructed in O( jVj+jEj) and graph structural clustering is done in O( jEj) [26]. Text feature vectors\\nand other graph comparison are done only in small blocks found in the initial stage of ASONET. These stages\\ntake negligible time as compared to other stages. In the above three steps, the blocking step dominates the\\ncomputational complexity. Hence, ASONET has an overall time complexity of O(B jSj). GHOST, presented\\n9HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nby Fan et al. in [4], used valid path-based similarity between two nodes in the order of O((\\x0c\\x0cR2\\x0c\\x0c)\\x02(jVj+jEj)),\\nwhere V is the vertices, E is the edges, and R is the set of names to resolve. ADANA, proposed by Wang et\\nal. in [7], used a pairwise factor graph that was also intractable due to the calculation of the normalization\\nfactor, which made their method complexity exponential to the number of nodes in the graph. Similarly, GFAD,\\nproposed by Shin et al. in [3], used Johnson\\'s simple cycle enumeration algorithm [27] that has a complexity of\\nO((V + E)(C + 1)) with V for vertices, E for edges, and C for elementary cycles in the graph. A system is said\\nto be scalable if it is applied to a larger dataset and gives statistically the same results. For instance, a system\\ndesigned for Arnetminer (small dataset) is applied to DBLP (a large dataset), and if it gives approximately\\nthe same results, then it is said to be scalable. These complexities of all the techniques are summarized in the\\nTable 2.\\nTable 2 .A comparison between baseline techniques.\\nReference Technique Complexity\\n[3] GFAD O((V+E)(C+1))\\n[4] ADANA Intractable\\n[7] GHOST O((\\x0c\\x0cR2\\x0c\\x0c)\\x02(jVj+jEj))\\nProposed ASONET O(BjSj)\\n4. Performance evaluation of ASONET\\nIn this section, the performance of ASONET is compared with baseline methods using Arnetminer and DBLP\\ndatasets in the form of clustering evaluation metrics.\\n4.1. Datasets\\nTo evaluate the ASONET performance, the \\x0crst dataset is Arnetminer, whose details are given in Table 3.\\nThis dataset was originally created by Wang et al. [7] and has been used in many previous studies with\\nslight variations [3,4,7]. We also choose a small subset of this dataset that is composed of 950 citation records\\nassociated with 344 distinct authors belonging to 39 ambiguous groups, with approximately 2.76 citation records\\nper author, as shown in Table 3.\\nCurrently, the DBLP lists more than 1500 journals and 5000 conferences and workshop series in computer\\nscience. The second collection is taken from DBLP that is composed of 8442 citation records associated with\\n480 distinct authors belonging to 14 ambiguous groups, which means an average of approximately 18 citation\\nrecords per author, as shown in Table 4. The original version of this collection was created by Han et al. in\\n2004, and, with slight variations, it has been used in performance evaluation of various author disambiguation\\nmethods [2,3,5,6,17]. Han et al. created this collection by collecting bibliographic citation records from DBLP\\nand authors\\' homepages [25]. After that, they transformed author names to abbreviated forms consisting of the\\n\\x0crst name\\'s initial and the last name and clustered the bibliographic citation records into ambiguous groups,\\neach of which corresponds to the authors with the same abbreviated name. This dataset is now considered as\\na de facto benchmark of AND.\\n4.2. Evaluation metrics\\nWe used average cluster purity (ACP), average author purity (AAP), K-metric, pairwise precision (PP), pairwise\\nrecall (PR), pairwise-F1 (PF1), cluster precision (CP), cluster recall (CR), and cluster-F1 (CF1) to measure\\nthe eﬀectiveness of ASONET [3,6].\\n10HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nTable 3 .Details of Arnetminer dataset.\\nName Ath. Rec. Name Ath. Rec Name Aut. Rec\\nAjay Gupta 8 31 B. Wilkinson 1 17 Bob Johnson 3 4\\nBin Zhu 15 45 Cheng Chang 5 23 M Lang 4 16\\nCharles Smith 4 7 Paul Wang 6 15 Fei Su 4 37\\nMichael Siege 6 50 David Cooper 7 18 Robert Allen 8 21\\nGang Luo 8 42 S. Huang 15 16 Hui Fang 8 42\\nJ. Guo 9 12 Hui Yu 20 30 Xiaoyan Li 6 31\\nYan Tang 11 31 Lei Fang 7 17 Ping Zhou 17 33\\nX Wang 14 33 Yue Zhao 9 36 Lei Jin 6 16\\nPaul Brown 8 20 Peter Phillips 3 13 T Taylor 3 4\\nDavid Nelson 10 15 F. Wang 14 15 Z. Wang 35 43\\nHong Xie 6 10 J. Yin 7 18 J Wang 5 35\\nJohn Hale 3 36 Kuo Zhang 4 16 M. Rahman 7 15\\nMichael Smith 16 27 R Taylor 11 27 Young Park 8 17\\nAth.: Number of distinct authors, Rec.: citation records associated with that author.\\nTable 4 .Details of DBLP dataset.\\nS. no. Name No. of authors Citation records\\n1 A. Gupta 26 577\\n2 A. Kumar 14 244\\n3 C. Chen 61 800\\n4 D. Johnson 15 368\\n5 J. Lee 100 1417\\n6 J. Martin 16 112\\n7 J. Robinson 12 171\\n8 J. Smith 31 927\\n9 K. Tanaka 10 280\\n10 M. Brown 13 153\\n11 M. Jones 13 259\\n12 M. Miller 12 412\\n13 S. Lee 86 1458\\n14 Y. Chen 71 1264\\nTotal 480 8442\\nACP, AAP, and K-metric are expressed in Eq. ( 1), where n iis the number of elements in cluster i, nj\\nis the number of elements in cluster j, nijis the number of elements belonging to both clusters iandj,N\\ndenotes the size of the citations in the collection, Jis the number of gold-standard reference clusters manually\\ngenerated, and Iis the number of clusters automatically generated by ASONET.\\nACP =1\\nNI∑\\ni=1J∑\\nj=1n2\\nij\\nni; AAP =1\\nNI∑\\ni=1J∑\\nj=1n2\\nij\\nnj; K=p\\nACP \\x03AAP (1)\\n11HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nThe PP, PR, and PF1 measures are expressed in Eq. ( 2), where C(n;i) denotes the number of combinations\\nofielements from nelements. Other parameters including i,j,ni, and nijare de\\x0cned as before in Eq. ( 1).\\nPP=R∑\\nr=1S∑\\ns=1C(nrs;2)\\nR∑\\nr=1C(nr;2); PR =R∑\\nr=1S∑\\ns=1C(nrs;2)\\nS∑\\ns=1C(ns;2); PF1 =2(PP\\x03PR)\\nPP+PR(2)\\nCF1 is the harmonic mean of CP and CR, where CP is the fraction of the generated clusters that are equal to\\nthe reference clusters and CR is the fraction of correctly retrieved clusters from the reference clusters. The CP,\\nCR, and CF1 measures are given in Eq. ( 3).\\nCP=I\\\\J\\nI; CP =I\\\\J\\nJ; CF1 =2(CP\\x03CR)\\nCP+CR(3)\\n4.3. Baseline methods\\nThree unsupervised author name disambiguation methods were used as baseline methods to compare with\\nASONET [3,4,7]; details of all these methods are given in the related works.\\n4.4. Experiments and comparisons\\nTable 5 shows the average ACP, AAP, K-metric, PP, PR, PF1, CP, CR, and CF1 of ASONET. In general\\nACP and PP of ASONET in the majority of cases are approximately equal to 100%. In Figure 5, ASONET\\nperformance is compared with three baseline methods, GHOST, ADANA, and GFAD, using Arnetminer. It\\nshows overall better results than these baseline methods and achieves values 12%, 8%, and 5% higher for K-\\nmetric; 32.1%, 27.6%, and 4.2% higher for CF1; and 3.7%, 6.3%, and 1.2% higher for PF-1 as compared to\\nGHOST, ADANA, and GFAD, respectively. Similarly, in Figure 6, ASONET performance is compared with\\nthree baseline methods, GHOST, ADANA, and GFAD, using DBLP. It also shows overall better results than\\nbaseline methods and achieves values 14%, 10%, and 7% higher for K-metric; 27.8%, 26.4%, and 10.3% higher\\nfor CF1; and 4.9%, 8.6%, and 6.2% higher for PF-1 as compared to GHOST, ADANA, and GFAD, respectively.\\n0% 10%20%30%40%50%60%70%80%90%100%\\nDISC ADANA GHOST GFAD KPF1CF1 \\n0% 10%20%30%40%50%60%70%80%90%100%\\nASONET ADANA GHOST GFAD KPF-1 CF-1 \\nFigure 5 .Average K-metric, PF1, and CF1 of the\\nASONET, ADANA, GHOST, and GFAD on Arnetminer.Figure 6 .Average K-metric, PF1, and CF1 of the\\nASONET, ADANA, GHOST, and GFAD on DBLP.\\n12HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nTable 5 .Performance evaluation of ASONET for ambiguous author group of Arnetminer.\\nName ACP AAP K PP PR PF1 CP CR CF1\\nAjay Gupta 1.00 0.64 0.80 1.00 0.59 0.74 0.31 0.62 0.42\\nBarry Wilkinson 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nBin Zhu 1.00 0.85 0.92 1.00 0.72 0.83 0.68 0.87 0.76\\nBob Johnson 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nCharles Smith 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nCheng Chang 1.00 0.79 0.89 1.00 0.70 0.82 0.38 0.60 0.46\\nDavid Cooper 1.00 0.92 0.96 1.00 0.90 0.95 0.75 0.86 0.80\\nDavid Nelson 1.00 0.93 0.97 1.00 0.88 0.93 0.82 0.90 0.86\\nF. Wang 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nFei Su 0.95 0.71 0.83 0.98 0.77 0.86 0.25 0.50 0.33\\nGang Luo 1.00 0.95 0.98 1.00 0.93 0.96 0.78 0.88 0.82\\nHong Xie 1.00 0.87 0.93 1.00 0.50 0.67 0.75 0.86 0.80\\nHui Fang 1.00 0.70 0.83 1.00 0.55 0.71 0.42 0.62 0.50\\nHui Yu 1.00 0.97 0.98 1.00 0.95 0.97 0.91 0.95 0.93\\nJ. Yin 0.95 0.52 0.70 1.00 0.20 0.33 0.91 0.95 0.93\\nJ. Guo 1.00 0.92 0.96 1.00 0.67 0.80 0.82 0.90 0.86\\nJianping Wang 1.00 0.63 0.80 1.00 0.56 0.72 0.50 0.80 0.62\\nJohn Hale 1.00 0.52 0.72 1.00 0.49 0.66 0.11 0.33 0.17\\nKuo Zhang 1.00 0.91 0.95 1.00 0.91 0.95 0.60 0.75 0.67\\nLei Fang 1.00 0.90 0.95 1.00 0.77 0.87 0.75 0.86 0.80\\nLei Jin 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nM. Rahman 1.00 0.79 0.89 1.00 0.43 0.60 0.67 0.86 0.75\\nMichael Lang 1.00 0.73 0.86 1.00 0.56 0.72 0.50 0.75 0.60\\nMichael Siege 1.00 0.83 0.91 1.00 0.83 0.91 0.36 0.67 0.47\\nMichael Smith 1.00 0.76 0.87 1.00 0.35 0.52 0.62 0.81 0.70\\nPaul Brown 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nPaul Wang 1.00 0.91 0.95 1.00 0.90 0.95 0.71 0.83 0.77\\nPeter Phillips 1.00 0.77 0.88 1.00 0.74 0.85 0.20 0.33 0.25\\nPing Zhou 1.00 0.97 0.98 1.00 0.98 0.99 0.89 0.94 0.91\\nRichard Taylor 1.00 0.73 0.86 1.00 0.49 0.65 0.64 0.82 0.72\\nRobert Allen 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nThomas Taylor 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\\nXiaoming Wang 0.97 0.86 0.92 1.00 0.94 0.97 0.62 0.71 0.67\\nXiaoyan Li 1.00 0.94 0.97 1.00 0.92 0.96 0.71 0.83 0.77\\nYan Tang 1.00 0.85 0.92 1.00 0.80 0.89 0.64 0.82 0.72\\nYoung Park 1.00 0.85 0.92 1.00 0.77 0.87 0.70 0.88 0.78\\nYue Zhou 1.00 0.66 0.81 1.00 0.52 0.68 0.46 0.67 0.55\\nZ. Wang 0.93 0.91 0.92 0.57 0.40 0.50 0.78 0.80 0.79\\nAverage 0.99 0.85 0.92 0.99 0.76 0.84 0.69 0.82 0.74\\n13HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nASONET performance is better as compared to baseline methods with both the clean and noisy datasets because\\nGHOST, ADANA, and GFAD are unable to detect outlier authors that only share one coauthor with the hub\\nnode whereas ASONET can identify these authors.\\nASONET and GFAD are automatically able to \\x0cnd hidden numbers of ambiguous authors \\\\K\". Com-\\nplete details of the ground truth clusters, ASONET-generated clusters, and GFAD-generated clusters of the\\nArnetminer dataset are shown in Figure 7. Using Arnetminer, 77.5% of clusters are within the range (Ground\\nTruth Clusters \\x063), whereas only 66.7% of the clusters of GFAD are within this range. ASONET performance\\nis better than all baseline methods, even though GHOST and ADANA used the prede\\x0cned number of clusters\\n\\\\K\".\\n010 20 30 40 50 60 \\nAjay Gupta \\nCharles Sm/i.dotth \\nGang Luo \\nYan Tang \\nPaul Brown \\nHong X/i.dote \\nM/i.dotchael Sm/i.dotth \\nCheng Chang \\nDav/i.dotd Cooper \\nHu/i.dot Yu \\nYue Zhao \\nF. Wang \\nKuo Zhang \\nBob Johnson \\nFe/i.dot Su \\nHu/i.dot Fang \\nP/i.dotng Zhou \\n/T_homas Taylor \\nJ/i.dotanp/i.dotng Wang \\nYoung Park GTC AGC GFADGC \\nFigure 7 .Comparison among ground truth clusters (GTC), ASONET-generated clusters (AGC), and GFAD-generated\\nclusters (GFADGC).\\nTo show the scalability of ASONET, comparison of running time of all methods on Arnetminer and\\nDBLP is shown in Table 6. All experiments were performed on a personal computer with Intel Core i5-5200U\\nCPU @ 2.20 GHz and 8 gigabytes of memory. GFAD is the slowest and ASONET is the fastest among all the\\nbaselines. GFAD most time-consuming stage is the cycle \\x0cnding in the coauthor\\'s graph.\\nTable 6 .Summary of all methods running time on both datasets (seconds).\\nDataset ASONET ADANA GHOST GFAD\\nArnetminer 22 86 27 257\\nDBLP 41 158 109 875\\nASONET is unable to identify when two diﬀerent authors with the same name have diﬀerent coauthors\\nwith the same name, which are called very ambiguous authors in AND literature [1]. However, analysis of\\nboth datasets shows that only 0.31% of authors are very ambiguous authors, particularly some Asians names\\n(Chinese and Korean).\\n5. Conclusions and future work\\nGraph structural clustering-based community detection algorithms are not used, to the best of our knowledge,\\nin author name disambiguation algorithms. In this paper, we have proposed ASONET, which solves namesakes\\nusing only coauthors and titles, which are surely present in all bibliographic datasets. ASONET is a graph-based\\n14HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\nalgorithm that requires neither costly training data nor a priori hidden information about how many ambiguous\\nauthors there are nor any web searches, and no expert knowledge is required. ASONET performance was tested\\non two Arnetminer datasets and it showed better results than baseline methods. ASONET delivered eﬀective\\nperformance in terms of cluster metrics and scalable solution to an author name ambiguity problem. In the\\nfuture, we plan to analyze self-citations and aﬃliation of authors (if available) to disambiguate very ambiguous\\nauthors.\\nReferences\\n[1]Ferreira AA, Gon\\x18 calves MA, Laender AH. A brief survey of automatic methods for author name disambiguation.\\nSIGMOD Rec 2012; 41: 15-26.\\n[2]Tang J, Fong AC, Wang B, Zhang J. A uni\\x0ced probabilistic framework for name disambiguation in digital library.\\nIEEE T Knowl Data En 2012; 24: 975-987.\\n[3]Shin D, Kim T, Choi J, Kim J. Author name disambiguation using a graph model with node splitting and merging\\nbased on bibliographic information. Scientometrics 2014; 100: 15-50.\\n[4]Fan X, Wang J, Pu X, Zhou L, Lv B. On graph-based name disambiguation. ACM J Data Inf Qual 2011; 2: 10.\\n[5]Han D, Liu S, Hu Y, Wang B, Sun Y. ELM-based name disambiguation in bibliography. World Wide Web 2015;\\n18: 253-263.\\n[6]Ferreira AA, Veloso A, Goncalves MA, Laender AH. Self-training author name disambiguation for information\\nscarce scenarios. J Assoc Inf Sci Tech 2014; 65: 1257-1278.\\n[7]Wang X, Tang J, Cheng H, Philip SY. Adana: Active name disambiguation. In: 2011 IEEE 11th International\\nConference on Data Mining; 11 December 2011. New York, NY, USA: IEEE. pp. 794-803.\\n[8]Liu Y, Li W, Huang Z, Fang Q. A fast method based on multiple clustering for name disambiguation in bibliographic\\ncitations. J Assoc Inf Sci Tech 2015; 66: 634-644.\\n[9]Liu W, Islamaj DR, Kim S, Comeau DC, Kim W, Yeganova L, Lu Z, Wilbur WJ. Author name disambiguation for\\nPubMed. J Assoc Inf Sci Tech 2014; 65: 765-781.\\n[10] Levin M, Krawczyk S, Bethard S, Jurafsky D. Citation-based bootstrapping for large-scale author disambiguation.\\nJ Am Soc Inform Sci 2012; 63: 1030-1047.\\n[11] Kang IS, Na SH, Lee S, Jung H, Kim P, Sung WK, Lee JH. On co-authorship for author disambiguation. Inform\\nProcess Manag 2009; 45: 84-97.\\n[12] Wu H, Li B, Pei Y, He J. Unsupervised author disambiguation using Dempster-Shafer theory. Scientometrics 2014;\\n101: 1955-1972.\\n[13] Onodera N, Iwasawa M, Midorikawa N, Yoshikane F, Amano K, Ootani Y, Kodama T, Kiyama Y, Tsunoda H,\\nYamazaki S. A method for eliminating articles by homonymous authors from the large number of articles retrieved\\nby author search. J Am Soc Inf Sci Tec 2011; 62: 677-690.\\n[14] Zhu J, Yang Y, Xie Q, Wang L, Hassan SU. Robust hybrid name disambiguation framework for large databases.\\nScientometrics 2014; 98: 2255-2274.\\n[15] DeCarvalho AP, Ferreira AA, Laender AH, Gon\\x18 calves MA. Incremental unsupervised name disambiguation in\\ncleaned digital libraries. Journal of Information and Data Management 2011; 2: 289.\\n16Peng HT, Lu CY, Hsu W, Ho JM. Disambiguating authors in citations on the web and authorship correlations.\\nExpert Syst Appl 2012; 39: 10521-10532.\\n[16] Tang L, Walsh JP. Bibliometric \\x0cngerprints: name disambiguation based on approximate structure equivalence of\\ncognitive maps. Scientometrics 2010; 84: 763-784.\\n[17] Li S, Cong G, Miao C. Author name disambiguation using a new categorical distribution similarity. Lect Notes\\nArtif Int 2012; 7523: 569-584.\\n15HUSSAIN and ASGHAR/Turk J Elec Eng & Comp Sci\\n[18] Vishnyakova D. Author name disambiguation in MEDLINE based on journal descriptors and semantic types. In:\\nBioTxtM; 2016. pp. 134-142.\\n[19] Tran HN, Huynh T, Do T. Author name disambiguation by using deep neural network. arXiv preprint 2015;\\n1502.08030.\\n[20] Song M, Kim EHJ, Kim HJ. Exploring author name disambiguation on PubMed-scale. J Informetr 2015; 9: 924-941.\\n[21] Qian Y, Zheng Q, Sakai T, Ye J, Liu J. Dynamic author name disambiguation for growing digital libraries. Inform\\nRetrieval J 2015; 18: 379-412.\\n[22] Santana AF, Gon\\x18 calves MA, Laender AH, Ferreira AA. Incremental author name disambiguation by exploiting\\ndomain-speci\\x0cc heuristics. J Assoc Inf Sci Tech 2017; 68: 931-945.\\n[23] Torvik VI, Smalheiser NR. Author name disambiguation in MEDLINE. ACM T Knowl Discov D 2009; 3: 11.\\n[24] Han H, Giles L, Zha H, Li C, Tsioutsiouliklis K. Two supervised learning approaches for name disambiguation in\\nauthor citations. In: Proceedings of 2004 Joint ACM/IEEE Conference on DL; 2004. New York, NY, USA: IEEE.\\npp. 296-305.\\n[25] Huang J, Sun H, Song Q, Deng H, Han J. Revealing density-based clustering structure from the core-connected\\ntree of a network. IEEE T Knowl Data En 2013; 25: 1876-1887.\\n[26] Johnson DB. Finding all the elementary circuits of a directed graph. SIAM J Comput 1975; 4: 77-84.\\n[27] Cota RG, Ferreira AA, Nascimento C, Gon\\x18 calves MA, Laender AH. An unsupervised heuristic-based hierarchical\\nmethod for name disambiguation in bibliographic citations. J Assoc Inf Sci Tech 2010; 61: 1853-1870.\\n16']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type:\", type(documents))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "991934d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all strings in documents to create a single string\n",
    "LM_corpus = ' '.join(documents)\n",
    "type(LM_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac15266",
   "metadata": {},
   "source": [
    "# Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8f528",
   "metadata": {},
   "source": [
    "### Function for spliting data into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed71b39",
   "metadata": {},
   "source": [
    "This function will split data into senetences on the basis on new line i.e '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5a43d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(Data):\n",
    "    sentences = Data.split('\\n')\n",
    "    # Removing leading and trailing spaces dropping empty strings\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba30ee",
   "metadata": {},
   "source": [
    "### Function for tokenizing the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f9c8f",
   "metadata": {},
   "source": [
    "This function will tokenize the above splited sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "da84d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences_list):\n",
    "    # Tokenizes a list of sentences into a list of lists of tokens\n",
    "    # Initialize an empty list to hold tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # Loop through each sentence in the list of sentences\n",
    "    for sentence in sentences_list:\n",
    "        # Convert the sentence to lowercase\n",
    "        lowercase_sentence = sentence.lower()\n",
    "        # Tokenize the lowercase sentence into a list of tokens\n",
    "        tokenized_sentence = nltk.word_tokenize(lowercase_sentence)\n",
    "        # Append the tokenized sentence to the list of tokenized sentences\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "        \n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7116e",
   "metadata": {},
   "source": [
    "Combining both above functions into one function and then testing the whole function by givivg a test string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d961d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \n",
    "    # Split the input data into sentences\n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    # Tokenize each sentence into a list of words\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    # Return the list of tokenized sentences\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a1525451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My name is Ahmad Faraz.\n",
      " I am from Lahore. \n",
      " I love Cricket.\n",
      "[['my', 'name', 'is', 'ahmad', 'faraz', '.'], ['i', 'am', 'from', 'lahore', '.'], ['i', 'love', 'cricket', '.']]\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\" My name is Ahmad Faraz.\\n I am from Lahore. \\n I love Cricket.\"\"\"\n",
    "print(x)\n",
    "verify = get_tokenized_data(x)\n",
    "print(verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1c6b5",
   "metadata": {},
   "source": [
    "### Splitining the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "24dfe1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenized data from the LM corpus\n",
    "tokenized_data = get_tokenized_data(LM_corpus)\n",
    "\n",
    "# Shuffle the data and split into train and test sets\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8) # Set the size of the training set to be 80% of the data\n",
    "train_data = tokenized_data[0:train_size] # Select the first 80% of the data for training\n",
    "test_data = tokenized_data[train_size:] # Select the remaining 20% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "292db8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 4902\n",
      "Train set: 3921\n",
      "Test set: 981\n",
      "First training sample:\n",
      "['in', 'digital', 'library', '.', 'ieee', 'transactions', 'on', 'knowledge', 'and']\n",
      "First test sample:\n",
      "['conﬂictory', 'names', 'from', 'the', 'results', 'of', 'the', 'previous', 'stage', '.', 'in', 'the', 'last', ',', 'post-processing', 'stage', ',', 'the', 'uncon']\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of sentences in the tokenized data along with\n",
    "# the number of sentences in the training and test sets.\n",
    "print(\"Total sentences: {}\\nTrain set: {}\\nTest set: {}\".format(len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "# Print the first training sample.\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "\n",
    "# Print the first test sample.\n",
    "print(\"First test sample:\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddaf7dc",
   "metadata": {},
   "source": [
    "### Function for word count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42da23",
   "metadata": {},
   "source": [
    "This function will loop through each tokenized sentence and check if the word is not in the word count array then set its value to 1 esle add 1 to its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00b23e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \n",
    "    # Create an empty dictionary to store the word counts\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Loop through each sentence in the tokenized sentences\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Loop through each word in the sentence\n",
    "        for word in sentence:\n",
    "\n",
    "            # If the word is not already in the dictionary, add it with a count of 1\n",
    "            if word not in word_counts.keys(): \n",
    "                word_counts[word] = 1\n",
    "            \n",
    "            # If the word is already in the dictionary, increment its count by 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51288940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'name': 1,\n",
       " 'is': 1,\n",
       " 'ahmad': 1,\n",
       " 'faraz': 1,\n",
       " '.': 3,\n",
       " 'i': 2,\n",
       " 'am': 1,\n",
       " 'from': 1,\n",
       " 'lahore': 1,\n",
       " 'love': 1,\n",
       " 'cricket': 1}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "count_words(verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fe3e9",
   "metadata": {},
   "source": [
    "### Function for threshold count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "51a5b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nplus_freq_words(tokenized_sentences, count_threshold):\n",
    "    \n",
    "    # Initialize an empty list to contain the words that appear at least 'count_threshold' times.\n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word counts of the tokenized sentences using the count_words function.\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    # Iterate over each word and its count in the word_counts dictionary.\n",
    "    for word, count in word_counts.items():\n",
    "        \n",
    "        # Check that the word's count is at least as great as the minimum count threshold.\n",
    "        if count >= count_threshold:\n",
    "            \n",
    "            # Append the word to the closed_vocab list.\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "acf34819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'i']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tmp_closed_vocab = Nplus_freq_words(verify, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d65cc",
   "metadata": {},
   "source": [
    "### Function for replacing OOV with unk token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30ad5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OOV_words_replcaement(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \n",
    "    # Convert the vocabulary into a set for faster search\n",
    "    vocabulary_set = set(vocabulary)\n",
    "    \n",
    "    replaced_sentences = []\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        replaced_sentence = []\n",
    "        \n",
    "        # for each token in the sentence\n",
    "        for token in sentence:\n",
    "            \n",
    "            # Check if the token is in the vocabulary\n",
    "            if token in vocabulary_set: \n",
    "                \n",
    "                # If yes , add  word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            \n",
    "            else:\n",
    "                # otherwise, add unknown token instead\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        \n",
    "        # Append the list of tokens to the list of replaced sentences\n",
    "        replaced_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac40a6",
   "metadata": {},
   "source": [
    "### Function for Pre-Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2c463d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "\n",
    "    # Get the closed vocabulary using the train data\n",
    "    closed_vocab = Nplus_freq_words(train_data, count_threshold)\n",
    "    \n",
    "    # For the train data, replace less frequent words with \"<unk>\"\n",
    "    train_data_replaced = OOV_words_replcaement(train_data, closed_vocab)\n",
    "    \n",
    "    # For the test data, replace less frequent words with \"<unk>\"\n",
    "    test_data_replaced = OOV_words_replcaement(test_data, closed_vocab)\n",
    "    \n",
    "    # Return the preprocessed train and test data, as well as the closed vocabulary\n",
    "    return train_data_replaced, test_data_replaced, closed_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d55f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum frequency count for words to be included in the vocabulary\n",
    "min_word_freq = 2\n",
    "\n",
    "# Preprocess the train and test data\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        min_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c50d06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample: ['in', 'digital', 'library', '.', 'ieee', 'transactions', 'on', 'knowledge', 'and']\n",
      "\n",
      "First preprocessed test sample: ['<unk>', 'names', 'from', 'the', 'results', 'of', 'the', 'previous', 'stage', '.', 'in', 'the', 'last', ',', '<unk>', 'stage', ',', 'the', '<unk>']\n",
      "\n",
      "First 10 vocabulary: ['in', 'digital', 'library', '.', 'ieee', 'transactions', 'on', 'knowledge', 'and', 'shannon']\n",
      "\n",
      "Size of vocabulary: 2613\n"
     ]
    }
   ],
   "source": [
    "print(f\"First preprocessed training sample: {train_data_processed[0]}\\n\")\n",
    "print(f\"First preprocessed test sample: {test_data_processed[0]}\\n\")\n",
    "print(f\"First 10 vocabulary: {vocabulary[:10]}\\n\")\n",
    "print(f\"Size of vocabulary: {len(vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec1064",
   "metadata": {},
   "source": [
    "# Building n-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca91bf",
   "metadata": {},
   "source": [
    "### Function for Counting n-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7fe2244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Grams_Count(data, n, start_token='<s>', end_token = '<e>'):\n",
    "\n",
    "    # Initialize an empty dictionary to hold the n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    # Iterate over each sentence in the data\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Add the start token n times to the beginning of the sentence and the end token once to the end\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # Convert the list of words into a tuple so that it can be used as a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use 'i' to indicate the start of the n-gram from index 0 to the last index\n",
    "        # where the end of the n-gram is within the sentence.\n",
    "        # If n is 1, then we only need to iterate over each word in the sentence.\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m):\n",
    "\n",
    "            # Get the n-gram from the sentence\n",
    "            n_gram = sentence[i:i+n]\n",
    "\n",
    "            # Check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams.keys():\n",
    "                \n",
    "                # If the n-gram already exists in the dictionary, increment its count\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                # If the n-gram is not yet in the dictionary, add it and set its count to 1\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9d96a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "# testing the code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(N_Grams_Count(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(N_Grams_Count(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7fa3d",
   "metadata": {},
   "source": [
    "### Function for estimating probability of each sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "12bba427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probablity_estimation(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    # Convert previous n-gram to a tuple to use as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # Get the count of the previous n-gram from the n-gram counts dictionary, or set it to 0 if not found\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "\n",
    "    # Calculate the denominator using the count of the previous n-gram and applying k-smoothing\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # Create an n plus 1 gram by concatenating the previous n-gram and the current word\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    # Get the count of the n plus 1 gram from the n plus 1 gram counts dictionary, or set it to 0 if not found\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "\n",
    "    # Calculate the numerator using the count of the n plus 1 gram and applying smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    # Calculate the probability as the numerator divided by the denominator\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "00a1f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'dog' given the previous n-gram 'the lazy' is: 0.0833\n"
     ]
    }
   ],
   "source": [
    "sentences = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
    "             ['the', 'lazy', 'dog', 'is', 'owned', 'by', 'john']]\n",
    "\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = N_Grams_Count(sentences, 1)\n",
    "bigram_counts = N_Grams_Count(sentences, 2)\n",
    "\n",
    "# Estimate the probability of the word 'dog' given the previous n-gram 'the lazy'\n",
    "tmp_prob = Probablity_estimation(\"dog\", \"the lazy\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'dog' given the previous n-gram 'the lazy' is: {tmp_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5772e",
   "metadata": {},
   "source": [
    "### Function for estimating probavilities of all words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "114eaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_Word_Probalities_Estimation(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "    # Convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # Add <e> and <unk> to the vocabulary\n",
    "    vocabulary += [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        # Estimate the probability of the word given the previous n-gram\n",
    "        probability = Probablity_estimation(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a96f629b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cream': 0.06666666666666667,\n",
       " 'cake': 0.06666666666666667,\n",
       " 'food': 0.06666666666666667,\n",
       " 'hates': 0.06666666666666667,\n",
       " 'love': 0.06666666666666667,\n",
       " 'she': 0.06666666666666667,\n",
       " 'spicy': 0.06666666666666667,\n",
       " 'he': 0.06666666666666667,\n",
       " 'i': 0.06666666666666667,\n",
       " 'likes': 0.06666666666666667,\n",
       " 'ice': 0.06666666666666667,\n",
       " 'vanilla': 0.06666666666666667,\n",
       " 'chocolate': 0.06666666666666667,\n",
       " '<e>': 0.06666666666666667,\n",
       " '<unk>': 0.06666666666666667}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['i', 'love', 'chocolate', 'cake'],\n",
    "             ['she', 'likes', 'vanilla', 'ice', 'cream'],\n",
    "             ['he', 'hates', 'spicy', 'food']]\n",
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]))\n",
    "unigram_counts = N_Grams_Count(sentences, 1)\n",
    "bigram_counts = N_Grams_Count(sentences, 2)\n",
    "All_Word_Probalities_Estimation(\"likes\", unigram_counts, bigram_counts, unique_words, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247925e",
   "metadata": {},
   "source": [
    "# Sentence Generation Part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8cfab",
   "metadata": {},
   "source": [
    "### Function for suggeting a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "879a7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_suggestion(Previous_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    # Determine the n-gram order by getting the length of the first key in the n-gram counts dictionary\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    # Get the last n tokens in the Previous_words list to form the previous_n_gram\n",
    "    previous_n_gram = Previous_words[-n:]\n",
    "    # Estimate the probability of each word that follows the previous_n_gram using n-gram counts and add-k smoothing\n",
    "    probabilities = All_Word_Probalities_Estimation(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
    "    # Initialize variables to store the suggestion and its probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    # Iterate through the probability dictionary to find the word with the highest probability\n",
    "    for word, prob in probabilities.items():\n",
    "        # If start_with is not None and the word does not start with start_with, skip to the next word\n",
    "        if start_with != None:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        # If the probability of the current word is higher than the previous max probability, update the suggestion and max_prob variables\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    # Return the suggested word and its probability\n",
    "    return suggestion, max_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3b59",
   "metadata": {},
   "source": [
    "### Function for getting multiple suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e074e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multiple_Suggestions(Previous_words, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    # Determine the number of models in the n-gram counts list\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    # Initialize a list to store the suggestions from each model\n",
    "    suggestions = []\n",
    "    # Iterate through the models and get suggestions from each one\n",
    "    for i in range(model_counts-1):\n",
    "        # Get the n-gram counts and n+1-gram counts for the current model\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        # Get a word suggestion from the current model using the previous tokens, n-gram and n+1-gram counts, vocabulary, k value, and start_with constraint\n",
    "        suggestion = Word_suggestion(Previous_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k, start_with=start_with)\n",
    "        # Add the suggestion to the list of suggestions\n",
    "        suggestions.append(suggestion)\n",
    "    # Return the list of suggestions from each model\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eab6e144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = N_Grams_Count(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058422c3",
   "metadata": {},
   "source": [
    "### Testing word generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8ae19098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['Author', 'name'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('disambiguation', 0.055014146494812954),\n",
       " ('in', 0.0003621876131836291),\n",
       " ('in', 0.0003619254433586681),\n",
       " ('in', 0.0003616636528028933)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Previous_words = [\"Author\", \"name\" ]\n",
    "Suggested_Words = Multiple_Suggestions(Previous_words, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {Previous_words}, the suggestions are:\")\n",
    "display(Suggested_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a3ce6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['ﬁnding', 'similar', 'names', 'the', 'geodesic'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('distance', 0.0025678650036683784),\n",
       " ('distance', 0.001834189288334556),\n",
       " ('distance', 0.0007342143906020558),\n",
       " ('distance', 0.0007336757153338225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Previous_words = [\"ﬁnding\", \"similar\", \"names\" , \"the\",  \"geodesic\"]\n",
    "Suggested_words = Multiple_Suggestions(Previous_words, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {Previous_words}, the suggestions are:\")\n",
    "display(Suggested_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "da3c37dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['ﬁnding', 'similar', 'names', 'the'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<e>', 0.022871376811594204),\n",
       " ('geodesic', 0.0007323324789454412),\n",
       " ('geodesic', 0.0007320644216691069),\n",
       " ('geodesic', 0.000731528895391368)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Previous_words = [\"ﬁnding\", \"similar\", \"names\" , \"the\"]\n",
    "Suggested_words = Multiple_Suggestions(Previous_words, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {Previous_words}, the suggestions are:\")\n",
    "display(Suggested_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "82b5a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['ﬁnding', 'similar'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(')', 0.003588087549336204),\n",
       " ('names', 0.0007304601899196494),\n",
       " ('in', 0.0003650967506389193),\n",
       " ('in', 0.00036483035388544326)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Previous_words = [\"ﬁnding\", \"similar\"]\n",
    "Suggested_words = Multiple_Suggestions(Previous_words, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {Previous_words}, the suggestions are:\")\n",
    "display(Suggested_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590c203",
   "metadata": {},
   "source": [
    "# Testing the model by Calculating the Perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7f0deffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perplexity_Calculation(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    # n-gram order\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    # Pad the sentence with start and end tokens \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    # Convert sentence to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "    # Length of the sentence\n",
    "    N = len(sentence)\n",
    "    # Initialize product of probabilities to 1.0\n",
    "    product_prob = 1.0\n",
    "    # Iterate over each word in the sentence\n",
    "    for t in range(n, N):\n",
    "        # Extract the n-gram preceding the current word\n",
    "        n_gram = sentence[t-n:t]\n",
    "        # Current word\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the current word given the n-gram\n",
    "        probability = Probablity_estimation(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "        # Multiply the product of probabilities by the inverse of the probability of the current word\n",
    "        product_prob *= 1 / probability\n",
    "    # Calculate perplexity as the Nth root of the product of probabilities\n",
    "    perplexity = product_prob**(1/float(N))\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8beec8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 21.9161\n",
      "Perplexity for test sample: 62.2042\n",
      "Perplexity for Second train sample: 22.2904\n",
      "Perplexity for test sample: 54.1756\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set(documents[0] + documents[1]+ documents[2] + documents[3] + documents[4] + documents[5]))\n",
    "\n",
    "bigram_counts = N_Grams_Count(tokenized_data, 2)\n",
    "trigram_counts = N_Grams_Count(tokenized_data, 3)\n",
    "\n",
    "# For Sample 1\n",
    "\n",
    "perplexity_train1 = Perplexity_Calculation(train_data_processed[0],\n",
    "                                         bigram_counts, trigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "perplexity_test1 = Perplexity_Calculation(test_data_processed[0],\n",
    "                                       bigram_counts, trigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test1:.4f}\")\n",
    "\n",
    "# For S 2\n",
    "\n",
    "perplexity_train2 = Perplexity_Calculation(train_data_processed[1],\n",
    "                                         bigram_counts, trigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for Second train sample: {perplexity_train2:.4f}\")\n",
    "\n",
    "perplexity_test2 = Perplexity_Calculation(test_data_processed[1],\n",
    "                                       bigram_counts, trigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcaa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
